---
title: "Statistical Analysis of Stock Volatility"
subtitle: "Introduction to Data Science E109 Spring 2016"
author: 
- Mohan Patnam
- Pulkit Bhanot
- Pooja Singh
date: "May 04, 2016"
output: html_document
---

#Background and Motivation

The primary motivation of the project is to highlight the "risk" associated with Stocks by applying statistical methods to measure factors that affect the volatility of stocks such as “Market Sentiment” ("Beta", "Standard Deviation") and “Social Sentiment” (“Twitter chatter volume", "Twitter sentiment score”). The fundamental reason to choose a topic in finance is partly current experience working in the industry and partly to understand the significance of "risk" being an end investor. Many of us manage different types of brokerage accounts (stock trading, 401k and IRA accounts) with no clear understanding of different risk factors associated with the returns of these accounts. The risk appetite itself differs widely with different classes of investors depending on their personal situation and other factors. Hence it is prudent for such investors to understand the risk factors associated with their individual stocks or funds. Further, the recent advances in the social media has provided a wealth of information in the form of Twitter feeds or Google trends that can further provide great insight on how the sentiment of investors can potentially affect the volatility of stocks. This research is an exploratory attempt to highlight factors that can affect stock volatility backed by appropriate statistical analysis and models. 

#Project Objectives 

Volatility is a statistical measure of the dispersion of returns for a given security or market-index. Commonly, the higher the volatility, the riskier the security. The volatility itself is affected by multiple factors such as “Beta”, “Standard Deviation”, “Company Earnings”, “Balance Sheet”, “Analyst Recommendations” and “Social Sentiment (tweets)”.  We will start with the concept of Beta as a measure of volatility of a stock relative to the benchmark (like S&P 500) and analyze how it affects the returns of a stock relative to the benchmark. In particular, we will run a simple case study by choosing three stocks with widely differing returns relative to the benchmark (S&P 500) to emphasize the effect of beta on such classes of stocks. We will then generalize this a bit more, by comparing Beta (risk) for a universe of stocks against their respective returns for a chosen period and more important, we want to understand if a portfolio of low-volatility stocks earn returns different from a portfolio of high-volatility stocks. This is one of the fundamental questions that this research ponders upon by providing a statistical insight into the results. 

The main focus of this research is to analysis market sentiment factors as well as social sentiment that can potentially affect Stock's volatility. We primarily designed this way to make this project complete. Hence the second part of the research focusses on user tweets (such as Twitter and StockTwits) that has been gaining momentum since the advent of Social Media Revolution. We try to investigate if the social sentiment (tweets) has any impact on stock volatility on a short term basis, in particular (given the project time period is about one month). We will run through several regression models to predict if the stock's volatility is affected by factors such as tweet count and sentiment score (as predictors). We will also ponder little bit about social chatter being possibly a leading or lagging indicator of stock volatility, if any. 

##Concepts and Methods:
**Beta**: A quantitative measure of the volatility of a stock relative to the benchmark index, S&P 500, which is generally considered representative of the overall US securities market. Specifically, Beta is the performance the stock has experienced in the last 60 months as the S&P 500 moved 1% up or down. 

A Beta of 1 indicates the stock's price will move with the S&P 500. A Beta of less than 1 means it will be less volatile than the S&P 500. Generally, the higher the Beta, the riskier the investment. For e.g. - utilities and consumer staple stocks which are considered relatively safe have a beta of less than 0.5. On the other hand, technology and financial stocks which are considered riskier have a beta more than 1. A beta of 1.10 shows that the stock has performed 10% better than its benchmark in up markets and 10% worse in down markets, assuming all other factors remain constant. Conversely, a beta of 0.85 indicates that the stock’s return is expected to perform 15% worse than the benchmark's return during up markets and 15% better during down markets.

A stock's Beta is the covariance between the return on the stock (%) and the return on the benchmark (%), compared to the variance of the index. In other words, **beta is the slope of the least squares regression line fit between S&P 500 returns (benchmark as the predictor) and particular stock’s returns**.

$$β = \frac{Cov(StockReturn, IndexReturn)} {Var(IndexReturn)}  = \frac{σ_{xy}} {σ^2_x}$$
$$x = Index return, y = Stock return$$

**R-Squared**: The R-squared of a stock tells investors if the beta of the stock is measured against an appropriate benchmark. Measuring the correlation of a stock’s movements to that of an index like S&P 500, R-squared describes the level of association between the stock’s volatility and market risk. It is derived by squaring the value of correlation coefficient (ρ) between stock return and index return. Its value is between -1 and +1.

$$\rho_{xy} = \frac{Cov(StockReturn, IndexReturn)} {(sd(StockReturn) * sd(IndexReturn))} = \frac{σ_{xy}} {σ_x * σ_y}$$
$$\rho_{xy} = correlation coefficient between x and y$$

**Standard Deviation**: The standard deviation is another statistical measure of a stock’s volatility. It indicates the tendency of the returns to rise or fall drastically in a short period of time. A security that is volatile is also considered higher risk because its performance may change quickly in either direction at any moment. Such volatile stocks typically have a high standard deviation.

**Twitter Volume**: Twitter volume is simply the number of chat messages (a.k.a tweets) per unique stock included in this project. This was collected for a period close to 4 weeks (April 6th to April 29th). In total, there are about 50 unique stocks and 339,345 number of raw tweets. Its approximately 120MBytes of compressed data which we process incrementally. We also collected StockTwits, a platform used mostly by financial analysts, data over the same period of time and we had 135,829 number of raw messages.

**Twitter Sentiment Score**: Twitter sentiment score is a number on a 10 point scale that quantifies if the sentiment associated with a raw tweet (max 140 chars) is positive or negative towards the stock under discussion. In other words, it expresses the opinion in the tweet as positive, negative or neutral. It is simply a function of #positive and #negative words used in a tweet. The sentiment score for project analysis purpose ranges from -5 to +5 with 0 being neutral. It is derived using a lexicon (a.k.a dictionary) that classifies each commonly used word to a numerical score (in terms of positive or negative). There are a couple of lexicons available, we decided to use [Opinion Lexicon](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html) as we found it referenced in several other related analysis. For StockTwits data, we also used [AFINN](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010) word list, combined with Opinion Lexicon list, for positive and negative word lists to compare the words in a tweet to determine if it is a postiver term or a negative term to calculate the sentiment score. 


```{r, messages=FALSE, warning=FALSE, echo=FALSE}
suppressMessages(library(dplyr))
suppressMessages(library(readr))
#opts_chunk$set(cache = TRUE, message = FALSE)

suppressMessages(library(ggplot2))
suppressMessages(library(gridExtra))
suppressMessages(library(gtable))
suppressMessages(library(broom))
suppressMessages(library(ggrepel))
# Disable logging of all warning messages
options(warn=-1)
theme_set(theme_bw(base_size = 14))
suppressMessages(library(broom))
suppressMessages(library(knitr))
suppressMessages(library(RColorBrewer))
suppressMessages(library(portfolio))
suppressMessages(library(stringr))
```

##Data Description:
1.	The first part of analysis around beta uses the public data available in [Yahoo! Finance](http://finance.yahoo.com/) and [Fidelity investment](https://research2.fidelity.com/Screener).

2. The second part of analysis around social sentiment involves scrapping social media feeds from two popular websites - 
[Twitter](https://twitter.com/search?q=%24AAPL&src=ctag) and [StockTwits](http://www.stocktwits.com/symbol/AAPL?q=AAPL).

3.	The first dataset consists of four dataframes with historical stock prices for three chosen stocks (IBM, GS, EIX) and the S&P index (SPY) for a period of 10 years (2006 to 2016). This is extracted from the Yahoo finance and has weekly stock prices for the chosen period.
    Eg: [GS Historical Prices](http://finance.yahoo.com/q/hp?s=GS&a=03&b=22&c=2006&d=03&e=22&f=2016&g=w)


```{r, echo=FALSE}
# Sample dataset for historical prices of a stock
stock_history_sample<-read_csv("https://raw.githubusercontent.com/goodwillyoga/E107project/master/Mohan/data/Stock%20Historical%20Data%20-%20Sample.csv")
stock_history_sample %>% head(4) %>% kable
```

4. The second dataset has the universe of US stocks (2500+) with Beta falling in the range (0 to 5.0). Fidelity stock screener provides tools to extract relevant data including stock attributes (data variables) such as Beta (1/5/10 year annualized), Sector (industry sector categorization), Total Return (5 year annualized), Standard Deviation etc
```{r, echo=FALSE}
# Sample dataset for financial attributes like beta, SD for a universe of 2500+ stocks
stocks_data_sample<-read_csv("https://raw.githubusercontent.com/goodwillyoga/E107project/master/Mohan/data/Stocks%20Financial%20Data%20-%20Sample.csv")
stocks_data_sample %>% head(4) %>% kable
```

5. The third dataset has the twitter volume data for a basket of stocks collected between Apr 6th and Apr 30th.
```{r, echo=FALSE}
data <- read_csv(file = "https://raw.githubusercontent.com/goodwillyoga/E107project/master/pooja/stock_twits_sentiment_score_na.csv")
colnames(data) <- c('id','message','createdat','symbol','sentiment_score')

head(data[,1:4], n=5) %>% kable
```

6. The fourth dataset has the twitter sentiment score for a basket of stocks collected during the same period as #3.
```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Sample dataset for twitter sentiment score
# TODO
```

##Data Scraping:
We wanted to gather tweets from [Twitter](https://twitter.com/) and stock data from [Yahoo! Finance](http://finance.yahoo.com/). Here we try to explain briefly the steps done for extraction/processing of tweets and stocks data.

![Processing Stages](https://github.com/goodwillyoga/E107project/raw/master/pulkit/Preprocessing-stages.png)

### Twitter Data Extraction and Processing
There are some limitations on the number of requests that can be made to the twitter REST api. So we decided to use the [java streaming api](https://github.com/twitter/hbc) to capture the live tweets and record them into a file twitter.json. We called this component [extractors](https://github.com/goodwillyoga/E107project/tree/master/pulkit/extractors). The file twitter.json was periodically backed-up, gzipped and truncated to continue gathering further data. The compressed data files can be found under [data](https://github.com/goodwillyoga/E107project/tree/master/data) directory.The script [twitter-data-cleaner.R](https://github.com/goodwillyoga/E107project/blob/master/pulkit/twitter-data-cleaner.R) is run on the files. Since this extraction process is very time consuming so this script was added the capability to perform the extraction in append mode, wherein the contents of a new file are appended to an already extracted dataset from previously processed files. The script creates a .RData file [twitter.RData](https://github.com/goodwillyoga/E107project/blob/master/pulkit/twitter.RData) with 4 datasets tweets, users, hashtags and symbols. For further analysis we will be using the tweets and symbols dataset. These datasets are read by [tweeter-score-compute.R](https://github.com/goodwillyoga/E107project/blob/master/pulkit/tweeter-score-compute.R) which mutate the time of each tweets to America/New_York timezone. It computes the sentiment score for each tweet using the [Opinion Lexicon](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), which has a list of 6800 positive/negative words. It stores the sentiment scores in a dataset scores. It also computes 2 additional datasets daily_tweet_score and hourly_tweet_score these datasets contain the normalized avg sentiment scores. It also creates a [word cloud](https://github.com/goodwillyoga/E107project/blob/master/pulkit/wordCloud.png) of the frequently used words in these tweets. It writes the 4 datasets into the file [processed-tweets.RData](https://github.com/goodwillyoga/E107project/blob/master/pulkit/processed-tweets.RData).

### Yahoo Finance Data Extraction and Processing
We wrote a rscript [yahoo-finance-data-extractor.R](https://github.com/goodwillyoga/E107project/blob/master/pulkit/yahoo-finance-data-extractor.R), that extracts the prices of the choosen 70 stocks and appends it into a csv stocks.csv. The file stocks.csv was periodically backed-up, gzipped and truncated to continue gathering further data. The compressed data files can be found under [data](https://github.com/goodwillyoga/E107project/tree/master/data) directory.The script [yahoo-finance-data-processor.R](https://github.com/goodwillyoga/E107project/blob/master/pulkit/yahoo-finance-data-processor.R) is run on these files which combines the records read from the current file with the already processed records and writes it into the file [yahoo-finance.RData](https://github.com/goodwillyoga/E107project/blob/master/pulkit/yahoo-finance.RData). This [yahoo-finance.RData](https://github.com/goodwillyoga/E107project/blob/master/pulkit/yahoo-finance.RData) is then read by [stocks-compute.R](https://github.com/goodwillyoga/E107project/blob/master/pulkit/stocks-compute.R) which creates 3 datasets stocks_est, hourly_stockData and dailyStockData. 

### StockTwits Data Extraction and Processing
We followed a similar approach as Twitter data to collect tweets for the chosen stocks for three weeks from [StockTwits](https://api.stocktwits.com/api/2/streams/symbol/AAPL.json). StockTwits is a financial communications platform for the investing community so we wanted to explore if we acheive similar results as in Twitter analysis. The data gathering, cleaning and processing steps from StockTwits can be summarized as below
  
  ![StockTwits Workflow](https://github.com/goodwillyoga/E107project/blob/master/pooja/StockTwitsProcess.png?raw=true)
  
In the [stock-twits_multiple.R](https://raw.githubusercontent.com/goodwillyoga/E107project/master/pooja/stock-twits_multiple.R) file we make an API call, to StockTwits for a given symbol every 15 minutes for 3 weeks, that returns response in JSON. We use _**rjson**_ library to extract the id, message, timestamp and symbol from the JSON response and converted the UTC time to EST time as shown below. 

Next we aggregate collected data in  [stock-twits-processing.R](https://raw.githubusercontent.com/goodwillyoga/E107project/master/pooja/stock-twits-processing.R) file and cleanup duplicate messages. We then calculate the sentiment score for each tweet( or message) by taking the counts of postive terms minus the counts of negative terms in a tweet as explained in a paper [here](http://www.r-bloggers.com/an-example-on-sentiment-analysis-with-r/). We used [AFINN](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010) word list and English Opinion Lexicon positive and negative word lists to compare the words in a tweet to determine if it is a postiver term or a negative term to calculate the sentiment score. 

In the final modelling step in [stock-twits-analysis.R](https://raw.githubusercontent.com/goodwillyoga/E107project/master/pooja/stock-twits-analysis.R) file, we combine the StockTwits data and [Yahoo! finance data](https://github.com/goodwillyoga/E107project/blob/master/pulkit/yahoo-finance.RData) to model the stock price based on the tweet counts and sentiment scores. (The Yahoo finance data is in the EST timezone and so we converted the StockTwits timestanp into EST timestamp also.)

#### Data Normalization
We normalize each time series to z-scores on the basis of a local mean and standard deviation to provide a common scale for comparison of our predictors and stock market indicators.

$$Z_({x_{i}}) = \frac{x_{i} - \mu(X)}{\sigma(X)}$$ 

##Assumptions:
1. The historical weekly close price is used to calculate changes in the returns of stocks for the chosen period.

2. Note that Beta can change depending on the duration of the time period, but its relation w.r.t the index returns remains the same. So specific duration of beta is not relevant as such for our analysis, but for Beta calculation, we focus on calculating the 10-year Beta (2005-2015) annualized basis.

3. Beta measures volatility of a stock relative to benchmark (S&P500) and so beta for the benchmark itself is 1 for comparison purposes.

4. We have chosen the exchange traded fund (SPY) as a proxy for S&P 500 index performance when comparing returns of individual stocks against benchmark.

5. We have chosen 3 stocks (IBM, GS, EIX) for our case study to understand the effect of beta on stock returns. We use the same set of stocks among others during our social sentiment analysis on stock returns.
    a. *EIX* (Edison International) is an Utility company whose returns are *relatively lower* compared to S&P500 and so its beta is expected to be lower (<0.5).
    b. *IBM* (International Business Machines) is a technology company whose *returns are relatively in line* with S&P500 and so its beta is expected to be close to 1.0.
    c. *GS* (Goldman Sachs) is a financial services company whose returns are *relatively higher* compared to S&P500 and so its beta is expected to be higher (>1.5).

6. For sentiment analysis using Twitter and StockTwits data, the basic assumption is that there is a correlation between public mood sentiment and market sentiment as explained in this paper [here](http://cs229.stanford.edu/proj2011/GoelMittal-StockMarketPredictionUsingTwitterSentimentAnalysis.pdf).

7. *TODO* Explain the criteria for choosing 50+ stocks for social sentiment analysis.

### Outline 

The rest of the project is outlined as below. We start with the *Beta Analysis* first followed by *Social Sentiment* analysis using two data sources: *Twitter and StockTwit*. The conclusions from each study are summarized in the end. 

# Beta Analysis

**Load beta values for a universe of 2500+ stocks**

```{r, warning=FALSE, echo=FALSE}
url <- "https://raw.githubusercontent.com/goodwillyoga/E107project/master/Mohan/data/2500%20Stocks%20Financial%20Data.csv"
financial_data <- read_csv(url)

# data wrangling - rename columns etc
names(financial_data)[names(financial_data) == 'Beta (5 Yr Annualized)'] <- 'Beta_5Year'
names(financial_data)[names(financial_data) == 'Beta (1 Year Annualized)'] <- 'Beta_1Year'
names(financial_data)[names(financial_data) == 'Standard Deviation (5 Yr Annualized)'] <- 'SD_5Year'
names(financial_data)[names(financial_data) == 'P/E (Next Year\'s Estimate)'] <- 'PE_NextYear'
names(financial_data)[names(financial_data) == 'Total Return (5 Yr Annualized)'] <- 'Annualized_Return_5Year'
names(financial_data)[names(financial_data) == 'Sector/Industry'] <- 'Sector'
names(financial_data)[names(financial_data) == 'Security Type'] <- 'SecurityType'

ggplot(financial_data, aes(x=Beta_5Year)) + 
  geom_histogram(aes(y=..density..), bins=20, colour="black", fill="white") + 
  geom_density(alpha=0.2, fill="#FF6666") +
  geom_vline(aes(xintercept=median(Beta_5Year)), color="red", linetype="dashed", size=1) +
  ggtitle("Beta distribution")

# Plot beta group_by sector to understand patterns
sectors<-c('Utilities', 'Consumer Staples', 'Health Care', 'Information Technology', 'Energy', 'Industrials', 'Consumer Discretionary' , 'Materials', 'Financials')
filter(financial_data, Sector %in% sectors) %>%
  ggplot(aes(x=Beta_5Year)) + 
  geom_histogram(aes(y=..density..), bins=20, colour="black", fill="white") + 
  geom_density(alpha=0.2, fill="#FF6666") +
  facet_wrap(~Sector)

# Summarize beta per sector
sectors_beta<-financial_data %>% filter(Sector %in% sectors) %>%
  group_by(Sector) %>%
  summarize(Beta_Median=round(median(Beta_5Year), 3), 
            Beta_Mean=round(mean(Beta_5Year), 3), 
            Beta_SD=round(sd(Beta_5Year), 3),
            Returns_Mean=round(mean(Annualized_Return_5Year), 3),
            Returns_SD=round(sd(Annualized_Return_5Year), 3)) %>% 
  arrange(Beta_Median)

all_beta<-financial_data %>% 
  summarize(Beta_Median=round(median(Beta_5Year), 3), 
            Beta_Mean=round(mean(Beta_5Year), 3), 
            Beta_SD=round(sd(Beta_5Year), 3),
            Returns_Mean=round(mean(Annualized_Return_5Year), 3),
            Returns_SD=round(sd(Annualized_Return_5Year), 3))
all_beta$Sector<-"ALL"
all_beta<-all_beta[,c("Sector", "Beta_Median", "Beta_Mean", "Beta_SD", "Returns_Mean", "Returns_SD")]

sectors_beta<-rbind(sectors_beta, all_beta)
# replace any -ve returns to 0.0 as they are insignificant for our analysis
sectors_beta<-mutate(sectors_beta, Returns_Mean=ifelse(Returns_Mean<0, 0.00, Returns_Mean))
sectors_beta %>% kable

# bar plot showing mean beta and stock-returns across sectors
ggplot(sectors_beta, aes(x=factor(Sector), y=Beta_Mean)) + 
  geom_bar(stat="identity", width=0.5, colour="black", fill="#FF6666", alpha=0.7) + 
  geom_bar(data=sectors_beta, aes(x=factor(Sector), y=Returns_Mean), stat="identity", width=0.5, colour="black", fill="red", alpha=0.2) + 
  xlab("Sector") +
  ylab("%") +
  scale_y_log10() +
  scale_x_discrete(labels = abbreviate) +
  ggtitle("Mean beta and returns across sectors")
```

Above plots presents the higtogram and summary stats of beta (overall and classified per sector) for a universe of 2500 stocks. The shape of the distribution is roughly mounded and symmetric and hence normal distribution can reasonably fit the data. X ~ N(µ , σ2) ~ N(1.2, 0.32).

For investing purposes, Beta is further classified into categories (low/medium/high) which helps investors to choose the class of stocks they can invest depending on their risk appetite. Typically, risk-averse investors choose low beta stocks. We will basically be using three categories for our beta analysis in this paper - 0 to 0.90 (Low), 0.90 to 1.30 (Medium) and 1.30+ (High). Note that Beta can be negative as well, which means that stock returns is negatively correlated to the benchmark index returns. However, this is outside the scope of this paper.

It is clear that beta for different industry sectors are different with utilities and consumer staples having very low beta values as expected. Utilities and consumer staples tend to fluctuate less with the overall market sentiment as they are basic necessities. On the other hand, industrials and materials are cyclical in nature and their beta tends to be very high. Financials have almost perfect correlation with S&P500 with beta almost 1.

Also it is clear that returns are different for different beta. We will later see if there is a clear relation between returns and beta for a universe of 2500 stocks.
  

####Compute weekly returns using historical prices - SPY and 3 chosen stocks

```{r, warning=FALSE, echo=FALSE}
SPY_url<-"https://raw.githubusercontent.com/goodwillyoga/E107project/master/Mohan/data/SPY%20weekly%2010%20years.csv"
SPY_history<-read_csv(SPY_url)

GS_url<-"https://raw.githubusercontent.com/goodwillyoga/E107project/master/Mohan/data/GS%20weekly%2010%20years.csv"
#GS_url<-"C:/Users/mpatnam/Documents/GitHub/E107project/Mohan/data/GS weekly 10 years.csv"
GS_history<-read_csv(GS_url)

IBM_url<-"https://raw.githubusercontent.com/goodwillyoga/E107project/master/Mohan/data/IBM%20weekly%2010%20years.csv"
IBM_history<-read_csv(IBM_url)

EIX_url<-"https://raw.githubusercontent.com/goodwillyoga/E107project/master/Mohan/data/EIX%20weekly%2010%20years.csv"
EIX_history<-read_csv(EIX_url)

# build a consolidated frame with weekly returns (%) for SPY/EIX/GS/IBM
stocks<-data.frame(Date=SPY_history$Date, 
                   SPY.Close=SPY_history$Close, SPY.Volume=SPY_history$Volume, SPY.LastClose=SPY_history$Close, 
                   EIX.Close=EIX_history$Close, EIX.Volume=EIX_history$Volume, EIX.LastClose=EIX_history$Close, 
                   GS.Close=GS_history$Close, GS.Volume=GS_history$Volume, GS.LastClose=GS_history$Close,
                   IBM.Close=IBM_history$Close, IBM.Volume=IBM_history$Volume, IBM.LastClose=IBM_history$Close)

numRows<-nrow(SPY_history)

# populate last week return to facilitate calculating weekly % returns
stocks$SPY.LastClose[1:numRows-1]<-SPY_history$Close[2:numRows]
stocks$EIX.LastClose[1:numRows-1]<-EIX_history$Close[2:numRows]
stocks$GS.LastClose[1:numRows-1]<-GS_history$Close[2:numRows]
stocks$IBM.LastClose[1:numRows-1]<-IBM_history$Close[2:numRows]


# compute weekly % returns
stocks<-mutate(stocks, SPY.Returns=round((SPY.Close-SPY.LastClose)*100/SPY.LastClose, 3),
               EIX.Returns=round((EIX.Close-EIX.LastClose)*100/EIX.LastClose, 3),
               GS.Returns=round((GS.Close-GS.LastClose)*100/GS.LastClose, 3),
               IBM.Returns=round((IBM.Close-IBM.LastClose)*100/IBM.LastClose, 3))


select(stocks, Date, SPY.Close, SPY.LastClose, SPY.Returns, EIX.Close, EIX.LastClose, EIX.Returns) %>% head(2) %>% kable
select(stocks, Date, IBM.Close, IBM.LastClose, IBM.Returns, GS.Close, GS.LastClose, GS.Returns) %>% head(2) %>% kable

# Summarize weekly returns
SPY_Weekly_Returns<-summary(stocks$SPY.Returns)
EIX_Weekly_Returns<-summary(stocks$EIX.Returns)
GS_Weekly_Returns<-summary(stocks$GS.Returns)
IBM_Weekly_Returns<-summary(stocks$IBM.Returns)
Weekly_Returns<-rbind(SPY_Weekly_Returns, EIX_Weekly_Returns)
Weekly_Returns<-rbind(Weekly_Returns, GS_Weekly_Returns)
Weekly_Returns<-rbind(Weekly_Returns, IBM_Weekly_Returns)
Weekly_Returns %>% kable

```

###Beta Computation: Case Study of 3 stocks
The next part of our analysis focusses on Beta calculation using simple linear regression for each of the three stocks with very distinct mean weekly returns. The summary stats for weekly returns as shown above provides a basic idea on how weekly returns of individual stocks are related to the benchmark (SPY) returns. The returns of IBM closely match the benchmark returns, while EIX returns are significantly lower and GS significantly higher. We will now attempt to fit a linear regression model on each of the stocks against the benchmark returns to see if SPY returns can be used as a general predictor for a stock return using the stock’s estimated Beta.

The summary of correlation and regression model coefficients for the selected stocks against SPY is presented below. Note that, from the definition of Beta, the slope of the best fit line provides an estimate of its value. A detailed analysis of how beta is estimated for each stock follows.

```{r, warning=FALSE, echo=FALSE}
# Correlation
EIX_cor<-cor(stocks$EIX.Returns, stocks$SPY.Returns)
GS_cor<-cor(stocks$GS.Returns, stocks$SPY.Returns)
IBM_cor<-cor(stocks$IBM.Returns, stocks$SPY.Returns)

# Beta is the slope of the regression line between SPY returns and the stock returns
# Use SPY returns to predict individual stock returns
EIX_beta<-tidy(lm(stocks$EIX.Returns ~ stocks$SPY.Returns), conf.int=TRUE) %>% 
  filter(term == "stocks$SPY.Returns")
GS_beta<-tidy(lm(stocks$GS.Returns ~ stocks$SPY.Returns), conf.int=TRUE) %>% 
  filter(term == "stocks$SPY.Returns")
IBM_beta<-tidy(lm(stocks$IBM.Returns ~ stocks$SPY.Returns), conf.int=TRUE) %>% 
  filter(term == "stocks$SPY.Returns")

# use kable to show beta estimates, p values, r-squared and CI values
EIX_beta$term[1]<-"Beta (EIX)"
GS_beta$term[1]<-"Beta (GS)"
IBM_beta$term[1]<-"Beta (IBM)"
beta_est<-union(IBM_beta, union(EIX_beta, GS_beta))
beta_est<-mutate(beta_est, cor=c(IBM_cor, GS_cor, EIX_cor))
beta_est<-mutate(beta_est, `r.squared (%)`=round(cor*cor*100, 3))
beta_est<-bind_rows(beta_est, data_frame(term="Beta (SPY)", estimate=1.0))
beta_est<-mutate(beta_est, estimate=round(as.numeric(estimate), 2))
beta_est %>% kable

# bar plot showing estimated beta and calculated returns
ggplot(beta_est, aes(x=factor(term), y=estimate)) + 
  geom_bar(stat="identity", width=0.5, colour="black", fill="#FF6666", alpha=0.3) + 
  xlab("Case Study") +
  ylab("Beta Estimate") +
  ggtitle("Estimated Beta against benchmark (SPY)")

# regression plot showing confidence intervals
ggplot_lm<-function(stock_col) {
  stocks %>% filter(SPY.Returns >= -10) %>% 
    ggplot(aes_string(x="SPY.Returns", y=stock_col)) + 
    geom_point() +
    geom_smooth(method="lm", se = TRUE, color="red") +
    ggtitle(paste(stock_col, " vs SPY"))
}

```

### Analysis for a low-beta stock (EIX)
The below plot shows the scatterplot of SPY weekly returns (%) vs the weekly returns (%) of a utility
company (EIX) over a period of last 10 years. The Correlation coefficient (0.50) suggests that there is a substantial positive relationship between stock returns of EIX against SPY. Observe that there is no clear pattern in the scatterplot. The trend sort of appears linear, the data fall around the line with few outliers and the variance is roughly constant. So we could apply simple regression. The beta derived using simple linear regression came out to 0.44 (see Table 4), which is on the lower end (Table 2). This means, an increase or decrease of 1% in SPY returns, would result in a corresponding increase or decrease of 0.44% only with EIX stock returns (on average). However, the trend is not strong enough suggesting that EIX stock returns are less correlated against the market risk, making it a defensive stock. Note that regardless of how economy performs, utility companies will meet their revenue targets as they provide basic/mandatory services for the people. For this reason alone, people tend to invest in such defensive stocks during times of distress as this class of stocks present much lower risk. This is also very evident from the low-beta value signifying low correlation with the market returns (SPY). The R-Squared value suggests that only 25% of the variation in EIX weekly returns can be explained by our model.

```{r, warning=FALSE, echo=FALSE}
ggplot_lm("EIX.Returns")
```

### Analysis for a medium-beta stock (IBM)
The below plot shows the scatterplot of SPY weekly returns against the weekly returns of IBM over the same period showing a clear linear trend. The rest of the conditions (data fall around the center line with few outliers and the variance is roughly constant) for a simple regression is also satisfied. The correlation coefficient (0.68) is much stronger in this case (compared to EIX) suggesting a much higher correlation of stock returns against SPY.  The beta derived is 0.83 (Table 4) suggesting a much closer alignment with the market (SPY) returns. This reason alone makes IBM a cyclical stock, meaning its stock performance is tied up pretty much with the performance of the market overall. In times of distress, its stock returns should suffer at the same rate as the benchmark index (SPY). The R-Squared value came out as 0.46 (Table 4). This suggests that about 46% of variation in the IBM weekly returns can be explained by the model. This is much higher than the previous model (for EIX stock).

```{r, warning=FALSE, echo=FALSE}
ggplot_lm("IBM.Returns")
```

### Analysis for a high-beta stock (GS)
The below plot shows the scatterplot of SPY weekly returns plotted against the weekly returns of GS over the same period showing a clear trend. The correlation coefficient (0.63) is very strong suggesting a very high correlation with the benchmark (SPY) returns. The beta derived in this case is 1.42 (Table 4) which is typically regarded high. This suggests very high volatility in stock price changes compared to the benchmark, presenting a much higher risk for the investors. The R-Squared value came out as 0.40 (Table 4) suggesting that 40% of variation in the GS weekly returns can be explained by the model.

```{r, warning=FALSE, echo=FALSE}
ggplot_lm("GS.Returns")
```

### Regression between beta and stock returns
Having computed Beta for three stocks with widely differing returns, we will now generalize the relationship between Beta and total returns (both 5-year annualized) by fitting a linear regression model for the chosen universe of 2500+ stocks. In order to fit a better model, we will exclude outlier stocks with returns in excess of 80%. Below scatterplot shows the results with the beta as the predictor.

```{r, warning=FALSE, echo=FALSE}
# Regression for the entire universe of stocks with Beta range - 0.0 to 4.5
beta_returns_cor<-cor(financial_data$Annualized_Return_5Year, financial_data$Beta_5Year)
beta_returns_model<-tidy(lm(Annualized_Return_5Year ~ Beta_5Year, financial_data), conf.int=TRUE)
y_intercept_all<-beta_returns_model %>% filter(term == "(Intercept)")
slope_all<-beta_returns_model %>% filter(term == "Beta_5Year")
slope_all$estimate<-round(slope_all$estimate, 3)
slope_all$std.error<-round(slope_all$std.error, 3)
slope_all$statistic<-round(slope_all$statistic, 3)
slope_all$conf.low<-round(slope_all$conf.low, 3)
slope_all$conf.high<-round(slope_all$conf.high, 3)
slope_all$cor<-round(beta_returns_cor, 3)
slope_all$`r.squared(%)`<-round(beta_returns_cor * beta_returns_cor * 100, 3)
slope_all$beta_range<-"0.0 to 4.5"
slope_all$term<-"Slope estimate for beta (0.0 to 4.5)"
slope_all %>% kable

# plot the regression line with a vertical showing the annualized return for beta=1.0 (S&P500)
SP500_beta<-1.0
returns_for_SP500<-y_intercept_all$estimate + slope_all$estimate * SP500_beta
financial_data %>%
  filter(Annualized_Return_5Year<80) %>%
  ggplot(aes_string(x="Beta_5Year", y="Annualized_Return_5Year")) + 
  geom_point() +
  geom_smooth(method="lm", se = TRUE, color="red") +
  geom_vline(aes(xintercept = 1.0), color="blue") +
  geom_hline(aes(yintercept = returns_for_SP500), color="blue") +
  ggtitle("Beta vs Annualized_Returns for ALL stocks")

```

In the absence of any clear pattern (linear or non-linear) in the scatterplot, it is ok to fit a linear regression model to this data. The correlation coefficient came as -0.27 showing moderate association. The best fit line shows that there is a slight negative slope with the stock returns as the Beta increases. Some noteworthy points -

1. The slope came out as -5.48. This means that for every 1-unit increase in beta (risk factor in the underlying stock), the total return of the stock would actually reduce by 5.48% on an annualized basis. At first, it appears counter-intuitive as one expects better performance with additional risk. But this is actually true with only handful of selected stocks. For the majority, stocks with very high beta actually perform worse over time.  

2. When Beta=1, the stock performance should be in line with the benchmark (S&P 500). This is highlighted in the scatterplot with average stock return = 11.68%.  

3. Assuming Rs represents the expected return on the stock (y-axis) and Rb represents the expected return of the benchmark (S&P500), this model can be generalized as follows - 
$$R_s = R_f + R_b*β$$ where β = Beta and Rf = intercept showing the stock return when there is no risk.

4. Fitting the regression coefficients, we get the equation - 
$$R_s = 17.16 - 5.48*β$$ where β<=4.5

### Trend estimation using smoothing
R-squared value for the fit came at only 3.8% which is rather a poor fit. Instead of blindly interpretting the regression results, we would now attempt to use smoothing to see if a different trend can explain the relationship better.

```{r, warning=FALSE, echo=FALSE}
financial_data %>%
  filter(Annualized_Return_5Year<80) %>%
  ggplot(aes_string(x="Beta_5Year", y="Annualized_Return_5Year")) + 
  geom_point() +
  geom_smooth(span=0.25, color="red") +
  ggtitle("Beta vs Annualized_Returns for ALL stocks (Smoothing)")
```

The above plot clearly shows that returns are actually trending upwards as the beta increases upto certain threshold. This trend appears to be true with in the universe of low beta stocks (< 0.8). After that threshold, returns actually goes lower with much higher beta values. 

### Regression between low beta stocks and returns
Low beta stocks are less volatile and their returns are expected to be more predictable when compared to the benchmark (S&P500) returns. We would like to now see if above relationship of lower returns with increasing beta holds good within the universe of low beta stocks. Basically, we will repeat above analysis for stocks having beta within the 1st quartile. As done before, we will exclude outlier stocks with returns in excess of 80%.


```{r, warning=FALSE, echo=FALSE}
# Regression for the bottom 25% of stocks with Beta range - 0.0 to 0.8
beta_quartiles<-summary(financial_data$Beta_5Year)
print("Beta summary stats")
beta_quartiles
beta_1st_quartile<-beta_quartiles[2]   #bottom 25% stocks
financial_data_filtered<-financial_data %>% 
  filter(Annualized_Return_5Year<80) %>%
  filter(Beta_5Year<=beta_1st_quartile)

beta_returns_cor2<-cor(financial_data_filtered$Annualized_Return_5Year, financial_data_filtered$Beta_5Year)
beta_returns_model2<-tidy(lm(Annualized_Return_5Year ~ Beta_5Year, financial_data_filtered), conf.int=TRUE)
y_intercept_75perc<-beta_returns_model2 %>% filter(term == "(Intercept)")
slope_75perc<-beta_returns_model2 %>% filter(term == "Beta_5Year")
slope_75perc$cor<-beta_returns_cor2
slope_75perc$`r.squared(%)`<-beta_returns_cor2 * beta_returns_cor2 * 100
beta_returns<-bind_rows(slope_all, data_frame(term="Slope estimate for beta (0.0 to 0.8)",
                                                 estimate=round(slope_75perc$estimate, 3),
                                                 std.error=round(slope_75perc$std.error, 3),
                                                 statistic=round(slope_75perc$statistic, 3),
                                                 p.value=round(slope_75perc$p.value, 3),
                                                 conf.low=round(slope_75perc$conf.low, 3),
                                                 conf.high=round(slope_75perc$conf.high, 3),
                                                 cor=round(slope_75perc$cor, 3),
                                                 `r.squared(%)`=round(slope_75perc$`r.squared(%)`, 3),
                                                 beta_range="0.0 to 0.8"))

print("Slope estimate with beta as predictor")
beta_returns %>% kable

financial_data_filtered %>%
  ggplot(aes_string(x="Beta_5Year", y="Annualized_Return_5Year")) + 
  geom_point() +
  geom_smooth(method="lm", se = TRUE, color="red") +
  ggtitle("Beta vs Annualized_Returns for bottom 25% stocks")

```

The results appears more interesting and are in agreeement with the smoother trend we have seen earlier. The returns within the universe of low beta stocks are actually better with increasing beta. The slope of the best fit regression line came as 6.44 which means that for every 0.1 unit increase in beta (range: 0.0 to 0.8), the total return of the stock would actually increase by about 0.64% on an annualised basis. This appears intuitive with expectations of higher returns with higher risk (beta) stocks, but however this trend of higher returns doesn't hold good with much higher beta stocks actually returning lower returns over time. This tells us there is a threshold with beta values after which higher risk actually translates lower returns.

Fitting the regression coefficients, we get the equation - 
$$R_s = 9.83 + 6.44*β$$  where β <= 0.8


### Analysis of a basket of low-volatility stocks vs high-volatility stocks
From the regression, it appears very well that very high beta stocks actually perform worse compared to lower beta stocks over time. We would want to see if this infact true by running a simulation. In particular, we want to see if a portfolio of low-volatility stocks earn returns better than a portfolio of high-volatility stocks. We will perform this analysis using Monte Carlo simulation by dividing the universe of stocks (2500+) into quartiles (4 buckets). We will analyse the returns of lowest (bottom 25%) and highest (top 25%) beta stocks by randomly sampling 100 stocks from each bucket repeatedly to calculate mean returns.

**MonteCarlo simulation**

```{r, warning=FALSE, echo=FALSE}
beta_sorted<-financial_data %>% arrange(Beta_5Year) %>% select(Symbol, Beta_5Year, Annualized_Return_5Year)

# Monte Carlo simulation parameters
set.seed(131)
num_bins<-4                     # distinct bins after sorting betas
num_stocks<-nrow(beta_sorted)   # total number of stocks in our universe
bin_size<-num_stocks/num_bins   # number of stocks in each bin
low_beta_bin<-1:bin_size        # low beta stocks position
high_beta_bin<-(num_stocks-bin_size+1):num_stocks  # high beta stocks position
num_samples<-100                # number of samples randomly chosen from each bin for each run of the simulation

run_expt<-function(bin) {
  samples<-beta_sorted[sample(bin, size=num_samples), ]$Annualized_Return_5Year
  mean_returns<-mean(samples)
  mean_returns
}

# run the simulation to compare returns of low beta stocks vs high beta stocks
B<-10^5
returns_low_beta<-replicate(B, run_expt(low_beta_bin))
mean_returns_low_beta<-mean(returns_low_beta)
sd_returns_low_beta<-sd(returns_low_beta)

returns_high_beta<-replicate(B, run_expt(high_beta_bin))
mean_returns_high_beta<-mean(returns_high_beta)
sd_returns_high_beta<-sd(returns_high_beta)

# tabulate the results
CI_95perc<-round(mean_returns_low_beta + c(-1, 1) * qnorm(0.975) * sd_returns_low_beta, 2)
returns_results<-data_frame(Classification="Low Beta Stocks (bottom 25%)",
                         Min_Beta=round(beta_sorted[1,]$Beta_5Year, 3),
                         Max_Beta=round(beta_sorted[bin_size,]$Beta_5Year, 3),
                         Mean_Returns=round(mean_returns_low_beta, 3), 
                         SE_Returns=round(sd_returns_low_beta, 3),
                         `CI_low (95%)`=CI_95perc[1],
                         `CI_high (95%)`=CI_95perc[2])

CI_95perc<-round(mean_returns_high_beta + c(-1, 1) * qnorm(0.975) * sd_returns_high_beta, 2)
returns_results<-bind_rows(returns_results, 
                    data_frame(Classification="High Beta Stocks (top 25%)",
                               Min_Beta=round(beta_sorted[num_stocks-bin_size+1,]$Beta_5Year, 3),
                               Max_Beta=round(beta_sorted[num_stocks,]$Beta_5Year, 3),
                               Mean_Returns=round(mean_returns_high_beta, 3), 
                               SE_Returns= round(sd_returns_high_beta, 3),
                               `CI_low (95%)`=CI_95perc[1],
                               `CI_high (95%)`=CI_95perc[2]))

returns_results %>% kable

```

<pre><code>
From above results, it appears very well that Monte Carlo simulation is in agreement with the regression results above. On an average, the mean returns of low beta stocks is much higher when compared to very high beta stocks.
</code></pre>

# Twitter Analysis on Stock volatility
We now focus on the second part of the research to investigate the impact of social media chats (Twitter and StockTwits) on Stock volatility on a short term basis (For the month of April).

```{r, echo=FALSE, message=FALSE}
# Following is the list of all the stocks which we have downloaded the tweets and the stocks from Yahoo finance
tickers_symbols <- c("GILD","EIX","GS","AMZN", "RKUS","AAPL","GRPN","XIV","YHOO","VA","MSFT","TSLA","BSX","NVDA","ORCL","EW","CPGX","MRK","V","BXLT","FOXA","ERIC","AVP","TWX","CMCSA","XRX","WY","GNCA","WBA","MO","MA","FOLD","TLT","SNY","RTN","UTX","LOW","MAS","GPT","RICE","IBM","KHC","CDNS","ANTM","HD","INO","OCLR","LULU","SABR","DYN","AXLL","WEN","COH","GOOG","FB","TWTR","XOM","PSX","VLO","PGR","CINF","FAF","JBLU","DAL","HA","ACN","INFY","CTSH")
# Sectors associate with each stock
sectors <- c("Healthcare","Utilities","Financial","Services","Technology","Consumer Goods","Technology","Financial","Technology","Services","Technology","Consumer Goods","Healthcare","Technology","Technology","Healthcare","Basic Materials","Healthcare","Financial","Healthcare","Services","Telecommunications","Consumer Goods","Services","Services","Technology","Industrial Goods","Healthcare","Services","Consumer Goods","Financial","Healthcare",
             "Financial","Healthcare","Industrial Goods","Industrial Goods","Services","Industrial Goods","Financial","Basic Materials","Technology","Consumer Goods","Technology","Healthcare","Services","Healthcare","Technology","Consumer Goods","Technology","Utilities","Basic Materials","Services","Consumer Goods","Technology","Technology","Technology","Basic Materials","Basic Materials","Basic Materials","Financial","Financial","Financial",
             "Services-Airlines","Services-Airlines","Services-Airlines","Technology","Technology","Technology")

selectively_analyzed_symbols <- c("GS","IBM","EIX")
ticker_sector <- data.frame(symbol = tickers_symbols, sector = sectors)
```

Load the Processed tweets and stocks data from github
```{r load tweets, messages=FALSE, warning=FALSE, echo=FALSE}
load(url("https://github.com/goodwillyoga/E107project/raw/master/pulkit/processed-tweets.RData"))
```
```{r load stocks, messages=FALSE, warning=FALSE, echo=FALSE}
load(url("https://github.com/goodwillyoga/E107project/raw/master/pulkit/processed-stocks.RData"))
```
```{r, echo=FALSE, message=FALSE}
# Create helper function to normalize the data
normalize<-function(m){
  (m - mean(m))/sd(m)
}
```
### Exploratory Data Analysis
We collected data between April 6th and April 29th for a period close to 4 weeks. Let us take a look at few datasets that we used in the project. Following are some of the columns extracted from the tweets dataset

```{r}

tail(tweets_est, n=3) %>% select(id_str,text,symbol,sector,date_timelb)%>% kable
```

We also did sentiment analysis on the text of the tweets collected from twitter. This word cloud shows some of the frequently used words on twitter.

![Word Cloud](https://github.com/goodwillyoga/E107project/raw/master/pulkit/wordCloud.png)

We were also polling Yahoo finance data for the selective stocks for the entire time duration. Some of the fields collected from yahoo api's are 

```{r}
tail(dailyStockData, n=3)%>% select(symbol,price,open,volume,date_timelb,sector)%>% kable

```

Twitter feeds also gives us the location that the user has given in the account registration, let us see what are the locations that these users belong to. Let us visualize number of users for each of the US state
<div style="width:100%; height=70%">
![US-stat distribution](https://github.com/goodwillyoga/E107project/raw/master/pulkit/stateuserdistribution.png)
</div>

Let us continue with looking at the amount of tweets that we have collected for each symbol on log scale  

```{r,echo=FALSE, message=FALSE, fig.width=9}

tweets_est %>% group_by(symbol) %>% # group on symbol
  summarise(count = n()) %>% inner_join(ticker_sector) %>% # count number of rows for each symbol
  ggplot(aes(symbol, log(count), color=sector)) + geom_point() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  ggtitle('Distribution of Tweets per Symbol') + xlab('Symbols')+ylab('Count(log scale) ')+
  theme(legend.position="bottom")
```

We can see that in general technology companies tend to have much more number of tweets as compared to companies in other sectors e.g AMZN, FB, GOOG etc

Let us look at the biggest ganners and loosers over the data we have collected and number of tweets for each of them

```{r ,echo=FALSE, message=FALSE, fig.width=9}
# Choose the last row in the dataset, that becomes the closing price for the window 
stockEnd <- dailyStockData %>% 
  group_by(symbol) %>% 
  arrange(date_time)%>% 
  do(tail(., n=1)) %>% select(symbol,price)

# Choose the first row in the dataset, that becomes the opening price for the window, join with stockEnd and find the difference between the price 
priceChangeOverPeriod <- dailyStockData %>% 
  group_by(symbol) %>% 
  arrange(date_time)%>% do(head(., n=1)) %>% 
  select(symbol,price) %>% 
  inner_join(stockEnd, by=c("symbol"="symbol")) %>% 
  mutate(prcChange = `price.y` - `price.x`)

# Join with tweets dataset to get the count of tweets for each stock
joinedDf <- tweets_est %>% 
  group_by(symbol) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  inner_join(priceChangeOverPeriod, by=c("symbol"="symbol"))

# Show the data in the heatmap
map.market(id=joinedDf$symbol, area=joinedDf$count, group=joinedDf$symbol, color=joinedDf$prcChange, main="Stock Price changes and Tweet counts", lab=c(FALSE,TRUE))

```

Let us look at daily closing prices/volume variation for the 3 choosen stocks (EIX, GS, IBM). These three stocks were previously investigated as part of Beta analysis which showed the role of beta in the underlying stock volatility.

```{r,echo=FALSE, message=FALSE, fig.width=9}
# Plot variation of closing price for the 3 chosen stocks
dailyStockData %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  mutate(md = lubridate::mday(day)) %>%
  ggplot(aes(md, price)) + geom_line(aes(color=symbol)) + 
  theme(legend.position="bottom")+
  ggtitle('Variation in Daily Closing Price') + xlab('Days in April')+ylab('Closing Price ') + facet_wrap(~symbol,scales = "free")

# Plot variation of daily volume for the 3 choosen stocks
dailyStockData %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  mutate(md = lubridate::mday(day)) %>%
  ggplot(aes(md, volume)) + geom_line(aes(color=symbol)) + 
  theme(legend.position="bottom")+
  ggtitle('Variation of Daily Trading Volume') + xlab('Days in April')+ylab('Daily Volume ') + facet_wrap(~symbol,scales = "free")
```

We can see that there is some variation in the daily traded volumes and the prices.

```{r,echo=FALSE, message=FALSE}
#We know that Stock markets are closed on weekends, let us compute the trading days so that we can focus on the trading days only
tradingDays <- stocks_est %>% mutate(day = lubridate::mdy(lastTradeDate)) %>% ungroup() %>% select(day) %>% distinct(day)
```

### Applying Linear Regression
#### Modeling based on count of number of tweets per day
We will start with modeling of daily stock/tweets data and try to do a linear regression to find relation between volume, closing Price and Daily Price Change with number of tweets. We will plot this once for all the stocks and then selectively for 3 choosen stocks. The regression model is

$$ Y_t = \alpha + \beta X_t + \varepsilon_t, $$

Yt represents a stock indicator for day t , Xt represents the related Twitter predictor on day t, α is the intercept, β is the slope, and εt is a random error term for day t.

Let us try to model Volume, Closing Price and Daily Price change
```{r,echo=FALSE, message=FALSE, fig.width=9}
# Normalized tweets count data is stored in daily_tweet_score, Daily Stock data is present in dailyStockData, 
#let us normalize the volume, closing price and Daily Price change and then join it daily_tweet_score
dailyStockTweetNormalized <- dailyStockData %>% 
  group_by(symbol) %>% 
  mutate_each(funs(normalize), volume) %>% 
  mutate_each(funs(normalize),price) %>% 
  mutate_each(funs(normalize),prcChange) %>% 
  inner_join(daily_tweet_score, by=c("symbol"="symbol", "day"="dt")) %>% select(symbol,price,volume,open,sector,date_timelb,day,prcChange,count,avgScore)

# Let us model closing price vs count for each stock
fits <- dailyStockTweetNormalized %>%
  group_by(symbol) %>%
  do(mod = lm(price ~ count, data = .))

# Filter out rows where term is count
results_price_count <- tidy(fits,mod, conf.int = TRUE) %>% 
  filter(term=='count') %>% 
  inner_join(ticker_sector)
colnames(results_price_count)[6]<- 'pval'

# Plot for all stocks after filtering stocks with p-value < .15
p1 <- results_price_count %>% 
  filter(pval <.15) %>% 
  ggplot( aes(symbol, pval, color=sector)) + 
  geom_point() + geom_hline(yintercept = .05, color = 'red') +
  theme(plot.title=element_text(size = rel(0.75)))+
  theme(axis.title.x=element_blank(), 
        axis.text.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank()) + scale_y_continuous(limits = c(0, .15))+
  ggtitle("ClosingPrice~Count")+
  geom_text_repel(aes(symbol, pval, label=symbol), data = filter(results_price_count,  pval <.05 )) +
  theme(legend.position="bottom")

```

```{r,echo=FALSE, message=FALSE, fig.width=9}
# Let us model volume ~ count for daily data
# Fit a model
fits <- dailyStockTweetNormalized %>%
  group_by(symbol) %>%
  do(mod = lm(volume ~ count, data = .))

# Filter out rows where term is count
results_volume_count <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='count') %>% inner_join(ticker_sector)
colnames(results_volume_count)[6]<- 'pval'

# Plot for all stocks and highlight the stocks with pvals <.05
p2<- results_volume_count %>% filter(pval <.15) %>% ggplot( aes(symbol, pval, color=sector)) + 
  geom_point() + geom_hline(yintercept = .05, color = 'red') +
  theme(plot.title=element_text(size = rel(0.75)))+
  theme(axis.title.x=element_blank(), 
        axis.text.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.text.y=element_blank(),
        axis.title.y=element_blank()) + scale_y_continuous(limits = c(0, .15))+
  ggtitle("Volume~Count") + xlab("Symbols") + ylab("Pvalues")+
  geom_text_repel(aes(symbol, pval, label=symbol), data = filter(results_volume_count,  pval <.05 )) +
  theme(legend.position="bottom")

```

```{r,echo=FALSE, message=FALSE, fig.width=9, fig.height=3}

# Let us model Price Change vs count for all stocks
fits <- dailyStockTweetNormalized %>%
  group_by(symbol) %>%
  do(mod = lm(prcChange ~ count, data = .))

# Filter out rows where term is count
results_prcChange_count <- tidy(fits,mod, conf.int = TRUE) %>% 
  filter(term=='count') %>% inner_join(ticker_sector)

#Rename the column
colnames(results_prcChange_count)[6]<- 'pval'

# Plot for all stocks
p3<- results_prcChange_count %>% filter(pval <.15) %>% ggplot( aes(symbol, pval, color=sector)) + 
  geom_point() + geom_hline(yintercept = .05, color = 'red') +
  theme(plot.title=element_text(size = rel(0.75)))+
  theme(axis.title.x=element_blank(), 
        axis.text.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank()) + scale_y_continuous(limits = c(0, .15))+ 
  ggtitle("PriceChange~Count") + xlab("Symbols") + ylab("Pvalues") + 
  geom_text_repel(aes(symbol, pval, label=symbol), data = filter(results_prcChange_count,  pval <.05 )) +
  theme(legend.position="bottom")

# Extract the legend from p1
legend = gtable_filter(ggplot_gtable(ggplot_build(p1)), "guide-box") 

grid.arrange(arrangeGrob(p1+theme(legend.position="none"), 
                         p2+theme(legend.position="none"), 
                         p3+theme(legend.position="none"), nrow=1),top = "P-Value for models Using Daily Normalized Data", left = "P-values", bottom="Symbols")
```
```{r,echo=FALSE, message=FALSE, fig.width=9, fig.height=.5}
grid.newpage()
grid.draw(legend) 
```

From the above 3 plots we can see that we can model Daily volume, Closing Price and Daily Price change for stocks like AVP, BSX, XRX, PSX, GILD with number of tweets for these shares with the p-values <.05. 
Let us look at the selected 3 stocks and try to see if the above linear regression explains something about them.

```{r,echo=FALSE, message=FALSE, fig.width=9, fig.height=3}

# Let us plot specific for the 3 stocks 

p1 <- results_price_count %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  ggplot( aes(symbol,pval, color=symbol))+geom_point()+ scale_y_continuous(limits = c(0, 1))+
  theme(plot.title=element_text(size = rel(0.75)))+
  theme(axis.title.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank()) +
  geom_hline(yintercept = .05, color='red')+ ggtitle("ClosingPrice~Count")+ theme(legend.position="bottom")

p2<- results_volume_count %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  ggplot( aes(symbol, pval, color=symbol)) + geom_point() + scale_y_continuous(limits = c(0, 1))+
  theme(plot.title=element_text(size = rel(0.75)))+
  theme(axis.title.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank()) + 
  geom_hline(yintercept = .05, color = 'red') + ggtitle("Volume~Count")+ theme(legend.position="bottom")

p3<- results_prcChange_count %>% 
  filter(symbol %in% selectively_analyzed_symbols)  %>% 
  ggplot( aes(symbol,pval, color=symbol))+geom_point()+ scale_y_continuous(limits = c(0, 1))+
  theme(plot.title=element_text(size = rel(0.75)))+
  theme(axis.title.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank()) + 
  geom_hline(yintercept = .05, color='red')+ ggtitle("PriceChange~Count")+ theme(legend.position="bottom")

grid.arrange(arrangeGrob(p1+theme(legend.position="none"), 
                         p2+theme(legend.position="none"), 
                         p3+theme(legend.position="none"), nrow=1), top = "P-Value for models Using Daily Normalized Data", left = "P-values", bottom="Symbols")
```

From the above we can see that Volume and Daily Price change models with Count of tweets have p-values <.05. Let us try to plot there confidence intervals

```{r,echo=FALSE, message=FALSE, fig.width=9, fig.height=3}
# Let us plot the confidence intervals

p1 <-results_price_count %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  select(symbol,estimate, pval,`conf.low`, `conf.high`) %>% 
  ggplot( aes(x = symbol, y = estimate, color = symbol)) +
  geom_point(size = 2) + ggtitle("ClosingPrice~Count") + scale_y_continuous(limits = c(-1, 1.2))+
  theme(plot.title=element_text(size = rel(0.75)))+
  theme(axis.title.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank())+ 
  geom_errorbar(aes(ymax = `conf.high`, ymin = `conf.low`))+ theme(legend.position="bottom")

p2 <- results_volume_count %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  select(symbol,estimate, pval,`conf.low`, `conf.high`) %>% 
  ggplot( aes(x = symbol, y = estimate, color = symbol)) +
  geom_point(size = 2) + ggtitle("Volume~Count") + scale_y_continuous(limits = c(-1, 1.2))+
  theme(plot.title=element_text(size = rel(0.75)))+
  theme(axis.title.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank())+ theme(legend.position="bottom")+ 
  geom_errorbar(aes(ymax = `conf.high`, ymin = `conf.low`))

p3 <-results_prcChange_count %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  select(symbol,estimate, pval,`conf.low`, `conf.high`) %>% 
  ggplot( aes(x = symbol, y = estimate, color = symbol)) +
  geom_point(size = 2) + ggtitle("PriceChange~Count") + scale_y_continuous(limits = c(-1, 1.2))+
  theme(plot.title=element_text(size = rel(0.75)))+
  theme(axis.title.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank())+ theme(legend.position="bottom")+ 
  geom_errorbar(aes(ymax = `conf.high`, ymin = `conf.low`))

grid.arrange(arrangeGrob(p1+theme(legend.position="none"), 
                         p2+theme(legend.position="none"),
                         p3+theme(legend.position="none"), nrow=1), top = "Confidence Interval Plots",left = "Confidence Interval", bottom="Symbols")

```

We can see that for the cases when p-values <.05, confidence intervals also do not include 0. Let us plot the regression plots as well.

```{r,echo=FALSE, message=FALSE, fig.width=9, fig.height=3}
# Do regression plots for the 3 stocks based on volume~count
dailyStockTweetNormalized %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  ggplot( aes(count,price))+geom_point()+
  theme(axis.title.x = element_text(size = rel(0.75))) +
  theme(axis.title.y = element_text(size = rel(0.75))) +
  geom_smooth(color='red') +facet_wrap(~symbol)+ xlab("Tweet Counts") + ylab("Price")

# Do regression plots for the 3 stocks based on volume~count
dailyStockTweetNormalized %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  ggplot( aes(count,volume))+geom_point()+
  theme(axis.title.x = element_text(size = rel(0.75))) +
  theme(axis.title.y = element_text(size = rel(0.75))) +
  geom_smooth(color='red') +facet_wrap(~symbol)+ xlab("Tweet Counts") + ylab("Volume")

# Do regression plots for the 3 stocks based on Daily Price change~count
dailyStockTweetNormalized %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  ggplot( aes(count,prcChange))+geom_point()+
  theme(axis.title.x = element_text(size = rel(0.75))) +
  theme(axis.title.y = element_text(size = rel(0.75))) +
  geom_smooth(color='red') +facet_wrap(~symbol)+ xlab("Tweet Counts") + ylab("Price Change")

```

The plots kind of show us why GS Daily volume and Price change can be represented by a linear regression model.
Let us try to plot the Daily Stock Prices and Volume of tweets for the choosen 3 stocks. 

```{r ,echo=FALSE, message=FALSE, fig.width=9}
selectively_analyzed_symbols <- c(selectively_analyzed_symbols)
dailyStockTweetNormalized %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  mutate(md = lubridate::mday(day)) %>%  
  ggplot()+geom_line(aes(md,volume, color="Volume"))+ xlab("Day of April")+ ylab("Price/Tweet Volume")+
  theme(axis.title.x = element_text(size = rel(0.75))) +
  theme(axis.title.y = element_text(size = rel(0.75))) +
  geom_line(aes(md,price, color="Price")) + facet_wrap(~symbol,scales = "free") + theme(legend.position="bottom")

```

From the above we can see that there is a trend in which the number of tweets seems to be following the Daily Prices with a lag.

#### Modeling based on Sentiment Score of tweets per day
Now let us look at the whether we can successfully model based on the sentiment scores. We model volume, closing Price and Daily Price Change with sentiment score of the stocks. 

We will first plot for all the stocks and then focus on 3 selective stocks

```{r ,echo=FALSE, message=FALSE, fig.width=9, fig.height=3}
# Let us model closing price vs SentimentScore for each stock
fits <- dailyStockTweetNormalized %>%
  group_by(symbol) %>%
  do(mod = lm(price ~ avgScore, data = .))

results_price <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='avgScore') %>% inner_join(ticker_sector)
colnames(results_price)[6]<- 'pval'

# Plot for all stocks
p1 <- results_price %>% filter(pval <.5) %>% ggplot( aes(symbol, pval, color=sector)) + 
  geom_point() + geom_hline(yintercept = .05, color = 'red') + xlab("Symbols") + ylab("P-values") + 
  theme(plot.title=element_text(size = rel(0.75)))+
  theme(axis.title.x=element_blank(), 
        axis.text.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank()) + 
  ggtitle("ClosingPrice~Score") + scale_y_continuous(limits = c(0, .5))+
  geom_text_repel(aes(symbol, pval, label=symbol), data = filter(results_price,  pval <.05 )) +
  theme(legend.position="bottom")

# Model volume ~ sentiment score
fits <- dailyStockTweetNormalized %>%
  group_by(symbol) %>%
  do(mod = lm(volume ~ avgScore, data = .))

# Filter out rows where term is avgScore
results_volume <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='avgScore') %>% inner_join(ticker_sector)
#Rename the column
colnames(results_volume)[6]<- 'pval'

# Plot for all stocks
p2 <- results_volume%>% filter(pval <.5) %>% ggplot( aes(symbol, pval, color=sector)) + 
  theme(plot.title=element_text(size = rel(0.75)))+
  theme(axis.title.x=element_blank(), 
        axis.text.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank()) +
  geom_point() + geom_hline(yintercept = .05, color = 'red') + xlab("Symbols") + ylab("P-values") +
  ggtitle("Volume~Score") + scale_y_continuous(limits = c(0, .5))+
  geom_text_repel(aes(symbol, pval, label=symbol), data = filter(results_volume,  pval <.05 )) +
  theme(legend.position="bottom")



# Let us model Price Change vs Sentimenet Score for all stocks
fits <- dailyStockTweetNormalized %>%
  group_by(symbol) %>%
  do(mod = lm(prcChange ~ avgScore, data = .))

results_prcChange <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='avgScore') %>% inner_join(ticker_sector)
colnames(results_prcChange)[6]<- 'pval'

# Plot for all stocks
p3<- results_prcChange%>% filter(pval <.5) %>% ggplot(aes(symbol, pval, color=sector)) + 
  geom_point() + geom_hline(yintercept = .05, color = 'red') + xlab("Symbols") + ylab("P-values") +
  theme(plot.title=element_text(size = rel(0.75)))+
  theme(axis.title.x=element_blank(), 
        axis.text.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank()) +
  ggtitle("PriceChange~Score") + scale_y_continuous(limits = c(0, .5))+
  geom_text_repel(aes(symbol, pval, label=symbol), data = filter(results_prcChange,  pval <.05 )) +
  theme(legend.position="bottom")
 
# Extract the legend from p1
legend = gtable_filter(ggplot_gtable(ggplot_build(p1)), "guide-box") 

grid.arrange(arrangeGrob(p1+ 
                           theme(legend.position="none"), p2+ 
                           theme(legend.position="none"), p3+ 
                           theme(legend.position="none"), nrow=1),top = "P-Value for models Using Daily Normalized Data",left = "P-values")
```
```{r,echo=FALSE, message=FALSE, fig.width=9, fig.height=.5}
grid.newpage()
grid.draw(legend) 
```

From the above plots we can see that for stocks AVP and PSX, we can fit a linear regression model for Daily Volume, Daily Closing Price and Daily Price Change with Average daily sentiment scores. Let us try to look at the behavior for the choosen 3 stocks 

```{r ,echo=FALSE, message=FALSE, fig.width=9, fig.height=3}
# Let us plot specific for the 3 stocks 
p1 <- results_price %>% 
  filter(symbol %in% selectively_analyzed_symbols)  %>% 
  ggplot( aes(symbol,pval, color=symbol))+geom_point() + 
  theme(plot.title=element_text(size = rel(0.75)))+
  theme(axis.title.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank()) + 
  geom_hline(yintercept = .05, color='red')+ scale_y_continuous(limits = c(0, .9))+ 
  ggtitle("ClosingPrice~Score")+ theme(legend.position="bottom")


p2<- results_volume %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  ggplot( aes(symbol, pval, color=symbol)) + geom_point() +
  theme(plot.title=element_text(size = rel(0.75)))+
  theme(axis.title.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank()) + 
  xlab("Symbols") + ylab("P-values") +
  geom_hline(yintercept = .05, color = 'red') + scale_y_continuous(limits = c(0, .9))+
  ggtitle("Volume~Score")+ theme(legend.position="bottom")


p3<- results_prcChange %>% 
  filter(symbol %in% selectively_analyzed_symbols)  %>% 
  ggplot( aes(symbol,pval, color=symbol))+geom_point()+
  theme(plot.title=element_text(size = rel(0.75)))+
  theme(axis.title.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank()) + 
  xlab("Symbols") + ylab("P-values")+
  geom_hline(yintercept = .05, color='red')+ scale_y_continuous(limits = c(0, .9))+ 
  ggtitle("PriceChange~Score")+ theme(legend.position="bottom")

grid.arrange(arrangeGrob(p1+ 
                           theme(legend.position="none"), p2+ 
                           theme(legend.position="none"), p3+ 
                           theme(legend.position="none"), nrow=1), top = "P-Value for Normalized Daily Data",left = "P-values", bottom="Symbols")
```

We see that for the none of the above plots the p-values are <.05. Let us try to look at the confidence intervals as well.

```{r ,echo=FALSE, message=FALSE, fig.width=9, fig.height=3}
# Let us plot the confidence intervals

p1 <-results_price %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  select(symbol,estimate, pval,`conf.low`, `conf.high`) %>% 
  ggplot( aes(x = symbol, y = estimate, color = symbol)) +
  geom_point(size = 2) + ggtitle("ClosingPrice~Score") + scale_y_continuous(limits = c(-2, 2))+
  theme(plot.title=element_text(size = rel(0.75)))+
  theme(axis.title.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank())+ 
  geom_errorbar(aes(ymax = `conf.high`, ymin = `conf.low`))+ theme(legend.position="bottom")

p2 <- results_volume %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  select(symbol,estimate, pval,`conf.low`, `conf.high`) %>% 
  ggplot( aes(x = symbol, y = estimate, color = symbol)) +
  geom_point(size = 2) + ggtitle("Volume~Score") + scale_y_continuous(limits = c(-2, 2))+
  theme(plot.title=element_text(size = rel(0.75)))+
  theme(axis.title.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank())+ theme(legend.position="bottom")+ 
  geom_errorbar(aes(ymax = `conf.high`, ymin = `conf.low`))

p3 <-results_prcChange %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  select(symbol,estimate, pval,`conf.low`, `conf.high`) %>% 
  ggplot( aes(x = symbol, y = estimate, color = symbol)) +
  geom_point(size = 2) + ggtitle("PriceChange~Score") + scale_y_continuous(limits = c(-2, 2))+
  theme(plot.title=element_text(size = rel(0.75)))+
  theme(axis.title.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank())+ theme(legend.position="bottom")+ 
  geom_errorbar(aes(ymax = `conf.high`, ymin = `conf.low`))

grid.arrange(arrangeGrob(p1+theme(legend.position="none"), 
                         p2+theme(legend.position="none"),
                         p3+theme(legend.position="none"), nrow=1), top = "Confidence Interval Plots",left = "Confidence Interval", bottom="Symbols")

```

The p-values for none of the 3 are <.05 also we saw that all the confidence intervals also include 0,so we can say that this linear regression model is not a good fit if use the Sentiment Scores. We also had hourly sentiment scores per stock. We did similar linear regression analysis for Hourly Average Price ~ Hourly sentiment score, Hourly Price Change ~ Hourly sentiment score and Hourly Volume change ~ Hourly sentiment score. We did not find the models significant predictors. The details are added under the appendix.

Let us try to plot the Daily Stock Prices and sentiment score for the choosen 3 stocks. 

```{r ,echo=FALSE, message=FALSE, fig.width=9}
selectively_analyzed_symbols <- c(selectively_analyzed_symbols)
dailyStockTweetNormalized %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  mutate(md = lubridate::mday(day)) %>%  
  ggplot()+geom_line(aes(md,avgScore, color="Sentiment Score"))+ xlab("Day in April")+ ylab("Price/Sentiment Score")+
  theme(axis.title.x = element_text(size = rel(0.75))) +
  theme(axis.title.y = element_text(size = rel(0.75))) +
  geom_line(aes(md,price, color="Price")) + facet_wrap(~symbol,scales = "free") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+ theme(legend.position="bottom")

```

The above graphs seems to allign with what we saw for Price/VolumeOfTweets daily variations. We see above that there is a lag between Sentiment Score and Price and Sentiment scores seems to be always following the Price trends for the stocks. 

# StockTwits Analysis on Stock Volatility

Load the processed data with  preprocessed sentiment score for each message from github.
```{r, warning=FALSE}
data <- read_csv(file = "https://raw.githubusercontent.com/goodwillyoga/E107project/master/pooja/stock_twits_sentiment_score_na.csv")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Following is the list of all the stocks which we have downloaded the tweets and the stocks from Yahoo finance
tickers_symbols <- c("GILD","EIX","GS","AMZN", "RKUS","AAPL","GRPN","XIV","YHOO","VA","MSFT","TSLA","BSX","NVDA","ORCL","EW","CPGX","MRK","V","BXLT","FOXA","ERIC","AVP","TWX","CMCSA","XRX","WY","GNCA","WBA","MO","MA","FOLD","TLT","SNY","RTN","UTX","LOW","MAS","GPT","RICE","IBM","KHC","CDNS","ANTM","HD","INO","OCLR","LULU","SABR","DYN","AXLL","WEN","COH","GOOG","FB","TWTR")
# Sectors associate with each stock
sectors <- c("Healthcare","Utilities","Financial","Services","Technology","Consumer Goods","Technology","Financial","Technology","Services","Technology","Consumer Goods","Healthcare","Technology","Technology","Healthcare","Basic Materials","Healthcare","Financial","Healthcare","Services","Telecommunications","Consumer Goods","Services","Services","Technology","Industrial Goods","Healthcare","Services","Consumer Goods","Financial","Healthcare",
             "Financial","Healthcare","Industrial Goods","Industrial Goods","Services","Industrial Goods","Financial","Basic Materials","Technology","Consumer Goods","Technology","Healthcare","Services","Healthcare","Technology","Consumer Goods","Technology","Utilities","Basic Materials","Services","Consumer Goods","Technology","Technology","Technology")

selectively_analyzed_symbols <- c("GS","IBM","EIX")
ticker_sector <- data.frame(symbol = tickers_symbols, sector = sectors)

## ----load stocks, messages=FALSE, warning=FALSE, echo=FALSE--------------
#Load YAHOO finance data
load(url("https://github.com/goodwillyoga/E107project/raw/master/pulkit/yahoo-finance.RData"))

colnames(data) <- c('id','message','createdat','symbol','sentiment_score')

#Convert to UTC to EST time zone 
data <- data %>% mutate(utc.time = as.POSIXct(data$createdat, tz="UTC")) %>%
  mutate(est.time = format(utc.time, tz="America/New_York")) %>%
  mutate(est.date = substr(as.character(format(strftime(est.time, '%m/%d/%Y'))), 2,10)) %>%
  mutate(est.hour = paste(est.date, lubridate::hour(est.time)))

#Get hour from time function
get_24hour <- function(x){
  ret <- ''
  splitVector<- strsplit(x,':')
  ret <- sapply(splitVector, function(z){
    if(str_count(z[2],'AM')|str_count(z[2],'am')){
      paste( z[1])
    }else{
      paste( ifelse(as.numeric(z[1]) == 12,12, as.numeric(z[1])+12 ))
    }
  })
  return(ret)
}

normalize<-function(m){
  (m - mean(m))/sd(m)
}

#Average sentiment score by day
avg_sentiment_score_by_day <- data %>% 
  group_by(symbol, day=est.date) %>%
  summarise(avg_score = mean(sentiment_score), tweet_counts = n()) %>%
  ungroup() %>%
  group_by(symbol) %>%
  mutate(scale_avg_score = normalize(avg_score)) %>% 
  mutate(scale_tweet_counts = normalize(tweet_counts)) %>% 
  select(symbol, day, avg_score, tweet_counts, scale_avg_score, scale_tweet_counts) %>%
  unique() 

avg_stocks_price_by_day <- stocks %>% 
  group_by(symbol, day=lastTradeDate) %>%
  group_by(day,symbol) %>% 
  filter(volume == max(volume)) %>% # Select last record for the day
  mutate(price_change = as.numeric(open) - price) %>% #Price change for a day
  ungroup() %>%
  group_by(symbol) %>%
  mutate(scale_price = normalize(price)) %>% 
  mutate(scale_price_change = normalize(price_change)) %>% 
  mutate(scale_volume = normalize(volume)) %>% 
  ungroup() %>%
  select(symbol, day, price, scale_price, scale_price_change, scale_volume) %>%
  unique()

#Join score and price
dat.byday <- inner_join(avg_stocks_price_by_day, avg_sentiment_score_by_day)

#EDA for positive words and negative words 
#Count all positive words that were matched in stocktwits messages
posWords <- read_csv(file = "https://raw.githubusercontent.com/goodwillyoga/E107project/master/pooja/posWords_na.csv")
colnames(posWords)[1] <- c('words')
posWords <- posWords %>% group_by(words) %>% summarize(counts=n())
len <- length(posWords$words)
posWords <- cbind(posWords, rep('positive',len))
colnames(posWords)[3] <- c('sentiment')

#Count all negative words that were matched in message stocktwits messages
negWords <- read_csv(file = "https://raw.githubusercontent.com/goodwillyoga/E107project/master/pooja/negWords_na.csv")
colnames(negWords)[1] <- c('words')
negWords <- negWords %>% group_by(words) %>% summarize(counts=n())
len <- length(negWords$words)
negWords <- cbind(negWords, rep('negative',len))
colnames(negWords)[3] <- c('sentiment')
 
#Combine al the positive and nagative words and select top 20 frquently used positive words and negative words
words <- rbind(posWords, negWords)
freqwords <- words %>% filter(sentiment=='positive') %>% top_n(50,counts)
freqwords <- rbind(freqwords, words %>% filter(sentiment=='negative') %>% top_n(50,counts))
```

### Exploratory Data Analysis for StockTwits Data

1. We plotted heatmap to show the stock price and tweet counts for all chosen symbols. The area for each symbol in the heatmap represents the price of the stock for a chosen day. The color scale represents the tweet counts with lighter green reperenting more number of tweet counts for a given symbol. (This data visualization be extended to a Shiny app in the future, where we can present user a date picker and they can see the heatmap for a selected day)

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=9}
dat.plot <- 
  dat.byday %>% select(symbol, day, price, tweet_counts) %>%
  filter(day == "4/19/2016") %>%
  mutate(price=round(price)) %>%
  select(symbol, round(price), tweet_counts) 

map.market(id=dat.plot$symbol, area=dat.plot$price, group=dat.plot$symbol, color=dat.plot$tweet_counts, main="Stock Price and Tweet counts", lab=c(FALSE,TRUE))
```

2. We plotted the heatmap for the top 50 positive and negative matched words in StockTwits messages. The area and color in the heatmap represents the counts for matched words.

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=9}
map.market(id=freqwords$words, area=freqwords$counts, group=freqwords$words, color=freqwords$counts, main="Top 50 Positive and Negative Sentiment Words", lab=c(FALSE,TRUE))
```

3. We plotted the histogram of sentiment score. 

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.height=3}
hist(data$sentiment_score, main='Sentiment score', xlab='Sentiment score')
```

### Applying Linear Regression
#### Modeling based on count of number of tweets per day

Similar to Twitter Analysis, we do linear regression for StockTwits dataset to find relation between volume, closing Price and Price Change with number of tweets by day. 

Let us try to model Closing Price, Volume and Daily Price change.

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=9, fig.height=3}
suppressMessages(library(lubridate))
# Let us model closing price vs count for each stock
fits <- dat.byday %>%
  group_by(symbol) %>%
  do(mod = lm(scale_price ~ scale_tweet_counts, data = .))

# Filter out rows where term is count
results_price_count <- tidy(fits,mod, conf.int = TRUE) %>% 
  filter(term=='scale_tweet_counts') %>% 
  inner_join(ticker_sector)
colnames(results_price_count)[6]<- 'pval'

# Plot for all stocks
p1 <- results_price_count %>% filter(pval <.15) %>% ggplot( aes(symbol, pval, color=sector)) + 
  geom_point() + geom_hline(yintercept = .05, color = 'red') +
  theme(axis.title.x=element_blank(), 
        axis.text.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank(),
        plot.title=element_text(size = rel(0.75))) + 
  scale_y_continuous(limits = c(0, .15))+
  ggtitle("ClosingPrice~Count")+
  geom_text_repel(aes(symbol, pval, label=symbol), data = filter(results_price_count,  pval <.05 )) +
  theme(legend.position="bottom")

# Model volume ~ count for daily data
fits <- dat.byday %>%
  group_by(symbol) %>%
  do(mod = lm(scale_volume ~ scale_tweet_counts, data = .))

# Filter out rows where term is count
results_volume_count <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='scale_tweet_counts') %>% inner_join(ticker_sector)
colnames(results_volume_count)[6]<- 'pval'

# Plot for all stocks and highlight the stocks with pvals <.05
p2<- results_volume_count %>% filter(pval <.15) %>% ggplot( aes(symbol, pval, color=sector)) + 
  geom_point() + geom_hline(yintercept = .05, color = 'red') +
  theme(axis.title.x=element_blank(), 
        axis.text.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.text.y=element_blank(), 
        axis.title.y=element_blank(),
        plot.title=element_text(size = rel(0.75))) +
  scale_y_continuous(limits = c(0, .15))+
  ggtitle("Volume~Count") +
  geom_text_repel(aes(symbol, pval, label=symbol), data = filter(results_volume_count,  pval <.05 )) +
  theme(legend.position="bottom")

# Model Price Change vs count for all stocks
fits <- dat.byday %>%
  group_by(symbol) %>%
  do(mod = lm(scale_price_change ~ scale_tweet_counts, data = .))

# Filter out rows where term is count
results_prcChange_count <- tidy(fits,mod, conf.int = TRUE) %>% 
  filter(term=='scale_tweet_counts') %>% inner_join(ticker_sector)

#Rename the column
colnames(results_prcChange_count)[6]<- 'pval'

# Plot for all stocks
p3<- results_prcChange_count %>% filter(pval <.15) %>% ggplot( aes(symbol, pval, color=sector)) + 
  geom_point() + geom_hline(yintercept = .05, color = 'red') +
  theme(axis.title.x=element_blank(), 
        axis.text.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank(),  
        axis.text.y=element_blank(),
        plot.title=element_text(size = rel(0.75))) +
  scale_y_continuous(limits = c(0, .15))+ 
  ggtitle("PriceChange~Count") + 
  geom_text_repel(aes(symbol, pval, label=symbol), data = filter(results_prcChange_count,  pval <.05 )) +
  theme(legend.position="bottom")

# Extract the legend from p1
legend = gtable_filter(ggplot_gtable(ggplot_build(p1)), "guide-box") 

grid.arrange(arrangeGrob(p1+ 
                           theme(legend.position="none"), p2+ 
                           theme(legend.position="none"), p3+ 
                           theme(legend.position="none"), nrow=1),top = "P-Value for models Using Daily Normalized Data", left = "P-values", bottom="Symbols")
```
```{r,echo=FALSE, message=FALSE, fig.width=9, fig.height=.5}
grid.newpage()
grid.draw(legend) 
```

From the above 3 plots we can see that we can model Daily volume, Closing Price and Daily Price change for stocks like AAPL, AVP, TWTR, TSLA, GILD with number of tweets for these shares with the p-values <.05. Let us look at the selected 3 stocks and try to see if the above linear regression explains something about them.

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=9, fig.height=3}
# Let us plot specific for the 3 stocks 
p1 <- results_price_count %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  ggplot( aes(symbol,pval, color=symbol))+geom_point()+ scale_y_continuous(limits = c(0, 1))+
  geom_hline(yintercept = .05, color='red') + 
  theme(plot.title=element_text(size = rel(0.75))) +
  theme(axis.title.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank())  + 
  ggtitle("ClosingPrc~Count")+ theme(legend.position="bottom")

p2<- results_volume_count %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  ggplot( aes(symbol, pval, color=symbol)) + geom_point() + scale_y_continuous(limits = c(0, 1))+ 
  geom_hline(yintercept = .05, color = 'red') + 
  theme(plot.title=element_text(size = rel(0.75))) +
  theme(axis.title.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank()) +
  ggtitle("Volume~Count")+ theme(legend.position="bottom")

p3<- results_prcChange_count %>% 
  filter(symbol %in% selectively_analyzed_symbols)  %>% 
  ggplot( aes(symbol,pval, color=symbol))+geom_point()+ scale_y_continuous(limits = c(0, 1))+
  geom_hline(yintercept = .05, color='red') + 
  theme(plot.title=element_text(size = rel(0.75))) +
  theme(axis.title.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank()) +
  ggtitle("DailyPrcChn~Count")+ theme(legend.position="bottom")

grid.arrange(arrangeGrob(p1+ 
                           theme(legend.position="none"), p2+ 
                           theme(legend.position="none"), p3+ 
                           theme(legend.position="none"), nrow=1), top = "P-Value for models Using Daily Normalized Data", left = "P-values", bottom="Symbols")

```
```{r,echo=FALSE, message=FALSE, fig.width=9, fig.height=.5}
grid.newpage()
grid.draw(legend) 
```

From the above we can see that linear model of Volume with Count of tweets have p-values <.05. Let plot there confidence intervals.

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=9, fig.height=3}
# Let us plot the confidence intervals
p1 <-results_price_count %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  select(symbol,estimate, pval,`conf.low`, `conf.high`) %>% 
  ggplot( aes(x = symbol, y = estimate, color = symbol)) +
  theme(plot.title=element_text(size = rel(0.75)))+
  theme(axis.title.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank()) +
  geom_point(size = 2) + ggtitle("ClosingPrc~Count")+ 
  geom_errorbar(aes(ymax = `conf.high`, ymin = `conf.low`)) + theme(legend.position="bottom")

p2 <- results_volume_count %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  select(symbol,estimate, pval,`conf.low`, `conf.high`) %>% 
  ggplot( aes(x = symbol, y = estimate, color = symbol)) +
  theme(plot.title=element_text(size = rel(0.75))) +
  theme(axis.title.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank()) +
  geom_point(size = 2) + ggtitle("Volume~Count")+ 
  geom_errorbar(aes(ymax = `conf.high`, ymin = `conf.low`)) + theme(legend.position="bottom")

p3 <-results_prcChange_count %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  select(symbol,estimate, pval,`conf.low`, `conf.high`) %>% 
  ggplot( aes(x = symbol, y = estimate, color = symbol)) +
  theme(plot.title=element_text(size = rel(0.75))) +
  theme(axis.title.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank()) +
  geom_point(size = 2) + ggtitle("DailyPrcChn~Count")+ 
  geom_errorbar(aes(ymax = `conf.high`, ymin = `conf.low`)) + theme(legend.position="bottom")

grid.arrange(arrangeGrob(p1+ 
                           theme(legend.position="none"), p2+ 
                           theme(legend.position="none"), p3+ 
                           theme(legend.position="none"), nrow=1), top = "Confidence Interval Plots", left = "Confidence Interval", bottom="Symbols")
```
```{r,echo=FALSE, message=FALSE, fig.width=9, fig.height=.5}
grid.newpage()
grid.draw(legend) 
```

We can see that for the cases when p-values <.05, confidence intervals also do not include 0. Let us try to look at the regression plots as well.

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=9, fig.height=3}
# Do regression plots for the 3 stocks based on volume~count
dat.byday %>% filter(symbol %in% selectively_analyzed_symbols) %>% ggplot( aes(scale_tweet_counts,scale_price)) +
  theme(axis.title.x = element_text(size = rel(0.75))) +
  theme(axis.title.y = element_text(size = rel(0.75))) +
  xlab("Tweet Counts") + ylab("Price") +
  geom_point()+geom_smooth(color='red') +facet_wrap(~symbol)

# Do regression plots for the 3 stocks based on volume~count
dat.byday %>% filter(symbol %in% selectively_analyzed_symbols) %>% ggplot( aes(scale_tweet_counts,scale_volume)) +
  theme(axis.title.x = element_text(size = rel(0.75))) +
  theme(axis.title.y = element_text(size = rel(0.75))) +
  xlab("Tweet Counts") + ylab("Volume") +
  geom_point()+geom_smooth(color='red') +facet_wrap(~symbol)

# Do regression plots for the 3 stocks based on Daily Price change~count
dat.byday %>% filter(symbol %in% selectively_analyzed_symbols) %>% ggplot( aes(scale_tweet_counts,scale_price_change))+
  theme(axis.title.x = element_text(size = rel(0.75))) +
  theme(axis.title.y = element_text(size = rel(0.75))) +
  xlab("Tweet Counts") + ylab("Price Change") +
  geom_point()+geom_smooth(color='red') +facet_wrap(~symbol)
```

The plots kind of show that GS Daily volume vs. tweet counts can be represented by a linear regression model. Let us try to plot the Daily Stock Prices and Volume of tweets for the choosen 3 stocks.

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=9, fig.height=3}
dat.byday %>% filter(symbol %in% selectively_analyzed_symbols) %>% mutate(week = lubridate::isoweek(mdy(day))) %>% 
  ggplot()+geom_line(aes(mdy(day),scale_tweet_counts, color="Sentiment Score")) +
  theme(axis.title.x = element_text(size = rel(0.75))) +
  theme(axis.title.y = element_text(size = rel(0.75))) +
  xlab("Day") + ylab("Price/Tweet Volume") +
  geom_line(aes(mdy(day),scale_price, color="Price")) + facet_wrap(~symbol,scales = "free") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+ theme(legend.position="bottom")
```

We see a lag relationship between Tweet counts and Stock price movements for GS. 

#### Modeling based on Sentiment Score of tweets per day

Now let us look at the whether we can successfully model based on the sentiment scores. We model volume, closing Price and Price Change by sentiment score of the stocks for daily data. 

We will first plot for all the stocks and then focus on 3 selective stocks

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=9, fig.height=3}
suppressMessages(library(lubridate))
# Let us model closing price vs SentimentScore for each stock
fits <- dat.byday %>%
  group_by(symbol) %>%
  do(mod = lm(scale_price ~ scale_avg_score, data = .))

results_price <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='scale_avg_score') %>% inner_join(ticker_sector)
colnames(results_price)[6]<- 'pval'

# Plot for all stocks
p1 <- results_price %>% filter(pval <.5) %>% ggplot( aes(symbol, pval, color=sector)) + 
  geom_point() + geom_hline(yintercept = .05, color = 'red') + xlab("Symbols") + ylab("P-values") + 
  theme(axis.title.x=element_blank(), 
        axis.text.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank(),        
        plot.title=element_text(size = rel(0.75))) + 
  ggtitle("ClosingPrc~Score") + scale_y_continuous(limits = c(0, .5))+
  geom_text_repel(aes(symbol, pval, label=symbol), data = filter(results_price,  pval <.05 )) +
  theme(legend.position="bottom")

# Model volume ~ sentiment score
fits <- dat.byday %>%
  group_by(symbol) %>%
  do(mod = lm(scale_volume ~ scale_avg_score, data = .))

# Filter out rows where term is avgScore
results_volume <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='scale_avg_score') %>% inner_join(ticker_sector)
#Rename the column
colnames(results_volume)[6]<- 'pval'

# Plot for all stocks
p2 <- results_volume%>% filter(pval <.5) %>% ggplot( aes(symbol, pval, color=sector)) + 
  theme(axis.title.x=element_blank(), 
        axis.text.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),        
        plot.title=element_text(size = rel(0.75))) +
  geom_point() + geom_hline(yintercept = .05, color = 'red') + 
  ggtitle("Volume~Score") + scale_y_continuous(limits = c(0, .5))+
  geom_text_repel(aes(symbol, pval, label=symbol), data = filter(results_volume,  pval <.05 )) +
  theme(legend.position="bottom")

# Model Price Change vs Sentimenet Score for all stocks
fits <- dat.byday %>%
  group_by(symbol) %>%
  do(mod = lm(scale_price_change ~ scale_avg_score, data = .))

results_prcChange <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='scale_avg_score') %>% inner_join(ticker_sector)
colnames(results_prcChange)[6]<- 'pval'

# Plot for all stocks
p3<- results_prcChange%>% filter(pval <.5) %>% ggplot(aes(symbol, pval, color=sector)) + 
  geom_point() + geom_hline(yintercept = .05, color = 'red') + 
  theme(axis.title.x=element_blank(), 
        axis.text.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),        
        plot.title=element_text(size = rel(0.75))) +
  ggtitle("DailyPrcChn~Score") + scale_y_continuous(limits = c(0, .5))+
  geom_text_repel(aes(symbol, pval, label=symbol), data = filter(results_prcChange,  pval <.05 )) +
  theme(legend.position="bottom")

# Extract the legend from p1
legend = gtable_filter(ggplot_gtable(ggplot_build(p1)), "guide-box") 

grid.arrange(arrangeGrob(p1+ 
                           theme(legend.position="none"), p2+ 
                           theme(legend.position="none"), p3+ 
                           theme(legend.position="none"), nrow=1),top = "P-Value for models Using Daily Normalized Data",left = "P-values")
```
```{r,echo=FALSE, message=FALSE, fig.width=9, fig.height=.5}
grid.newpage()
grid.draw(legend) 
```

From the above plots we can see that for stocks AAPL, TWTR, IBM and WY, can fit a linear regression model for Daily Volume, Daily Closing Price and Daily Price Change with Average daily sentiment scores. Let us try to look at the behavior for the choosen 3 stocks.

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=9, fig.height=3}
# Let us plot specific for the 3 stocks 
p1 <- results_price %>% filter(symbol %in% selectively_analyzed_symbols)  %>% 
  ggplot( aes(symbol,pval, color=symbol))+geom_point()+ xlab("Symbols") + ylab("P-values") +
  geom_hline(yintercept = .05, color='red') + 
  theme(plot.title=element_text(size = rel(0.75))) +
  theme(axis.title.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank()) + 
  ggtitle("ClosingPrc~Score")+ theme(legend.position="bottom")


p2<- results_volume %>% filter(symbol %in% selectively_analyzed_symbols) %>% 
  ggplot( aes(symbol, pval, color=symbol)) + geom_point() + xlab("Symbols") + ylab("P-values") +
  geom_hline(yintercept = .05, color = 'red') + 
  theme(plot.title=element_text(size = rel(0.75))) +
  theme(axis.title.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank()) + 
  ggtitle("Volume~Score")+ theme(legend.position="bottom")


p3<- results_prcChange %>% filter(symbol %in% selectively_analyzed_symbols)  %>% 
  ggplot( aes(symbol,pval, color=symbol))+geom_point()+ xlab("Symbols") + ylab("P-values") +
  geom_hline(yintercept = .05, color='red') + 
  theme(plot.title=element_text(size = rel(0.75))) +
  theme(axis.title.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank()) + 
  ggtitle("DailyPrcChange~Score")+ theme(legend.position="bottom")

grid.arrange(arrangeGrob(p1+ 
                           theme(legend.position="none"), p2+ 
                           theme(legend.position="none"), p3+ 
                           theme(legend.position="none"), nrow=1), top = "P-Value for Normalized Daily Data",left = "P-values", bottom="Symbols")
```

We see that for IBM, Closing Price vs Sentiment Score yields a significant result with p-value <.05. Let us try to look at the confidence intervals as well.

```{r ,echo=FALSE, message=FALSE, warning=FALSE, fig.width=9, fig.height=3}
# Let us plot the confidence intervals
p1 <-results_price %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  select(symbol,estimate, pval,`conf.low`, `conf.high`) %>% 
  ggplot( aes(x = symbol, y = estimate, color = symbol)) +
  theme(plot.title=element_text(size = rel(0.75))) +
  theme(axis.title.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank())+ 
  geom_point(size = 2) + ggtitle("ClosingPrc~Score") + theme(legend.position="bottom")
  geom_errorbar(aes(ymax = `conf.high`, ymin = `conf.low`))

p2 <- results_volume %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  select(symbol,estimate, pval,`conf.low`, `conf.high`) %>% 
  ggplot( aes(x = symbol, y = estimate, color = symbol)) +
  theme(plot.title=element_text(size = rel(0.75))) +
  theme(axis.title.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank()) + 
  geom_point(size = 2) + ggtitle("Volume~Score") + 
  geom_errorbar(aes(ymax = `conf.high`, ymin = `conf.low`)) + theme(legend.position="bottom")

p3 <-results_prcChange %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  select(symbol,estimate, pval,`conf.low`, `conf.high`) %>% 
  ggplot( aes(x = symbol, y = estimate, color = symbol)) +
  theme(plot.title=element_text(size = rel(0.75)))  +
  theme(axis.title.x=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank()) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  geom_point(size = 2) + ggtitle("DailyPrcChn~Score") + 
  geom_errorbar(aes(ymax = `conf.high`, ymin = `conf.low`)) + theme(legend.position="bottom")

grid.arrange(arrangeGrob(p1+ 
                           theme(legend.position="none"), p2+ 
                           theme(legend.position="none"), p3+ 
                           theme(legend.position="none"), nrow=1), top = "Confidence Interval Plots", left = "Confidence Interval", bottom="Symbols")
```
```{r,echo=FALSE, message=FALSE, fig.width=9, fig.height=.5}
grid.newpage()
grid.draw(legend) 
```

We see a significant result for IBM as confidence intervals for Closing Price and Sentiment score do not include 0. Let us try to look at the regression plots as well.

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=9, fig.height=3}
# Do regression plots for the 3 stocks based on price~score
dat.byday %>% filter(symbol %in% selectively_analyzed_symbols) %>% ggplot( aes(scale_avg_score,scale_price))+
  theme(axis.title.x = element_text(size = rel(0.75))) +
  theme(axis.title.y = element_text(size = rel(0.75))) +
  xlab("Sentiment Score") + ylab("Price") +
  geom_point()+geom_smooth(color='red') +facet_wrap(~symbol)

# Do regression plots for the 3 stocks based on volume~score
dat.byday %>% filter(symbol %in% selectively_analyzed_symbols) %>% ggplot( aes(scale_avg_score,scale_volume))+
  theme(axis.title.x = element_text(size = rel(0.75))) +
  theme(axis.title.y = element_text(size = rel(0.75))) +
  xlab("Sentiment Score") + ylab("Volume") +
  geom_point()+geom_smooth(color='red') +facet_wrap(~symbol)

# Do regression plots for the 3 stocks based on Daily Price change~score
dat.byday %>% filter(symbol %in% selectively_analyzed_symbols) %>% ggplot( aes(scale_avg_score,scale_price_change))+
  theme(axis.title.x = element_text(size = rel(0.75))) +
  theme(axis.title.y = element_text(size = rel(0.75))) +
  xlab("Sentiment Score") + ylab("Price Change") +
  geom_point()+geom_smooth(color='red') +facet_wrap(~symbol)
```

The plots shows that IBM Closing Price vs. Sentiment Score can be represented by a linear regression model.

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=9, fig.height=3}
dat.byday %>% filter(symbol %in% selectively_analyzed_symbols) %>% mutate(week = lubridate::isoweek(mdy(day))) %>% 
  ggplot()+geom_line(aes(mdy(day),scale_avg_score, color="Sentiment Score")) +
  theme(axis.title.x = element_text(size = rel(0.75))) +
  theme(axis.title.y = element_text(size = rel(0.75))) +
  xlab("Day") + ylab("Price/Sentiment Score") +
  geom_line(aes(mdy(day),scale_price, color="Price")) + facet_wrap(~symbol,scales = "free") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+ theme(legend.position="bottom")
```

We see a lag relationship betweent the sentiment score and price for GS that did not give significant results and for IBM though the lag relation exists but April 18th is an outlier for IBM. 

# Conclusion 
The detailed statistical analysis is an attempt to present the Beta as a measure of stock volatility (or risk). The exploratory data analysis of "beta" is shown to be normal and further the case study of 3 very distinct stocks demonstrates that the volatility of a stock can be explained to some extent using the volatility of the benchmark index (S&P500). The next study of beta using an universe of 2500+ stocks demonstrates that there is a trend between beta and the stock returns. The returns within the universe of low beta stocks are actually better with increasing beta (< 0.8). however, this trend doesn't hold good with stocks having much higher beta (> 1.0) where returns actually trend lower with increasing beta. This was also confirmed with the Monte Carlo simulation performed by comparing the returns of low-beta stocks against the high-beta ones.

Twitter Data shows that there are more number of tweets for technology sector companies then compared to other sectors. We can successively model Daily volumes, Closing prices and Daily Price variations for some of the stocks like AVP, BSX, XRX, PSX, GILD just based on the number of tweets using Linear Regression. 
Linear regression model based on sentiment scores with Daily volumes, Closing prices and Daily Price are not good predictors for these models as p-values >.05.
We see a clear follower relationship between Sentiment scores and Stock price movements. Analyzing the trends using Cross correlation may provide some more insights into this. 

StockTwits data results were slightly different from Twitter results. For example, we got significant results for linear models for GS Daily Volume vs. Tweet counts and we got significant results for IBM for Daily Price vs. Sentiment Score. This may be attruibuted to different users contiributing to different datasets. In StockTwits data set we did not gather user data so we could not see the overlap of users between the Twitter and StockTwits datasets. 

As the stock markets are closed over the weekend, but people are tweeting continously, we have some missing data for the weekend stock prices in the model. We can use simple avarage scheme to fill the missing data for the weekend before fitting the model but is left for future consideration. Also, we can use more advanced natural language processing techniques to classify tweets into different moods like bearish, bullish and neutral and model using the mood sentiment. Lastly we can study the time-lag of the mood/sentiment for better models for market and mood cross-correlation. 

# References

1. [Stock Prediction Using Twitter Sentiment Analysis](http://cs229.stanford.edu/proj2011/GoelMittal-StockMarketPredictionUsingTwitterSentimentAnalysis.pdf)
2. [An example on sentiment analysis with R](http://www.r-bloggers.com/an-example-on-sentiment-analysis-with-r/)
3. [Understanding Beta](http://www.investopedia.com/articles/financial-theory/09/calculating-beta.asp)
4. [Low Volatility Investing](http://blog.alphaarchitect.com/2014/10/09/avoid-high-beta-stocks-period/#gs.8pw4tXI)

# Appendix

_**[Project Github Repository](https://github.com/goodwillyoga/E107project/tree/master/project)**_

[Distribution of Tweets per Day](https://github.com/goodwillyoga/E107project/blob/master/project/Appendix.html)

[Hourly Price chnage with Hourly Sentiment Score](https://github.com/goodwillyoga/E107project/blob/master/project/Appendix.html)

[Hourly Price with Hourly Sentiment Score](https://github.com/goodwillyoga/E107project/blob/master/project/Appendix.html)

[HourlyVolumeChange with Hourly Sentiment Score](https://github.com/goodwillyoga/E107project/blob/master/project/Appendix.html)

