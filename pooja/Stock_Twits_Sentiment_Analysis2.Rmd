---
title: "Stock_Twits_Sentiment_Analysis"
output: html_document
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
if (!require("knitr")) {
   install.packages("knitr", dependencies = TRUE)
   }
if (!require("dplyr")) {
   install.packages("dplyr", dependencies = TRUE)
   }
if (!require("readr")) {
   install.packages("readr", dependencies = TRUE)
   }
if (!require("stringr")) {
   install.packages("stringr", dependencies = TRUE)
   }
if (!require("ggplot2")) {
   install.packages("ggplot2", dependencies = TRUE)
   }
if (!require("broom")) {
   install.packages("broom", dependencies = TRUE)
   }
if (!require("lubridate")) {
   install.packages("lubridate", dependencies = TRUE)
   }
if (!require("ggrepel")) {
   install.packages("ggrepel", dependencies = TRUE)
   }
if (!require("RColorBrewer")) {
   install.packages("RColorBrewer", dependencies = TRUE)
   }
if (!require("gplots")) {
  install.packages("gplots", dependencies = TRUE)
}
if (!require("portfolio")) {
  install.packages("portfolio")
}
library(knitr)
library(dplyr)
library(readr)
library(stringr)
library(ggplot2)
library(broom)
library(lubridate)
library(ggrepel)
library(RColorBrewer)
library(gplots)
library(portfolio)
   

#Supress warnings
options(warn=-1)

#read stick-twits_sentiments generate from stock_twits_processing.R file
#data <- read_csv(file = "https://raw.githubusercontent.com/goodwillyoga/E107project/master/pooja/data/stock_twits_sentiment_score3.csv")[-1]
data <- read_csv(file = "https://raw.githubusercontent.com/goodwillyoga/E107project/master/pooja/stock_twits_sentiment_score_na.csv")
colnames(data)[1] <- "id"
colnames(data)[2] <- "message"
colnames(data)[3] <- "createdat"
colnames(data)[4] <- "symbol"
colnames(data)[5] <- "sentiment_score"
length(data$id)

#Convert to UTC to EST time zone 
data <- data %>% mutate(utc.time = as.POSIXct(data$createdat, tz="UTC")) %>%
                 mutate(est.time = format(utc.time, tz="America/New_York")) %>%
                 mutate(est.date = substr(as.character(format(strftime(est.time, '%m/%d/%Y'))), 2,10)) %>%
                 mutate(est.date.plus.one = substr(as.character(format(mdy(est.date)+1, '%m/%d/%Y')), 2,10)) %>%
                 mutate(est.hour = paste(est.date, hour(est.time)))

#Percent change function
pcchange=function(x,lag=1) {
  c(diff(x,lag))/x
}

#Get hour from time function
get_24hour <- function(x){
  ret <- ''
  splitVector<- strsplit(x,':')
  ret <- sapply(splitVector, function(z){
    if(str_count(z[2],'AM')|str_count(z[2],'am')){
      paste( z[1])
    }else{
      paste( ifelse(as.numeric(z[1]) == 12,12, as.numeric(z[1])+12 ))
    }
  })
  return(ret)
}

# Scale function
scale<-function(m){
  (m - mean(m))/sd(m)
}

#Average sentiment score by hour
avg_sentiment_score_by_hour <- data %>% 
            group_by(symbol, dayhour = est.hour) %>%
            summarise(avg_score = mean(sentiment_score), tweet_counts = n()) %>%
            mutate(pchange_tweet_counts = pcchange(tweet_counts)) %>%
            mutate(pchange_avg_score = pcchange(avg_score)) %>%
            mutate(pchange_avg_score = ifelse(is.na(pchange_avg_score) | is.infinite(pchange_avg_score) |is.nan(pchange_avg_score), 0, pchange_avg_score)) %>%
            ungroup() %>%
            group_by(symbol) %>%
            mutate(scale_pchange_avg_score = scale(pchange_avg_score)) %>% 
            mutate(scale_pchange_tweet_counts = scale(pchange_tweet_counts)) %>% 
            mutate(scale_pchange_avg_score = ifelse(is.na(scale_pchange_avg_score) | is.infinite(scale_pchange_avg_score) |is.nan(scale_pchange_avg_score), 0, scale_pchange_avg_score)) %>%
            ungroup() %>%
            select(symbol, dayhour, avg_score, tweet_counts, scale_pchange_avg_score, scale_pchange_tweet_counts) %>%
            unique() 

#Average sentiment score by day
avg_sentiment_score_by_day <- data %>% 
            group_by(symbol, day=est.date) %>%
            summarise(avg_score = mean(sentiment_score), tweet_counts = n()) %>%
            mutate(pchange_tweet_counts = pcchange(tweet_counts)) %>%
            mutate(pchange_avg_score = pcchange(avg_score)) %>%
            mutate(pchange_avg_score = ifelse(is.na(pchange_avg_score) | is.infinite(pchange_avg_score) |is.nan(pchange_avg_score), 0, pchange_avg_score)) %>%
            ungroup() %>%
            group_by(symbol) %>%
            mutate(scale_pchange_avg_score = scale(pchange_avg_score)) %>% 
            mutate(scale_pchange_tweet_counts = scale(pchange_tweet_counts)) %>% 
            mutate(scale_pchange_avg_score = ifelse(is.na(scale_pchange_avg_score) | is.infinite(scale_pchange_avg_score) |is.nan(scale_pchange_avg_score), 0, scale_pchange_avg_score)) %>%
            ungroup() %>%
            select(symbol, day, avg_score, tweet_counts, scale_pchange_avg_score, scale_pchange_tweet_counts) %>%
            unique() 

#Average sentiment score for plus one day
avg_sentiment_score_plus_a_day <- data %>% 
            group_by(symbol, day=est.date.plus.one) %>%
            summarise(avg_score = mean(sentiment_score), tweet_counts = n()) %>%
            select(symbol, day, avg_score, tweet_counts) %>%
            unique() 
 
#Load YAHOO finance data
load("/Users/poojasingh/Documents/HE107/E107project/pulkit/yahoo-finance.RData")

##Average Stock Price by hour
avg_stocks_price_by_hour <- stocks %>% 
                  mutate(dayhour = paste(lastTradeDate,get_24hour(lastTradeTime))) %>%
                  group_by(symbol, dayhour) %>%
                  summarise(price = mean(price), volume=mean(volume)) %>%
                  mutate(pchange_price_change = pcchange(price)) %>%
                  mutate(pchange_volume_change = pcchange(volume)) %>%
                  mutate(pchange_price_change = ifelse(is.na(pchange_price_change) | is.infinite(pchange_price_change) |is.nan(pchange_price_change), 0, pchange_price_change)) %>%
                  mutate(pchange_volume_change = ifelse(is.na(pchange_volume_change) | is.infinite(pchange_volume_change) |is.nan(pchange_volume_change), 0, pchange_volume_change)) %>%
                  ungroup() %>%
                  group_by(symbol) %>%
                  mutate(scale_pchange_price_change = scale(pchange_price_change)) %>% 
                  mutate(scale_pchange_price_change = ifelse(is.na(scale_pchange_price_change) | is.infinite(scale_pchange_price_change) |is.nan(scale_pchange_price_change), 0, scale_pchange_price_change)) %>%
                  ungroup() %>%
                  mutate(scale_pchange_volume_change = scale(pchange_volume_change)) %>% 
                  mutate(scale_pchange_volume_change = ifelse(is.na(pchange_volume_change) | is.infinite(pchange_volume_change) |is.nan(pchange_volume_change), 0, pchange_volume_change)) %>%
                  ungroup() %>%
                  select(symbol, dayhour, price, scale_pchange_price_change, scale_pchange_volume_change) %>%
                  unique()

#Average Stock Price by day
avg_stocks_price_by_day <- stocks %>% 
                  group_by(symbol, day=lastTradeDate) %>%
                  summarise(price = mean(price), volume = mean(volume), prvClose = max(prvClose)) %>%
                  mutate(pchange_price_change = pcchange(price)) %>%
                  mutate(pchange_volume = pcchange(volume)) %>%
                  mutate(pchange_price_change = ifelse(is.na(pchange_price_change) | is.infinite(pchange_price_change) |is.nan(pchange_price_change), 0, pchange_price_change)) %>%
                  mutate(pchange_volume = ifelse(is.na(pchange_volume) | is.infinite(pchange_volume) |is.nan(pchange_volume), 0, pchange_volume)) %>%
                  ungroup() %>%
                  group_by(symbol) %>%
                  mutate(scale_pchange_price_change = scale(pchange_price_change)) %>% 
                  mutate(scale_pchange_volume_change = scale(pchange_volume)) %>% 
                  mutate(scale_pchange_price_change = ifelse(is.na(scale_pchange_price_change) | is.infinite(scale_pchange_price_change) |is.nan(scale_pchange_price_change), 0, scale_pchange_price_change)) %>%
                  mutate(scale_pchange_volume_change = ifelse(is.na(scale_pchange_price_change) | is.infinite(scale_pchange_price_change) |is.nan(scale_pchange_price_change), 0, scale_pchange_price_change)) %>%
                  ungroup() %>%
                  select(symbol, day, price, prvClose, scale_pchange_price_change, scale_pchange_volume_change) %>%
                  unique()

#prevClosePrice
prev_close_by_day <- stocks %>% 
                  group_by(symbol, day=lastTradeDate) %>%
                  select(symbol, day, prvClose) %>%
                  unique()

#Join score and price
dat <- inner_join(avg_stocks_price_by_hour, avg_sentiment_score_by_hour)
dat.byday <- inner_join(avg_stocks_price_by_day, avg_sentiment_score_by_day)
dat.prevClose <- inner_join(avg_sentiment_score_plus_a_day, prev_close_by_day)

#Plot overlay denisty plots for perchange change in stock price and percent change in sentiment_score
dat3 <- subset(dat, symbol=="EIX")
dat3 <- data.frame(Normalized_Percent_Change_By_Hour = c(dat3$scale_pchange_price_change, dat3$scale_pchange_avg_score)
                   , lines = rep(c("Normalized percent change in price", "Normalized percent change in sentiment score"), each = length(dat3$scale_pchange_price_change)))
dat3 <- cbind(dat3, symbol="EIX")

dat4 <- subset(dat, symbol=="GS")
dat4 <- data.frame(Normalized_Percent_Change_By_Hour = c(dat4$scale_pchange_price_change, dat4$scale_pchange_avg_score)
                   , lines = rep(c("Normalized percent change in price", "Normalized percent change in sentiment score"), each = length(dat4$scale_pchange_price_change)))
dat4 <- cbind(dat4, symbol="GS")

dat5 <- subset(dat, symbol=="IBM")
dat5 <- data.frame(Normalized_Percent_Change_By_Hour = c(dat5$scale_pchange_price_change, dat5$scale_pchange_avg_score)
                   , lines = rep(c("Normalized percent change in price", "Normalized percent change in sentiment score"), each = length(dat5$scale_pchange_price_change)))
dat5 <- cbind(dat5, symbol="IBM")

#EDA for positive words and negative words 
#Count all positive words that were matched in stocktwits messages
posWords <- read_csv(file = "https://raw.githubusercontent.com/goodwillyoga/E107project/master/pooja/posWords_na.csv")
colnames(posWords)[1] <- c('words')
posWords <- posWords %>% group_by(words) %>% summarize(counts=n())
len <- length(posWords$words)
posWords <- cbind(posWords, rep('positive',len))
colnames(posWords)[3] <- c('sentiment')

#Count all negative words that were matched in message stocktwits messages
negWords <- read_csv(file = "https://raw.githubusercontent.com/goodwillyoga/E107project/master/pooja/negWords_na.csv")
colnames(negWords)[1] <- c('words')
negWords <- negWords %>% group_by(words) %>% summarize(counts=n())
len <- length(negWords$words)
negWords <- cbind(negWords, rep('negative',len))
colnames(negWords)[3] <- c('sentiment')
 
#Combine al the positive and nagative words and select top 20 frquently used positive words and negative words
words <- rbind(posWords, negWords)
freqwords <- words %>% filter(sentiment=='positive') %>% top_n(50,counts)
freqwords <- rbind(freqwords, words %>% filter(sentiment=='negative') %>% top_n(50,counts))
```

### Methodology for extraction of Data - StockTwits 
  We follwed a similar approach as for Twitter data to collect tweets for the chosen stocks for three weeks from [StockTwits](https://api.stocktwits.com/api/2/streams/symbol/AAPL.json). The data gathering, cleaning and processing steps can be summarized as below
  
  ![StockTwits Workflow](https://github.com/goodwillyoga/E107project/blob/master/pooja/StockTwitsProcess.png?raw=true)
  
In the [stock-twits_multiple.R](https://raw.githubusercontent.com/goodwillyoga/E107project/master/pooja/stock-twits_multiple.R) file we make an API call, to StockTwits for a given symbol every 15 minutes for 3 weeks, that returns response in JSON. We use _**rjson**_ library to extract the id, message, timestamp and symbol from the JSON response and converted the UTC time to EST time as shown below. 

```{r}
head(data, n=5) %>% kable
```

Next we aggregate collected data in  [stock-twits-processing.R](https://raw.githubusercontent.com/goodwillyoga/E107project/master/pooja/stock-twits-processing.R) file and cleanup duplicate messages. We then calculate the sentiment score for each tweet( or message) by taking the counts of postive terms minus the counts of negative terms in a tweet as explained in a paper [here](http://www.r-bloggers.com/an-example-on-sentiment-analysis-with-r/). We used [AFINN](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010) word list and English Opinion Lexicon positive and negative word lists to compare the words in a tweet to determine if it is a postiver term or a negative term to calculate the sentiment score. 

In the final modelling step in [stock-twits-analysis.R](https://raw.githubusercontent.com/goodwillyoga/E107project/master/pooja/stock-twits-analysis.R) file, we combine the StockTwits data and [Yahoo! finance data](https://github.com/goodwillyoga/E107project/blob/master/pulkit/yahoo-finance.RData) to model the stock price based on the tweet counts and sentiment scores. (The Yahoo finance data is in the EST timezone and we converted the StockTwits timestanp into EST timestamp also.)

Next we proceed with exploratory data analysis. 

#### Exploratory Data Analysis for Stock Twits Data

1. We plotted heatmap to show the stock price and tweet counts for all chosen symbols. The area for each symbol in the heatmap represents the price of the stock for a given day. The color scale represents the tweet counts with lighter green reperenting more number of tweet counts for a given symbol. (This data visualization be extended to a Shiny app in the future, where we can present user a date picker and they can see the heatmap for a given day)


```{r, echo=FALSE, message=FALSE, warning=FALSE}
dat.plot <- 
  dat.byday %>% select(symbol, day, price, tweet_counts) %>%
  filter(day == "4/19/2016") %>%
  mutate(price=round(price)) %>%
  select(symbol, round(price), tweet_counts) 

map.market(id=dat.plot$symbol, area=dat.plot$price, group=dat.plot$symbol, color=dat.plot$tweet_counts, main="Stock Price and Tweet counts", lab=c(FALSE,TRUE))
```

2. We plotted the heatmap for the top 50 positive and negative matched words in StockTwits messages. The area and color in the heatmap represents the counts for matched words.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
map.market(id=freqwords$words, area=freqwords$counts, group=freqwords$words, color=freqwords$counts, main="Top 50 Positive and Negative Sentiment Words", lab=c(FALSE,TRUE))
```

3. We plotted the density plot of normalized hourly percent change in sentiment score and price change for three chosen stocks (EIX, IBM, and GS).

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#Overlay Density Plot
datz <- rbind(dat3, dat5, dat4)
datz %>% 
  ggplot(aes(x = Normalized_Percent_Change_By_Hour, fill = lines)) + geom_density(alpha = 0.5) + 
  facet_wrap(~symbol) +
  theme(legend.position="bottom") +
  labs(title='Density plot of sentiment score and price change')
```

4. We plotted the histogram of sentiment score. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
hist(data$sentiment_score, main='Sentiment score', xlab='Sentiment score')
```

#### Modeling hourly and daily stock price and volume change by sentiment score and tweet counts 

1. We fit the linear regression model for _**daily stock price change**_ by sentiment score and tweet counts. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#Fit linear models by daily percent change in price ~ avg_score + tweet_counts 
fits4 <- dat.byday %>%
  group_by(symbol) %>%
  do(mod = lm(scale_pchange_price_change ~ scale_pchange_avg_score + scale_pchange_tweet_counts, data = .))

tidy(fits4, mod) %>% filter(p.value < 0.05 & term != '(Intercept)') %>% kable

results4 <- tidy(fits4, mod) %>% filter(p.value < 0.05 & term != '(Intercept)') #filter(term=='scale_pchange_avg_score') 

#Plot p-values and see symbols that have significant results
results4 %>% 
  ggplot(aes(symbol, p.value, color=symbol)) + geom_point() + geom_hline(yintercept = .05, color = 'red') +
  geom_text_repel(aes(symbol, p.value, label=symbol), data = filter(results4,  p.value <.05 )) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("Daily percent change in price  ~ sentiment score + counts for all stocks") +
  theme(legend.position = "none") 
```

2. We fit the linear regression model for _**daily stock volume change**_ by sentiment score and tweet counts.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#Fit linear models by daily percent change in volume ~ avg_score + tweet_counts 
fits3 <- dat.byday %>%
  group_by(symbol) %>%
  do(mod = lm(scale_pchange_volume_change ~ scale_pchange_avg_score + scale_pchange_tweet_counts, data = .))

tidy(fits3, mod) %>% filter(p.value < 0.05 & term != '(Intercept)') %>% kable

results3 <- tidy(fits3, mod) %>% filter(p.value < 0.05 & term != '(Intercept)') #filter(term=='scale_pchange_avg_score') 

#Plot p-values and see symbols that have significant results
results3 %>% 
  ggplot(aes(symbol, p.value, color=symbol)) + geom_point() + geom_hline(yintercept = .05, color = 'red') +
  geom_text_repel(aes(symbol, p.value, label=symbol), data = filter(results3,  p.value <.05 )) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("Daily percent change in volume  ~ sentiment score + counts for all stocks") +
  theme(legend.position = "none") 
```

3. We fit the linear regression model for _**hourly stock price change**_ by sentiment score and tweet counts. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#Fit linear models by hourly percent change in price ~ avg_score + tweet_counts 
fits2 <- dat %>%
  group_by(symbol) %>%
  do(mod = lm(scale_pchange_price_change ~ scale_pchange_avg_score + scale_pchange_tweet_counts, data = .))

tidy(fits2, mod) %>% filter(p.value < 0.05 & term != '(Intercept)') %>% kable

results2 <- tidy(fits2, mod) %>% filter(p.value < 0.05 & term != '(Intercept)') #filter(term=='scale_pchange_avg_score') 

#Plot p-values and see symbols that have significant results
results2 %>% 
  ggplot(aes(symbol, p.value, color=symbol)) + geom_point() + geom_hline(yintercept = .05, color = 'red') +
  geom_text_repel(aes(symbol, p.value, label=symbol), data = filter(results2,  p.value <.05 )) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("Hourly percent change in price  ~ sentiment score + counts for all stocks") +
  theme(legend.position = "none") 
```

4. We fit the linear regression model for _**hourly stock volume change**_ by sentiment score and tweet counts. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#Fit linear models by hourly percent change in volume ~ avg_score + tweet_counts 
fits1 <- dat %>%
  group_by(symbol) %>%
  do(mod = lm(scale_pchange_volume_change ~ scale_pchange_avg_score + scale_pchange_tweet_counts, data = .))

tidy(fits1, mod) %>% filter(p.value < 0.05 & term != '(Intercept)') %>% kable

results1 <- tidy(fits1, mod) %>% filter(p.value < 0.05 & term != '(Intercept)') #filter(term=='scale_pchange_avg_score') 

#Plot p-values and see symbols that have significant results
results1 %>% 
  ggplot(aes(symbol, p.value, color=symbol)) + geom_point() + geom_hline(yintercept = .05, color = 'red') +
  geom_text_repel(aes(symbol, p.value, label=symbol), data = filter(results1,  p.value <.05 )) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("Hourly percent change in volume  ~ sentiment score + counts for all stocks") +
  theme(legend.position = "none") 
```

5. We fit the linear regression model for _**previous day close price** using previous day's sentiment scores and counts. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#Fit linear models for matching prvClose ~ avg_score + tweet_counts 
fits5 <- dat.prevClose %>%
  group_by(symbol) %>%
  do(mod = lm(prvClose ~ avg_score + tweet_counts, data = .))

tidy(fits5, mod) %>% filter(p.value < 0.05 & term != '(Intercept)') %>% kable

results5 <- tidy(fits5, mod) %>% filter(p.value < 0.05 & term != '(Intercept)') #filter(term=='avg_score') 

#Plot p-values and see symbols that have significant results
results5 %>% 
  ggplot(aes(symbol, p.value, color=symbol)) + geom_point() + geom_hline(yintercept = .05, color = 'red') +
  geom_text_repel(aes(symbol, p.value, label=symbol), data = filter(results5,  p.value <.05 )) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("Daily prvClose ~ avg_score + tweet_counts for all stocks") +
  theme(legend.position = "none") 
```

6. Lastly we fit the linear regression model for _**daily stock price** using the previous close price, sentiment score and tweet counts.  

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#Fit linear models by daily stock price ~ prvClose + avg_score + tweet_counts
fits6 <- dat.byday %>%
  group_by(symbol) %>%
  do(mod = lm(price ~ prvClose + avg_score + tweet_counts, data = .))

tidy(fits6, mod) %>% filter(p.value < 0.05 & term != '(Intercept)') %>% kable

results6 <- tidy(fits6, mod) %>% filter(p.value < 0.05 & term != '(Intercept)') #filter(term=='avg_score') 

#Plot p-values and see symbols that have significant results
results6 %>% 
  ggplot(aes(symbol, p.value, color=symbol)) + geom_point() + geom_hline(yintercept = .05, color = 'red') +
  geom_text_repel(aes(symbol, p.value, label=symbol), data = filter(results6,  p.value <.05 )) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("Daily price ~ prvClose + avg_score + tweet_counts for all stocks") +
  theme(legend.position = "none") 
  
```

#### Conclusion

1. Throghout different models, including daily and hourly price change we noticed that sentiment score can be a significant predictor for some symbols. But for most symbols, tweet counts was a more significant predictor. 

2. The histogram of sentiment scores varied from -5 to 5 and with this limited change we are trying to model the market. We need more advanced natural language processing techniques to determine whether it is a positive tweet or negative tweet. 

3. The users who are tweeting can have a significant impact on the model. If these users are market players, their tweets will carry more weight that an uneducated user of the markets. We did not gather StockTwits user data for this project but can be considered in future.

4. When we included the previous close price in the model along with sentiment score and tweet counts, we noticed that it was a significant predictor for all symbols. This is not very surprising as markets do not swing on a daily basis and instead of simple linear regression model, more advanced algorithms like LDA or nearest K-neighbour algorthms should be considered for modeling based on previous close price to predict the stock markets. 