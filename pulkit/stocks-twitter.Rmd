---
title: "stocks-twitter"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## This file is only used for preliminary analysis. Final results can be accessed in tweets-stocks-final.Rmd

```{r }
library(dplyr)
library(readr)
library(lubridate)
library(stringr)
library(ggplot2)
library(gridExtra)
library(broom)
library(ggrepel)
# Following is the list of all the stocks which we have downloaded the tweets and the stocks from Yahoo finance
tickers_symbols <- c("GILD","EIX","GS","AMZN", "RKUS","AAPL","GRPN","XIV","YHOO","VA","MSFT","TSLA","BSX","NVDA","ORCL","EW","CPGX","MRK","V","BXLT","FOXA","ERIC","AVP","TWX","CMCSA","XRX","WY","GNCA","WBA","MO","MA","FOLD","TLT","SNY","RTN","UTX","LOW","MAS","GPT","RICE","IBM","KHC","CDNS","ANTM","HD","INO","OCLR","LULU","SABR","DYN","AXLL","WEN","COH","GOOG","FB","TWTR","XOM","PSX","VLO","PGR","CINF","FAF","JBLU","DAL","HA","ACN","INFY","CTSH")
# Sectors associate with each stock
sectors <- c("Healthcare","Utilities","Financial","Services","Technology","Consumer Goods","Technology","Financial","Technology","Services","Technology","Consumer Goods","Healthcare","Technology","Technology","Healthcare","Basic Materials","Healthcare","Financial","Healthcare","Services","Telecommunications","Consumer Goods","Services","Services","Technology","Industrial Goods","Healthcare","Services","Consumer Goods","Financial","Healthcare",
             "Financial","Healthcare","Industrial Goods","Industrial Goods","Services","Industrial Goods","Financial","Basic Materials","Technology","Consumer Goods","Technology","Healthcare","Services","Healthcare","Technology","Consumer Goods","Technology","Utilities","Basic Materials","Services","Consumer Goods","Technology","Technology","Technology","Basic Materials","Basic Materials","Basic Materials","Financial","Financial","Financial",
             "Services-Airlines","Services-Airlines","Services-Airlines","Technology","Technology","Technology")

selectively_analyzed_symbols <- c("GS","IBM","EIX")
symbols_EDA <- c("AAPL","EIX","GS","IBM","YHOO","MSFT","TSLA","GOOG","FB")
ticker_sector <- data.frame(symbol = tickers_symbols, sector = sectors)

existingStocksDataLocation <- "/code/CSCIE-107/E107project/pulkit/yahoo-finance.RData"
existingTweetsDataLocation <- "/code/CSCIE-107/E107project/pulkit/twitter.RData"

load(existingTweetsDataLocation)
load(existingStocksDataLocation)
# Change the column name to symbol
colnames(symbols)[2] <- "symbol"
#conver the date to proper format
convert_24Time <- function(x){
  ret <- ''
    splitVector<- strsplit(x,':')
    ret <- sapply(splitVector, function(z){
      if(str_count(z[2],'AM')){
        paste( z[1],':', str_replace(z[2],'AM','') ,sep = '')
      }else{
        paste( ifelse(as.numeric(z[1]) == 12,12, as.numeric(z[1])+12 ),':', str_replace(z[2],'PM','') ,sep = '')
      }
    })
  return(ret)
}

#Function to normalize the values
normalize<-function(m){
  (m - mean(m))/sd(m)
}

#Function to find %age change between consecutive values
pcchange=function(x,lag=1) {
  c(diff(x,lag))/x
}
# Data obtained from Yahoo finance is in EST timezone.
# The date we recieve from yahoo finance is of the form 2:54PM, let us convert it into 14:54 and take away the PM part
stocks_est<- stocks %>% 
  mutate(date_time = paste(lastTradeDate,convert_24Time(lastTradeTime))) %>%
  inner_join(ticker_sector)

# Now let us use lubridate and convert to date_time
stocks_est <- stocks_est %>% mutate(date_timelb = mdy_hm(date_time), daysHigh = as.numeric(daysHigh), daysLow = as.numeric(daysLow), open = as.numeric(open), prvClose= as.numeric(prvClose))
# The trading hours vary from 9:30 a.m. to 4:00 p.m EST. After the tradinng hours when we query the api it keeps on returning the values for that day. 
nrow(stocks_est)
# Let us get the distict values for all the stocks
stocks_est <- distinct(stocks_est) 
nrow(stocks_est)
# Let us make a dataset of the dailyStock data 
dailyStockData <- stocks_est %>% 
  mutate(day = mdy(lastTradeDate)) %>% 
  group_by(day,symbol) %>% 
  filter(volume == max(volume)) 
# We started collecting data from April 6th. Let us visualize the price at the end of each hour with the 
# no of tweets for the stock with maximum number of stocks in each sector
# create a dataset for hourly avg price of each stock.
hourly_avg_stockPriceChng <- stocks_est %>% 
  mutate(day_hr = make_datetime(year = year(date_timelb), month = month(date_timelb), day = day(date_timelb), hour=hour(date_timelb))) %>% 
  group_by(symbol,day_hr) %>% summarise(avgPrice = mean(price)) %>%
  group_by(symbol) %>% arrange(day_hr)  %>%
  mutate(prcChange = pcchange(avgPrice)) %>% ungroup()

# We know that the stock markets are closed on weekends let us find find the trading days
tradingDays <- stocks_est %>% mutate(day = mdy(lastTradeDate)) %>% ungroup() %>% select(day) %>% distinct(day)
# create a dataset for hourly number of tweets for each stock
# Convert the date to New_york timezone as all the stock prices are also available in that timezone
# Tweets are stored in tweets dataset, and the symbols in the tweets are stored in the symbols dataset.
# Join the tweets_est dataset with symbols dataset and filter the data set to only contain required stock symbols
# Filter out tweets only for the selective days
tweets_est <- mutate(tweets, date_timelb = as.POSIXct(as.numeric(time_stamp)/1000, origin="1970-01-01",tz = "America/New_York")) %>% 
  mutate(day_hr = make_datetime(year = year(date_timelb), month = month(date_timelb), day = day(date_timelb), hour=hour(date_timelb)),dt = date(date_timelb)) %>%
  filter(dt %in% tradingDays$day) %>% 
  inner_join(symbols) %>% 
  inner_join(ticker_sector) 

# Let us see the distribuiton of tweets across each symbol on log scale
tweets_est %>% group_by(symbol) %>% 
  summarise(count = n()) %>% inner_join(ticker_sector) %>%
  ggplot(aes(symbol, log(count), color=sector)) + geom_point() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  ggtitle('Distribution of tweets') + xlab('Symbols')+ylab('Count(log scale) ')

# Let us see the distribution of tweets across each day 
tweets_est %>% mutate(dt = mday(dt)) %>% group_by(dt) %>% 
  summarise(count = n()) %>% 
  ggplot(aes(dt, log(count))) + geom_point() +
  ggtitle('# of Tweets per day For April') + xlab('Date')+ylab('Count(log scale) ')

# Let us calculate %age change in number of tweets grouped per hour for every symbol over the entire date range
hourly_tweetsCount_changePer <- tweets_est %>% group_by(symbol,day_hr) %>% 
  summarise(count = n()) %>%
  group_by(symbol) %>% arrange(day_hr) %>%
  mutate(prcCountChange = pcchange(count)) %>% ungroup() 

# Let us plot the price change with tweets count change for a few stocks

filter(hourly_tweetsCount_changePer , symbol %in% symbols_EDA) %>% 
  inner_join(hourly_avg_stockPriceChng) %>% 
  ggplot()+
  geom_point(aes(day_hr,log(prcChange),color='%age Price Change')) + 
  geom_point(aes(day_hr,log(prcCountChange), color='%age TweetCount Change')) +
  facet_wrap(~symbol,scales = "free") + ggtitle(" Change in %ages of Tweets and Price") +
  xlab('%age change') + ylab('date')

# From the above we can see that there are some points where the higher price variation also has higher number of tweets.

####################### Modelling related code #############################
# Let us model number of tweets per day for the selected stocks with closing price, volume of stocks and price change.
# We will be standardizing all the elements 
selective_tweets_est <- filter(tweets_est, symbol %in% selectively_analyzed_symbols)
df_stocktweetsperday <-  selective_tweets_est %>% 
  group_by(symbol,dt) %>% summarize(count = n()) %>% ungroup() %>% 
  group_by(symbol) %>% mutate_each(funs(normalize), count) 

# Standardize the volume for each of the choosen currency pairs
dailyStockData <- filter(dailyStockData, symbol %in% selectively_analyzed_symbols)  %>% 
  group_by(symbol) %>% mutate_each(funs(normalize), volume) 

# Let us join this with dailyStockData dataset

df_stocktweetsperday <- inner_join(df_stocktweetsperday,dailyStockData, by=c("symbol"="symbol","dt"="day")) %>% 
  select(symbol, dt, count,price,volume, open, sector) %>%
  mutate(prcChange = open - price) %>% mutate(absPrcChange = abs(prcChange))  # price is the closing price so we calculate the closing price for the day

fits <- df_stocktweetsperday %>%
  group_by(symbol) %>%
  do(mod = lm(volume ~ count, data = .))

results <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='count')
colnames(results)[6]<- 'pval'

p1<- ggplot(results, aes(symbol, pval, color=symbol)) + geom_point() + geom_hline(yintercept = .05, color = 'red') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("volume~count model")

results %>% select(symbol,pval,`conf.low`, `conf.high`)
# Let us see the list
#results[with(results, order(pval)), ] %>% print(n=70)

# Let us model closing price w.r.t count of tweets
fits <- df_stocktweetsperday %>%
  group_by(symbol) %>%
  do(mod = lm(price ~ count, data = .))

results <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='count') 
colnames(results)[6]<- 'pval'
p2 <- ggplot(results, aes(symbol,pval, color=symbol))+geom_point()+geom_hline(yintercept = .05, color='red')+ 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+ ggtitle("Closing Price~count model")
results %>% select(symbol,pval,`conf.low`, `conf.high`)
# Let us see the list
#results[with(results, order(pval)), ] %>% print(n=70)

# Let us model priceChange w.r.t count of tweets
fits <- df_stocktweetsperday %>%
  group_by(symbol) %>%
  do(mod = lm(prcChange ~ count, data = .))

results <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='count') 
colnames(results)[6]<- 'pval'
# Let us see the list
#results[with(results, order(pval)), ] %>% print(n=70)
p3<- ggplot(results, aes(symbol,pval, color=symbol))+geom_point()+geom_hline(yintercept = .05, color='red')+ 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+ ggtitle("Price Change~count model")
results %>% select(symbol,pval,`conf.low`, `conf.high`)

grid.arrange(p1, p2, p3, nrow=1, top = "P-Value for standardized")

# Let us model absolute priceChange w.r.t count of tweets
#fits <- df_stocktweetsperday %>%
 # group_by(symbol) %>%
#  do(mod = lm(absPrcChange ~ count, data = .))

#results <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='count') %>% inner_join(ticker_sector, by=c("symbols"="symbol"))
#colnames(results)[6]<- 'pval'
#ggplot(results, aes(symbols,pval, color=symbols))+geom_point()+geom_hline(yintercept = .05, color='red')+ 
#  theme(axis.text.x = element_text(angle = 90, hjust = 1))+ ggtitle("P-values for Abs. Price Change~count model")


######################### Below section is for sentiment analysis #######
# Load a list of positive and -ve words 
pos <- scan('/code/CSCIE-107/E107project/pulkit/opinion-lexicon-English/positive-words.txt', what='character', comment.char=';') #folder with positive dictionary
neg <- scan('/code/CSCIE-107/E107project/pulkit/opinion-lexicon-English/negative-words.txt', what='character', comment.char=';') #folder with negative dictionary
pos.words <- c(pos, 'upgrade')
neg.words <- c(neg, 'wtf', 'wait', 'waiting', 'epicfail')

score.sentiment <- function(sentences, pos.words, neg.words, .progress='none')
{
  scores <- plyr::laply(sentences, function(sentence, pos.words, neg.words){
    sentence <- gsub('[[:punct:]]', "", sentence)
    sentence <- gsub('[[:cntrl:]]', "", sentence) 
    sentence <- gsub('\\d+', "", sentence)
    sentence <- tolower(sentence)
    word.list <- str_split(sentence, '\\s+')
    words <- unlist(word.list)
    pos.matches <- match(words, pos.words)
    neg.matches <- match(words, neg.words)
    pos.matches <- !is.na(pos.matches)
    neg.matches <- !is.na(neg.matches)
    score <- sum(pos.matches) - sum(neg.matches)
    return(score)
  }, pos.words, neg.words, .progress=.progress)
  scores.df <- data.frame(score=scores, text=sentences)
  return(scores.df)
}
# calculate the sentiment score
scores <- score.sentiment(selective_tweets_est$text, pos.words, neg.words)
stat <- scores
# add the timestamp of the score
stat$created <- selective_tweets_est$date_timelb
# add the id of the str
stat$id_str <- selective_tweets_est$id_str
# classify them to positive/nagative 
stat <- mutate(stat, tweet=ifelse(stat$score > 0, 'positive', ifelse(stat$score < 0, 'negative', 'neutral')))
by.tweet <- group_by(stat, tweet, created) %>% summarise( number=n())

# Let us see the histogram of scores
hist(stat$score)

# Let us group the sentiment score on hourly basis and then standardize them
stat_symbol <- filter(symbols,symbol %in% selectively_analyzed_symbols) %>% inner_join(stat)
normalized_scores<- stat_symbol %>% 
  mutate(day = as_date(created)) %>% 
  group_by(symbol,day) %>% summarise(avgScore = mean(score)) %>% group_by(symbol) %>% 
  mutate_each(funs(normalize), avgScore)

hist(normalized_scores$avgScore)

######## Plot with sentiment score
normalized_scores <- inner_join(normalized_scores,dailyStockData) %>% 
  select(symbol, day, avgScore,price,volume, open, sector) %>%
  mutate(prcChange = open - price) %>% mutate(absPrcChange = abs(prcChange)) %>% ungroup() # price is the closing price so we calculate the closing price for the day

fits <- normalized_scores %>%
  group_by(symbol) %>%
  do(mod = lm(volume ~ avgScore, data = .))

results <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='avgScore')
colnames(results)[6]<- 'pval'

p1 <- ggplot(results, aes(symbol, pval, color=symbol)) + geom_point() + geom_hline(yintercept = .05, color = 'red') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("P-values for volume~Score model")
results %>% select(symbol,pval,`conf.low`, `conf.high`)

# Let us model closing price w.r.t count of tweets
fits <- normalized_scores %>%
  group_by(symbol) %>%
  do(mod = lm(price ~ avgScore, data = .))

results <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='avgScore') 
colnames(results)[6]<- 'pval'
p2<- ggplot(results, aes(symbol,pval, color=symbol))+geom_point()+geom_hline(yintercept = .05, color='red')+ 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+ ggtitle("P-values for Closing Price~Score model")
results %>% select(symbol,pval,`conf.low`, `conf.high`)
# Let us see the list
#results[with(results, order(pval)), ] %>% print(n=70)

# Let us model priceChange w.r.t count of tweets
fits <- normalized_scores %>%
  group_by(symbol) %>%
  do(mod = lm(prcChange ~ avgScore, data = .))

results <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='avgScore') 
colnames(results)[6]<- 'pval'
# Let us see the list
#results[with(results, order(pval)), ] %>% print(n=70)
p3<- ggplot(results, aes(symbol,pval, color=symbol))+geom_point()+geom_hline(yintercept = .05, color='red')+ 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+ ggtitle("P-values for Price Change~Score model")

grid.arrange(p1, p2, p3, nrow=1, top = "P-Value for standardized")
# Let us model absolute priceChange w.r.t count of tweets
#fits <- normalized_scores %>%
#  group_by(symbols) %>%
#  do(mod = lm(absPrcChange ~ avgScore, data = .))

#results <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='avgScore') 
#colnames(results)[6]<- 'pval'
#ggplot(results, aes(symbols,pval, color=symbols))+geom_point()+geom_hline(yintercept = .05, color='red')+ 
#  theme(axis.text.x = element_text(angle = 90, hjust = 1))+ ggtitle("P-values for Abs. Price Change~Score model")



#####      LAST let us do cross correlation between standardized prices and Twitter sentimenet score 


hourlynormalized_scores<- stat_symbol %>% 
  mutate(day_ms = ymd_hms(created)) %>% 
  mutate(day_hr = make_datetime(year = year(day_ms), month = month(day_ms), day = day(day_ms), hour=hour(day_ms))) %>%
  group_by(symbol,day_hr) %>% summarise(avgScore = mean(score)) %>% group_by(symbol) %>% 
  mutate_each(funs(normalize), avgScore) %>% ungroup()

hourly_avg_stockPriceChng <- hourly_avg_stockPriceChng %>% 
  group_by(symbol) %>% mutate_each(funs(normalize), avgPrice) %>% ungroup()

hourly_stocks_score <- inner_join(hourlynormalized_scores,hourly_avg_stockPriceChng)

hourly_stocks_score %>% mutate(week = isoweek(day_hr)) %>% 
  ggplot()+geom_line(aes(day_hr,avgScore, color="Sentiment Score"))+
  geom_line(aes(day_hr,avgPrice, color="Price")) + facet_wrap(~symbol+week,scales = "free") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))


```
