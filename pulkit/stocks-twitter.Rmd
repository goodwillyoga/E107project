---
title: "stocks-twitter"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

```{r }
library(dplyr)
library(readr)
library(lubridate)
library(stringr)
library(ggplot2)
library(gridExtra)
library(broom)
library(ggrepel)
# Following is the list of all the stocks which we have downloaded the tweets and the stocks from Yahoo finance
tickers_symbols <- c("GILD","EIX","GS","AMZN", "RKUS","AAPL","GRPN","XIV","YHOO","VA","MSFT","TSLA","BSX","NVDA","ORCL","EW","CPGX","MRK","V","BXLT","FOXA","ERIC","AVP","TWX","CMCSA","XRX","WY","GNCA","WBA","MO","MA","FOLD","TLT","SNY","RTN","UTX","LOW","MAS","GPT","RICE","IBM","KHC","CDNS","ANTM","HD","INO","OCLR","LULU","SABR","DYN","AXLL","WEN","COH","GOOG","FB","TWTR","XOM","PSX","VLO","PGR","CINF","FAF","JBLU","DAL","HA","ACN","INFY","CTSH")
# Sectors associate with each stock
sectors <- c("Healthcare","Utilities","Financial","Services","Technology","Consumer Goods","Technology","Financial","Technology","Services","Technology","Consumer Goods","Healthcare","Technology","Technology","Healthcare","Basic Materials","Healthcare","Financial","Healthcare","Services","Telecommunications","Consumer Goods","Services","Services","Technology","Industrial Goods","Healthcare","Services","Consumer Goods","Financial","Healthcare",
             "Financial","Healthcare","Industrial Goods","Industrial Goods","Services","Industrial Goods","Financial","Basic Materials","Technology","Consumer Goods","Technology","Healthcare","Services","Healthcare","Technology","Consumer Goods","Technology","Utilities","Basic Materials","Services","Consumer Goods","Technology","Technology","Technology","Basic Materials","Basic Materials","Basic Materials","Financial","Financial","Financial",
             "Services-Airlines","Services-Airlines","Services-Airlines","Technology","Technology","Technology")

ticker_sector <- data.frame(symbol = tickers_symbols, sector = sectors)
table(ticker_sector$sector)
# We have only 2 stocks in Utilities and 1 in telecommunication, we will not use them for sectorwise analysis

existingStocksDataLocation <- "/code/CSCIE-107/E107project/pulkit/yahoo-finance.RData"
existingTweetsDataLocation <- "/code/CSCIE-107/E107project/pulkit/twitter.RData"
null_value <- 0
load(existingTweetsDataLocation)
load(existingStocksDataLocation)

#conver the date to proper format
convert_24Time <- function(x){
  ret <- ''
    splitVector<- strsplit(x,':')
    ret <- sapply(splitVector, function(z){
      if(str_count(z[2],'AM')){
        paste( z[1],':', str_replace(z[2],'AM','') ,sep = '')
      }else{
        paste( ifelse(as.numeric(z[1]) == 12,12, as.numeric(z[1])+12 ),':', str_replace(z[2],'PM','') ,sep = '')
      }
    })
  return(ret)
}

# Function to download S&P 500 data from yahoo finance
sandp500 <-function(){
  url <- 'http://real-chart.finance.yahoo.com/table.csv?s=%5EGSPC&a=03&b=7&c=2016&'
  url <- paste(url,'d=',(month(now()) -1), '&e=',day(now()),'&f=2016&g=d&ignore=.csv', sep = '')
  read_csv(url, col_names = FALSE)
}

# normalize function
normalit<-function(m){
  (m - mean(m))/sd(m)
}

# Data obtained from Yahoo finance is in EST timezone.
# The date we recieve from yahoo finance is of the form 2:54PM, let us convert it into 14:54 and take away the PM part
tmp1<- stocks %>% mutate(date_time = paste(lastTradeDate,convert_24Time(lastTradeTime)))

# Add the sectors column to the dataset
tmp1 <- inner_join(tmp1,ticker_sector)

# Now let us use lubridate and convert to date_time
tmp1 <- tmp1 %>% mutate(date_timelb = mdy_hm(date_time), daysHigh = as.numeric(daysHigh), daysLow = as.numeric(daysLow), open = as.numeric(open), prvClose= as.numeric(prvClose))
# The trading hours vary from 9:30 a.m. to 4:00 p.m EST. After the tradinng hours when we query the api it keeps on returning the values for that day. 
nrow(tmp1)
# Let us get the distict values for all the stocks
tmp1 <- distinct(tmp1)
nrow(tmp1)

# We started collecting data from April 6th. Let us look at the data for April 7th and see how we can find out the days high/low and volume from this, we will take the yahoo stock to look at this
tmp1 %>% filter(symbol == 'YHOO') %>% mutate(day = mdy(lastTradeDate)) %>% group_by(day) %>% filter(volume == max(volume)) %>% ggplot(aes(date_time,price,size = volume)) + geom_point() 

# Some things to see, the prvClose is always the last price that we see for the previous day. The time is always the 4:00pm.  We can also see that there are a few missing days 9/10, 16/17, 23/24 
# These are weekends and the markets remain close on these days

# Let us make plots of price variations of stocks based on there sectors for the entire period of days in the similar fashion. We will take out the data for April 6th as we started collecting data later that day and do not have large number of data points for it
dailyStockData <- tmp1 %>% mutate(day = mdy(lastTradeDate)) %>% group_by(day,symbol) %>% filter(volume == max(volume) & day != ymd("2016-04-06")) 
# We know that we have the largest number of technology sector stock let us plot them together.
dailyStockData %>% filter(sector == 'Technology') %>% ggplot(aes(day,price, group=symbol, color = symbol)) + geom_line()
# Let us plot all the other stocks together 
dailyStockData %>% filter(!sector %in% c('Technology',"Utilities","Telecommunications")) %>% ggplot(aes(day,price, group=symbol, color = symbol)) + geom_line()
tradingDays <- unique(dailyStockData$day)
# let us make another dataset of hourly stock prices, we will take the last entry of the hour and use the price corresponding to that entry for that hour
hourlyStockData <- tmp1 %>% mutate(day = mdy(lastTradeDate), hr = hour(date_timelb)) %>% group_by(symbol,day,hr) %>% filter(volume == max(volume)) 

# Let us plot the hourly plot of the data for yahoo stocks over the entire time duration
hourlyStockData %>% filter(symbol == 'YHOO') %>% ggplot(aes(date_timelb, price))+geom_line()

####################### Below is specific to tweets #############################
# Convert the date to New_york timezone as all the stock prices are also available in that timezone
tweets_tmp <- mutate(tweets, date_timelb = as.POSIXct(as.numeric(time_stamp)/1000, origin="1970-01-01",tz = "America/New_York")) %>% mutate(dt = date(date_timelb), hr = hour(date_timelb)) 

# Let us do EDA on the tweets
tweets_tmp %>% group_by(user_id) %>% summarize(count=n()) %>% filter(count >5) %>% ggplot(data=., aes(log(count)))+geom_histogram()

# Let us see who are the top 20 authors, and also view the follower count and friend count for them
tweets_tmp %>% group_by(user_id) %>% summarize(count=n()) %>% filter(count >5) %>% inner_join(users) %>% arrange(desc(count)) %>% print(n=20)

# Let us see number of tweets by company, this data is stored in symbols, Each tweet can have multiple symbols
inner_join(tweets_tmp,symbols) %>% filter(symbols %in% tickers_symbols) %>% group_by(symbols) %>% summarize(count = n())  %>% ggplot(aes(symbols, count)) + geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = 1))
# Let us see number of tweets per day
inner_join(tweets_tmp,symbols) %>% filter(symbols %in% tickers_symbols) %>% mutate(day = day(date_timelb)) %>% group_by(day) %>% summarize(count = n()) %>% ggplot(aes(day, count)) + geom_point() + theme(axis.text.x = element_text(angle = 90, hjust = 1))

df_tweetsperday <- inner_join(tweets_tmp,symbols) %>% inner_join(ticker_sector, by=c("symbols"="symbol")) %>% group_by(dt) %>% summarize(count = n()) %>% filter(dt %in% tradingDays)
df_tweetsperday$scaled <- scale(df_tweetsperday$count, center = TRUE, scale = TRUE)
  
df_stocktweetsperday <-  inner_join(tweets_tmp,symbols) %>% inner_join(ticker_sector, by=c("symbols"="symbol")) %>% group_by(symbols,dt) %>% summarize(count = n()) 
df_stocktweetsperday <- df_stocktweetsperday %>% group_by(symbols) %>% mutate_each(funs(normalit), count) 

#df_stocktweetsperday$scaled <- scale(df_stocktweetsperday$count, center = TRUE, scale = TRUE)
# Looks like normal distribution
hist(df_stocktweetsperday$count)
# Let us join this with dailyStockData dataset

df_stocktweetsperday <- inner_join(df_stocktweetsperday,dailyStockData, by=c("symbols"="symbol","dt"="day")) %>% 
  select(symbols, dt, count,price,volume, open, sector) %>%
  mutate(prcChange = open - price) %>% mutate(absPrcChange = abs(prcChange))  # price is the closing price so we calculate the closing price for the day

fits <- df_stocktweetsperday %>%
  group_by(symbols) %>%
  do(mod = lm(volume ~ count, data = .))

results <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='count') %>% inner_join(ticker_sector, by=c("symbols"="symbol"))
colnames(results)[6]<- 'pval'

ggplot(results, aes(symbols, pval, color=sector)) + geom_point() + geom_hline(yintercept = .05, color = 'red') + 
  geom_text_repel(aes(symbols, pval, label=symbols), data = filter(results,  pval <.05 )) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("P-values for volume~count model")
# Let us see the list
#results[with(results, order(pval)), ] %>% print(n=70)

# Let us model closing price w.r.t count of tweets
fits <- df_stocktweetsperday %>%
  group_by(symbols) %>%
  do(mod = lm(price ~ count, data = .))

results <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='count') %>% inner_join(ticker_sector, by=c("symbols"="symbol"))
colnames(results)[6]<- 'pval'
ggplot(results, aes(symbols,pval, color=sector))+geom_point()+geom_hline(yintercept = .05, color='red')+ 
  geom_text_repel(aes(symbols, pval, label=symbols), data = filter(results,  pval <.05 )) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+ ggtitle("P-values for Closing Price~count model")

# Let us see the list
#results[with(results, order(pval)), ] %>% print(n=70)

# Let us model priceChange w.r.t count of tweets
fits <- df_stocktweetsperday %>%
  group_by(symbols) %>%
  do(mod = lm(prcChange ~ count, data = .))

results <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='count') %>% inner_join(ticker_sector, by=c("symbols"="symbol"))
colnames(results)[6]<- 'pval'
# Let us see the list
#results[with(results, order(pval)), ] %>% print(n=70)
ggplot(results, aes(symbols,pval, color=sector))+geom_point()+geom_hline(yintercept = .05, color='red')+ 
  geom_text_repel(aes(symbols, pval, label=symbols), data = filter(results,  pval <.05 )) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+ ggtitle("P-values for Price Change~count model")

######################### Below section is for sentiment analysis #######
# Let us see the list
#results[with(results, order(pval)), ] %>% print(n=70)





# Load a list of positive and -ve words 
pos <- scan('/code/CSCIE-107/E107project/pulkit/opinion-lexicon-English/positive-words.txt', what='character', comment.char=';') #folder with positive dictionary
neg <- scan('/code/CSCIE-107/E107project/pulkit/opinion-lexicon-English/negative-words.txt', what='character', comment.char=';') #folder with negative dictionary
pos.words <- c(pos, 'upgrade')
neg.words <- c(neg, 'wtf', 'wait', 'waiting', 'epicfail')

score.sentiment <- function(sentences, pos.words, neg.words, .progress='none')
{
  scores <- plyr::laply(sentences, function(sentence, pos.words, neg.words){
    sentence <- gsub('[[:punct:]]', "", sentence)
    sentence <- gsub('[[:cntrl:]]', "", sentence) 
    sentence <- gsub('\\d+', "", sentence)
    sentence <- tolower(sentence)
    word.list <- str_split(sentence, '\\s+')
    words <- unlist(word.list)
    pos.matches <- match(words, pos.words)
    neg.matches <- match(words, neg.words)
    pos.matches <- !is.na(pos.matches)
    neg.matches <- !is.na(neg.matches)
    score <- sum(pos.matches) - sum(neg.matches)
    return(score)
  }, pos.words, neg.words, .progress=.progress)
  scores.df <- data.frame(score=scores, text=sentences)
  return(scores.df)
}
# calculate the sentiment score
scores <- score.sentiment(tweets$text, pos.words, neg.words)
stat <- scores
# add the timestamp of the score
stat$created <- tweets_tmp$date_timelb
# add the id of the str
stat$id_Str <- tweets_tmp$id_str
# classify them to positive/nagative 
stat <- mutate(stat, tweet=ifelse(stat$score > 0, 'positive', ifelse(stat$score < 0, 'negative', 'neutral')))
by.tweet <- group_by(stat, tweet, created) %>% summarise( number=n())

ggplot(by.tweet, aes(created, number)) + geom_line(aes(group=tweet, color=tweet), size=2) +
  geom_point(aes(group=tweet, color=tweet), size=4) +
  theme(text = element_text(size=18), axis.text.x = element_text(angle=90, vjust=1)) 

hist(stat$score)

View(scores)

```
