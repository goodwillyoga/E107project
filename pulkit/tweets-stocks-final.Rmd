---
title: "Analysis of Tweets(volume/sentiments) with Stocks volume and Prices"
output: html_document
---

```{r , echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(readr)
library(lubridate)
library(stringr)
library(ggplot2)
library(gridExtra)
library(broom)
library(ggrepel)
# Disable logging of all warning messages
options(warn=-1)
#install.packages("RColorBrewer")
#install.packages("portfolio")
library(RColorBrewer)
library(portfolio)
library(knitr)
theme_set(theme_bw(base_size = 14))
```
#Background and Motivation
Twitter is a widely used online social media. Here we analyze 70 stocks across different industries and apply a linear regression to predict stock market indicators, using Twitter data. During the time window between April 6th to April 29th we were able to collect 339345 tweets. We find that there are a few stocks out of the choosed stocks for which the 3 stock predictors can be modelled based on the count of tweets. We then focus specifically on the 3 stocks high beta (GS - Financials), med beta (IBM - Info Tech) and low beta (EIX - utilities) to see if this model can successfuly be used to predict market indicators. We do the above analysis both for daily and hourly data. Then we move towards analyzing the sentiments from the stocks and see if we can model the the predictors with the average sentiment score on the hourly basis. Coming towards the end we try to visualize the pattern between the daily sentiment score with the closing prices and see that sentiments follow the market movements.

### Data Normalization
We normalize each time series to z-scores on the basis of a local mean and standard deviation to provide a common scale for comparison of our predictors and stock market indicators.

$$Z_({x_{i}}) = \frac{x_{i} - \mu(X)}{\sigma(X)}$$ 

## Data Extraction
We wanted to gather tweets from [Twitter](https://twitter.com/) and stock data from [Yahoo! Finance](http://finance.yahoo.com/). Here we try to explain briefly the steps done for extraction/processing of tweets and stocks data.

![Processing Stages](https://github.com/goodwillyoga/E107project/raw/master/pulkit/Preprocessing-stages.png)

### Twitter Data Extraction and Processing
There are some limitations on the number of requests that can be made to the twitter REST api. So we decided to use the [java streaming api](https://github.com/twitter/hbc) to capture the live tweets and record them into a file twitter.json. We called this component [extractors](https://github.com/goodwillyoga/E107project/tree/master/pulkit/extractors). The file twitter.json was periodically backed-up, gzipped and truncated to continue gathering further data. The compressed data files can be found under [data](https://github.com/goodwillyoga/E107project/tree/master/data) directory.The script [twitter-data-cleaner.R](https://github.com/goodwillyoga/E107project/blob/master/pulkit/twitter-data-cleaner.R) is run on the files. Since this extraction process is very time consuming so this script was added the capability to perform the extraction in append mode, wherein the contents of a new file are appended to an already extracted dataset from previously processed files. The script creates a .RData file [twitter.RData](https://github.com/goodwillyoga/E107project/blob/master/pulkit/twitter.RData) with 4 datasets tweets, users, hashtags and symbols. For further analysis we will be using the tweets and symbols dataset. These datasets are read by [tweeter-score-compute.R](https://github.com/goodwillyoga/E107project/blob/master/pulkit/tweeter-score-compute.R) which mutate the time of each tweets to America/New_York timezone. It computes the sentiment score for each tweet using the [Opinion Lexicon](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), which has a list of 6800 positive/negative words. It stores the sentiment scores in a dataset scores. It also computes 2 additional datasets daily_tweet_score and hourly_tweet_score these datasets contain the normalized avg sentiment scores. It also creates a [word cloud](https://github.com/goodwillyoga/E107project/blob/master/pulkit/wordCloud.png) of the frequently used words in these tweets. It writes the 4 datasets into the file [processed-tweets.RData](https://github.com/goodwillyoga/E107project/blob/master/pulkit/processed-tweets.RData).

### Yahoo Finance Data Extraction and Processing
We wrote a rscript [yahoo-finance-data-extractor.R](https://github.com/goodwillyoga/E107project/blob/master/pulkit/yahoo-finance-data-extractor.R), that extracts the prices of the choosen 70 stocks and appends it into a csv stocks.csv. The file stocks.csv was periodically backed-up, gzipped and truncated to continue gathering further data. The compressed data files can be found under [data](https://github.com/goodwillyoga/E107project/tree/master/data) directory.The script [yahoo-finance-data-processor.R](https://github.com/goodwillyoga/E107project/blob/master/pulkit/yahoo-finance-data-processor.R) is run on these files which combines the records read from the current file with the already processed records and writes it into the file [yahoo-finance.RData](https://github.com/goodwillyoga/E107project/blob/master/pulkit/yahoo-finance.RData). This [yahoo-finance.RData](https://github.com/goodwillyoga/E107project/blob/master/pulkit/yahoo-finance.RData) is then read by [stocks-compute.R](https://github.com/goodwillyoga/E107project/blob/master/pulkit/stocks-compute.R) which creates 3 datasets stocks_est, hourly_stockData and dailyStockData. 

```{r, echo=FALSE, message=FALSE}
# Following is the list of all the stocks which we have downloaded the tweets and the stocks from Yahoo finance
tickers_symbols <- c("GILD","EIX","GS","AMZN", "RKUS","AAPL","GRPN","XIV","YHOO","VA","MSFT","TSLA","BSX","NVDA","ORCL","EW","CPGX","MRK","V","BXLT","FOXA","ERIC","AVP","TWX","CMCSA","XRX","WY","GNCA","WBA","MO","MA","FOLD","TLT","SNY","RTN","UTX","LOW","MAS","GPT","RICE","IBM","KHC","CDNS","ANTM","HD","INO","OCLR","LULU","SABR","DYN","AXLL","WEN","COH","GOOG","FB","TWTR","XOM","PSX","VLO","PGR","CINF","FAF","JBLU","DAL","HA","ACN","INFY","CTSH")
# Sectors associate with each stock
sectors <- c("Healthcare","Utilities","Financial","Services","Technology","Consumer Goods","Technology","Financial","Technology","Services","Technology","Consumer Goods","Healthcare","Technology","Technology","Healthcare","Basic Materials","Healthcare","Financial","Healthcare","Services","Telecommunications","Consumer Goods","Services","Services","Technology","Industrial Goods","Healthcare","Services","Consumer Goods","Financial","Healthcare",
             "Financial","Healthcare","Industrial Goods","Industrial Goods","Services","Industrial Goods","Financial","Basic Materials","Technology","Consumer Goods","Technology","Healthcare","Services","Healthcare","Technology","Consumer Goods","Technology","Utilities","Basic Materials","Services","Consumer Goods","Technology","Technology","Technology","Basic Materials","Basic Materials","Basic Materials","Financial","Financial","Financial",
             "Services-Airlines","Services-Airlines","Services-Airlines","Technology","Technology","Technology")

selectively_analyzed_symbols <- c("GS","IBM","EIX")
ticker_sector <- data.frame(symbol = tickers_symbols, sector = sectors)
```

Load the Processed tweets and stocks data from github
```{r load tweets}
load(url("https://github.com/goodwillyoga/E107project/raw/master/pulkit/processed-tweets.RData"))
```
```{r load stocks}
load(url("https://github.com/goodwillyoga/E107project/raw/master/pulkit/processed-stocks.RData"))
```
```{r, echo=FALSE, message=FALSE}
# Create helper function to normalize the data
normalize<-function(m){
  (m - mean(m))/sd(m)
}
```
### Exploratory Data Analysis
We started collecting the data from 6th of April and collected till 29th April. Let us look at a few of the datasets that we have

```{r}

tail(tweets_est, n=3) %>% select(id_str,text,symbol,sector,date_timelb)%>% kable

tail(dailyStockData, n=3)%>% select(symbol,price,open,volume,date_timelb,sector)%>% kable

```

Let us start with looking at the amount of tweets that we have collected for each symbol on log scale  

```{r,echo=FALSE, message=FALSE, fig.width=9}

tweets_est %>% group_by(symbol) %>% # group on symbol
  summarise(count = n()) %>% inner_join(ticker_sector) %>% # count number of rows for each symbol
  ggplot(aes(symbol, log(count), color=sector)) + geom_point() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  ggtitle('Distribution of Tweets per Symbol') + xlab('Symbols')+ylab('Count(log scale) ')+
  theme(legend.position="bottom")
```

We can see that in general technology companies tend to have much more number of tweets as compared to companies in other sectors e.g AMZN, FB, GOOG etc

Look at distribution of tweets per day
```{r,echo=FALSE, message=FALSE, fig.width=9}
tweets_est %>% group_by(dt) %>% # group on Date
  summarise(count = n())  %>% mutate(wday = lubridate::wday(dt)) %>% # count number of rows for each date
  ggplot(aes(dt, log(count), color=wday)) + geom_point() +
  scale_colour_gradientn(colours=rainbow(7)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  theme(legend.position="bottom")+
  ggtitle('Distribution of Tweets per Day') + xlab('Dates')+ylab('Count(log scale) ')
```

Sunday(Day 1) is color coded as Red, we can see that Saturday's have the least number of tweets followed by Sunday's. and in the middle of the week (Tuesday/Wednessday) we have the maximum number of tweets.

Let us look at the biggest ganners and loosers over the data we have collected and number of tweets for each of them

```{r ,echo=FALSE, message=FALSE, fig.width=9}
# Choose the last row in the dataset, that becomes the closing price for the window 
stockEnd <- dailyStockData %>% 
  group_by(symbol) %>% 
  arrange(date_time)%>% 
  do(tail(., n=1)) %>% select(symbol,price)

# Choose the first row in the dataset, that becomes the opening price for the window, join with stockEnd and find the difference between the price 
priceChangeOverPeriod <- dailyStockData %>% 
  group_by(symbol) %>% 
  arrange(date_time)%>% do(head(., n=1)) %>% 
  select(symbol,price) %>% 
  inner_join(stockEnd, by=c("symbol"="symbol")) %>% 
  mutate(prcChange = `price.y` - `price.x`)

# Join with tweets dataset to get the count of tweets for each stock
joinedDf <- tweets_est %>% 
  group_by(symbol) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  inner_join(priceChangeOverPeriod, by=c("symbol"="symbol"))

# Show the data in the heatmap
map.market(id=joinedDf$symbol, area=joinedDf$count, group=joinedDf$symbol, color=joinedDf$prcChange, main="Stock Price changes and Tweet counts", lab=c(FALSE,TRUE))

```

Let us look at daily closing prices/Volumes for the 3 choosen stocks
```{r,echo=FALSE, message=FALSE, fig.width=9}
# Plot variation of closing price for the 3 chosen stocks
dailyStockData %>% filter(symbol %in% selectively_analyzed_symbols) %>% 
  ggplot(aes(day, price)) + geom_line(aes(color=symbol)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  theme(legend.position="bottom")+
  ggtitle('Varition in Daily Closing Price') + xlab('Dates')+ylab('Closing Price ') + facet_wrap(~symbol,scales = "free")

# Plot variation of daily volume for the 3 choosen stocks
dailyStockData %>% filter(symbol %in% selectively_analyzed_symbols) %>% 
  ggplot(aes(day, volume)) + geom_line(aes(color=symbol)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  theme(legend.position="bottom")+
  ggtitle('Variation of Daily Trading Volume') + xlab('Dates')+ylab('Daily Volume ') + facet_wrap(~symbol,scales = "free")
```

We can see that there is some variation in the daily traded volumes and the prices.

```{r,echo=FALSE, message=FALSE}
#We know that Stock markets are closed on weekends, let us compute the trading days so that we can focus on the trading days only
tradingDays <- stocks_est %>% mutate(day = lubridate::mdy(lastTradeDate)) %>% ungroup() %>% select(day) %>% distinct(day)
```

### Applying Linear Regression
#### Modeling based on count of number of tweets per day
We will start with modeling of daily stock/tweets data and try to do a linear regression to find relation between volume, closing Price and Daily Price Change with number of tweets. We will plot this once for all the stocks and then selectively for 3 choosen stocks. The regression model is

$$ Y_t = \alpha + \beta X_t + \varepsilon_t, $$

Yt represents a stock indicator for day t , Xt represents the related Twitter predictor on day t, α is the intercept, β is the slope, and εt is a random error term for day t.


```{r,echo=FALSE, message=FALSE, fig.width=9}
# Normalized tweets count data is stored in daily_tweet_score, Daily Stock data is present in dailyStockData, 
#let us normalize the volume, closing price and Daily Price change and then join it daily_tweet_score
dailyStockTweetNormalized <- dailyStockData %>% 
  group_by(symbol) %>% 
  mutate_each(funs(normalize), volume) %>% 
  mutate_each(funs(normalize),price) %>% 
  mutate_each(funs(normalize),prcChange) %>% 
  inner_join(daily_tweet_score, by=c("symbol"="symbol", "day"="dt")) %>% select(symbol,price,volume,open,sector,date_timelb,day,prcChange,count,avgScore)

# Fit a model
fits <- dailyStockTweetNormalized %>%
  group_by(symbol) %>%
  do(mod = lm(volume ~ count, data = .))

# Filter out rows where term is count
results_volume_count <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='count') %>% inner_join(ticker_sector)
colnames(results_volume_count)[6]<- 'pval'

# Plot for all stocks and highlight the stocks with pvals <.05
ggplot(results_volume_count, aes(symbol, pval, color=sector)) + 
  geom_point() + geom_hline(yintercept = .05, color = 'red') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  ggtitle("Volume~Count model for Daily data") + xlab("Symbols") + ylab("Pvalues")+
  geom_text_repel(aes(symbol, pval, label=symbol), data = filter(results_volume_count,  pval <.05 )) +
  theme(legend.position="bottom")
```

From the above we can see that there a couple of stocks with p-values <.05 for the volume~count model. Let us model Closing price with Number of tweets for each stock

```{r,echo=FALSE, message=FALSE, fig.width=9}

# Let us model closing price vs count for each stock
fits <- dailyStockTweetNormalized %>%
  group_by(symbol) %>%
  do(mod = lm(price ~ count, data = .))

# Filter out rows where term is count
results_price_count <- tidy(fits,mod, conf.int = TRUE) %>% 
  filter(term=='count') %>% 
  inner_join(ticker_sector)
colnames(results_price_count)[6]<- 'pval'

# Plot for all stocks
ggplot(results_price_count, aes(symbol, pval, color=sector)) + 
  geom_point() + geom_hline(yintercept = .05, color = 'red') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  ggtitle("Closing Price~count model") + xlab("Symbols") + ylab("Pvalues")+
  geom_text_repel(aes(symbol, pval, label=symbol), data = filter(results_price_count,  pval <.05 )) +
  theme(legend.position="bottom")

```

Let us model Daily Price Change with Number of tweets for each stock 

```{r,echo=FALSE, message=FALSE, fig.width=9}

# Let us model Price Change vs count for all stocks
fits <- dailyStockTweetNormalized %>%
  group_by(symbol) %>%
  do(mod = lm(prcChange ~ count, data = .))

# Filter out rows where term is count
results_prcChange_count <- tidy(fits,mod, conf.int = TRUE) %>% 
  filter(term=='count') %>% inner_join(ticker_sector)

#Rename the column
colnames(results_prcChange_count)[6]<- 'pval'

# Plot for all stocks
ggplot(results_prcChange_count, aes(symbol, pval, color=sector)) + 
  geom_point() + geom_hline(yintercept = .05, color = 'red') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  ggtitle("Daily Price Change~count model for all Stocks") + xlab("Symbols") + ylab("Pvalues") + 
  geom_text_repel(aes(symbol, pval, label=symbol), data = filter(results_prcChange_count,  pval <.05 )) +
  theme(legend.position="bottom")
```

From the above 3 plots we can see that we can model Daily volume, Closing Price and Daily Price change for AVP, BSX, XRX, PSX, GILD with number of tweets for these shares with the p-values <.05. Let us look at the selected 3 stocks and try to see if the above linear regression explains something about them.

```{r,echo=FALSE, message=FALSE, fig.width=9, fig.height=3}

# Let us plot specific for the 3 stocks 
p1<- results_volume_count %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  ggplot( aes(symbol, pval, color=symbol)) + geom_point() + 
  geom_hline(yintercept = .05, color = 'red') + xlab("Symbols") + ylab("Pvalues") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("Volume~Count")+ theme(legend.position="bottom")

p2 <- results_price_count %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  ggplot( aes(symbol,pval, color=symbol))+geom_point()+
  geom_hline(yintercept = .05, color='red')+ xlab("Symbols") + ylab("Pvalues") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("ClosingPrc~Count")+ theme(legend.position="bottom")

p3<- results_prcChange_count %>% 
  filter(symbol %in% selectively_analyzed_symbols)  %>% 
  ggplot( aes(symbol,pval, color=symbol))+geom_point()+
  geom_hline(yintercept = .05, color='red')+ xlab("Symbols") + ylab("Pvalues") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+ ggtitle("DailyPrcChn~Count")+ theme(legend.position="bottom")

grid.arrange(arrangeGrob(p1+ 
                           theme(legend.position="none"), p2+ 
                           theme(legend.position="none"), p3+ 
                           theme(legend.position="none"), nrow=1), top = "P-Value for standardized")

# Let us plot the confidence intervals
p1 <- results_volume_count %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  select(symbol,estimate, pval,`conf.low`, `conf.high`) %>% 
  ggplot( aes(x = symbol, y = estimate, color = symbol)) +
  geom_point(size = 2) + ggtitle("Volume~Count")+ xlab("Symbols") + ylab("Conf. Interval") +
  geom_errorbar(aes(ymax = `conf.high`, ymin = `conf.low`))

p2 <-results_price_count %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  select(symbol,estimate, pval,`conf.low`, `conf.high`) %>% 
  ggplot( aes(x = symbol, y = estimate, color = symbol)) +
  geom_point(size = 2) + ggtitle("ClosingPrc~Count")+ xlab("Symbols") + ylab("Conf. Interval") +
  geom_errorbar(aes(ymax = `conf.high`, ymin = `conf.low`))

p3 <-results_prcChange_count %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  select(symbol,estimate, pval,`conf.low`, `conf.high`) %>% 
  ggplot( aes(x = symbol, y = estimate, color = symbol)) +
  geom_point(size = 2) + ggtitle("DailyPrcChn~Count")+ xlab("Symbols") + ylab("Conf. Interval") +
  geom_errorbar(aes(ymax = `conf.high`, ymin = `conf.low`))

grid.arrange(arrangeGrob(p1+ 
                           theme(legend.position="none"), p2+ 
                           theme(legend.position="none"), p3+ 
                           theme(legend.position="none"), nrow=1), top = "Confidence Interval Plots")


```

From the above we can see that Volume and Daily Price change models have p-values <.05 and also there confidence intervals do not include 0, so the relationship is significant. Let us try to look at the regression plots as well

```{r,echo=FALSE, message=FALSE, fig.width=9, fig.height=3}
# Do regression plots for the 3 stocks based on volume~count
dailyStockTweetNormalized %>% filter(symbol %in% selectively_analyzed_symbols) %>% ggplot( aes(count,volume))+geom_point()+geom_smooth(color='red') +facet_wrap(~symbol)

# Do regression plots for the 3 stocks based on Daily Price change~count
dailyStockTweetNormalized %>% filter(symbol %in% selectively_analyzed_symbols) %>% ggplot( aes(count,prcChange))+geom_point()+geom_smooth(color='red') +facet_wrap(~symbol)

```

The plots kind of show us why GS Daily volume and Price change can be represented by a linear regression model.

#### Modeling based on count of number of tweets per day
Let us try to model volume, closing Price and Daily Price Change with sentiment score of the stocks. We start by looking at the popular words that people use in the tweets. 

![Word Cloud](https://github.com/goodwillyoga/E107project/raw/master/pulkit/wordCloud.png)

Let us also look at the distribution of Scores

```{r ,echo=FALSE, message=FALSE, fig.width=9}
qplot(scores$score)
```

We see that this looks like a normal distribution with a lot of tweets having the score of 0.Now let us look at whether we can successfully model based on the sentiment scores. We will first plot for all the stocks and then focus on 3 selective stocks

```{r ,echo=FALSE, message=FALSE, fig.width=9}
# Model volume ~ sentiment score
fits <- dailyStockTweetNormalized %>%
  group_by(symbol) %>%
  do(mod = lm(volume ~ avgScore, data = .))

# Filter out rows where term is avgScore
results_volume <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='avgScore') %>% inner_join(ticker_sector)
#Rename the column
colnames(results_volume)[6]<- 'pval'

# Plot for all stocks
ggplot(results_volume, aes(symbol, pval, color=sector)) + 
  geom_point() + geom_hline(yintercept = .05, color = 'red') + xlab("Symbols") + ylab("P-values") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  ggtitle("Volume~Sentiment-Score model") +
  geom_text_repel(aes(symbol, pval, label=symbol), data = filter(results_volume,  pval <.05 )) +
  theme(legend.position="bottom")

# Let us model closing price vs SentimentScore for each stock
fits <- dailyStockTweetNormalized %>%
  group_by(symbol) %>%
  do(mod = lm(price ~ avgScore, data = .))

results_price <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='avgScore') %>% inner_join(ticker_sector)
colnames(results_price)[6]<- 'pval'

# Plot for all stocks
ggplot(results_price, aes(symbol, pval, color=sector)) + 
  geom_point() + geom_hline(yintercept = .05, color = 'red') + xlab("Symbols") + ylab("P-values") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  ggtitle("Closing Price~Sentiment Score model") +
  geom_text_repel(aes(symbol, pval, label=symbol), data = filter(results_price,  pval <.05 )) +
  theme(legend.position="bottom")

# Let us model Price Change vs Sentimenet Score for all stocks
fits <- dailyStockTweetNormalized %>%
  group_by(symbol) %>%
  do(mod = lm(prcChange ~ avgScore, data = .))

results_prcChange <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='avgScore') %>% inner_join(ticker_sector)
colnames(results_prcChange)[6]<- 'pval'

# Plot for all stocks
ggplot(results_prcChange, aes(symbol, pval, color=sector)) + 
  geom_point() + geom_hline(yintercept = .05, color = 'red') + xlab("Symbols") + ylab("P-values") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  ggtitle("Daily Price Change~Sentiment Score model") +
  geom_text_repel(aes(symbol, pval, label=symbol), data = filter(results_prcChange,  pval <.05 )) +
  theme(legend.position="bottom")

```

From the above plots we can see that for stocks AVP and PSX, we can fit a linear regression model for Daily Volume, Daily Closing Price and Daily Price Change with Average daily sentiment scores. Let us try to look at the behavior for the choosen 3 stocks 

```{r ,echo=FALSE, message=FALSE, fig.width=9, fig.height=3}
# Let us plot specific for the 3 stocks 
p1<- results_volume %>% filter(symbol %in% selectively_analyzed_symbols) %>% 
  ggplot( aes(symbol, pval, color=symbol)) + geom_point() + xlab("Symbols") + ylab("P-values") +
  geom_hline(yintercept = .05, color = 'red') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("Volume~Score")+ theme(legend.position="bottom")

p2 <- results_price %>% filter(symbol %in% selectively_analyzed_symbols)  %>% 
  ggplot( aes(symbol,pval, color=symbol))+geom_point()+ xlab("Symbols") + ylab("P-values") +
  geom_hline(yintercept = .05, color='red')+ 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+ ggtitle("ClosingPrc~Score")+ theme(legend.position="bottom")

p3<- results_prcChange %>% filter(symbol %in% selectively_analyzed_symbols)  %>% 
  ggplot( aes(symbol,pval, color=symbol))+geom_point()+ xlab("Symbols") + ylab("P-values")+
  geom_hline(yintercept = .05, color='red')+ 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+ ggtitle("PrcChange~Score")+ theme(legend.position="bottom")

grid.arrange(arrangeGrob(p1+ 
                           theme(legend.position="none"), p2+ 
                           theme(legend.position="none"), p3+ 
                           theme(legend.position="none"), nrow=1), top = "P-Value for standardized")

```

The p-values for none of the 3 are <.05 so we can say that this linear model is not a good fit. We also had hourly sentiment scores per stock, let us try to model the 3 elements for stocks on the hourly sentiment score and visualize.

```{r ,echo=FALSE, message=FALSE, fig.width=9}

# Create a hourlyStockTweetNormalized dataset which has normalized hourly volume change, Average Price, Hourly price change
hourlyStockTweetNormalized <- hourly_stockData %>% 
  mutate(day = as.Date(day_hr)) %>%
  group_by(symbol,day) %>% mutate(n=n()) %>%  filter(n>1) %>%
  mutate_each(funs(normalize), hourlyVolumeChng) %>% 
  mutate_each(funs(normalize),avgPrice) %>% 
  mutate_each(funs(normalize),prcChange) %>% 
  inner_join(hourly_tweet_score, by=c("symbol"="symbol", "day_hr"="day_hr")) %>% ungroup() %>%
  select(symbol,avgPrice,hourlyVolumeChng,prcChange,sector,day_hr, date_timelb,avgScore)

# Let us model hourlyVolumeChange with hourly Sentiment Score and 

fits <- hourlyStockTweetNormalized %>%
  group_by(symbol) %>%
  do(mod = lm(hourlyVolumeChng ~ avgScore, data = .))

results_volume <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='avgScore') %>% inner_join(ticker_sector)
colnames(results_volume)[6]<- 'pval'

# Plot for all stocks
ggplot(results_volume, aes(symbol, pval, color=sector)) + geom_point() + 
  geom_hline(yintercept = .05, color = 'red') + xlab("Symbols") + ylab("P-values") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("Hourly VolumeChng~Sentiment Score model") +
  geom_text_repel(aes(symbol, pval, label=symbol), data = filter(results_volume,  pval <.05 )) +
  theme(legend.position="bottom")

# Let us model closing hourly avg Price vs Sentiment Score for each stock
fits <- hourlyStockTweetNormalized %>%
  group_by(symbol) %>%
  do(mod = lm(avgPrice ~ avgScore, data = .))

results_price <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='avgScore') %>% inner_join(ticker_sector)
colnames(results_price)[6]<- 'pval'

# Plot for all stocks
ggplot(results_price, aes(symbol, pval, color=sector)) + geom_point() + 
  geom_hline(yintercept = .05, color = 'red') + xlab("Symbols") + ylab("P-values") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("Hourly Price~Sentiment Score model") +
  geom_text_repel(aes(symbol, pval, label=symbol), data = filter(results_price,  pval <.05 )) +
  theme(legend.position="bottom")


# Let us model Hourly Price Change vs Sentiment Score for all stocks
fits <- hourlyStockTweetNormalized %>%
  group_by(symbol) %>%
  do(mod = lm(prcChange ~ avgScore, data = .))

results_prcChange <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='avgScore') %>% inner_join(ticker_sector)
colnames(results_prcChange)[6]<- 'pval'

# Plot for all stocks
ggplot(results_prcChange, aes(symbol, pval, color=sector)) + 
  geom_point() + xlab("Symbols") + ylab("P-values") + 
  geom_hline(yintercept = .05, color = 'red') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("Hourly Price Change~Sentiment Score model") +
  geom_text_repel(aes(symbol, pval, label=symbol), data = filter(results_prcChange,  pval <.05 )) +
  theme(legend.position="bottom")

```

We see in the above 3 plots that number of stocks that can be modeled based on the hourly Sentiment score for the 3 attributes Hourly Volume change, Hourly price and Hourly price change is very few than those for the daily statistics. 

Let us try to plot the Daily Stock Prices and sentiment score for a few stocks

```{r ,echo=FALSE, message=FALSE, fig.width=9}
selectively_analyzed_symbols <- c(selectively_analyzed_symbols, 'AAPL', 'GOOG','AVP')
dailyStockTweetNormalized %>% filter(symbol %in% selectively_analyzed_symbols) %>% mutate(week = lubridate::isoweek(day)) %>% 
  ggplot()+geom_line(aes(day,avgScore, color="Sentiment Score"))+
  geom_line(aes(day,price, color="Price")) + facet_wrap(~symbol,scales = "free") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+ theme(legend.position="bottom")

```

From the above plot we can clearly see that stock price movements are more strongly predictive of twitter sentiment movements. 

## Observations and Conclusions
Some of the takeaways that we can derive based on the data
1) Data shows that there are more number of tweets for technology sector companies then compared to other sectors.
2) We can successively model predictore for Daily volumes, Closing prices and Daily Price variations for some of the stocks like AVP, BSX, XRX, PSX, GILD just based on the number of tweets using Linear Regression. 
3) Linear regression model based on sentiment scores with Daily volumes, Closing prices and Daily Price are not good predictors for these models as p-values >.05.
4) We see a clear follower relationship between Sentiment scores and Stock price movements. Analyzing the trends using Cross correlation may provide some more insights into this. 
