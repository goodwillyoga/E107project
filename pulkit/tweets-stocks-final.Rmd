---
title: "Analysis of Tweets(volume/sentiments) with Stocks volume and Prices"
output: html_document
---

```{r , echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(readr)
library(lubridate)
library(stringr)
library(ggplot2)
library(gridExtra)
library(broom)
library(ggrepel)
# Disable logging of all warning messages
options(warn=-1)
#install.packages("RColorBrewer")
#install.packages("portfolio")
library(RColorBrewer)
library(portfolio)

```
#Background and Motivation
Twitter is a widely used online social media. Here we analyze 70 stocks across different industries and apply a linear regression to predict stock market indicators, using Twitter data. We find that there are a few stocks out of the choosed stocks for which the 3 stock predictors can be modelled based on the count of tweets. We then focus specifically on the 3 stocks high beta (GS - Financials), med beta (IBM - Info Tech) and low beta (EIX - utilities) to see if this model can successfuly be used to predict market indicators. We do the above analysis both for daily and hourly data. Then we move towards analyzing the sentiments from the stocks and see if we can model the the predictors with the average sentiment score on the hourly basis. Coming towards the end we try to visualize the pattern between the daily sentiment score with the closing prices and see that sentiments follow the market movements.

### Data Normalization
We normalize each time series to z-scores on the basis of a local mean and standard deviation to provide a common scale for comparison of our predictors and stock market indicators.

$$Z_({x_{i}}) = \frac{x_{i} - \mu(X)}{\sigma(X)}$$ 

## Data Extraction
We wanted to gather tweets from [Twitter](https://twitter.com/) and stock data from [Yahoo! Finance](http://finance.yahoo.com/). Here we try to explain briefly the steps done for extraction/processing of tweets and stocks data.

![Processing Stages](/code/CSCIE-107/E107project/pulkit/Preprocessing-stages.png)

### Twitter Data Extraction and Processing
There are some limitations on the number of requests that can be made to the twitter REST api. So we decided to use the [java streaming api](https://github.com/twitter/hbc) to capture the live tweets and record them into a file twitter.json. We called this component [extractors](https://github.com/goodwillyoga/E107project/tree/master/pulkit/extractors). The file twitter.json was periodically backed-up, gzipped and truncated to continue gathering further data. The compressed data files can be found under [data](https://github.com/goodwillyoga/E107project/tree/master/data) directory.The script [twitter-data-cleaner.R](https://github.com/goodwillyoga/E107project/blob/master/pulkit/twitter-data-cleaner.R) is run on the files. Since this extraction process is very time consuming so this script was added the capability to perform the extraction in append mode, wherein the contents of a new file are appended to an already extracted dataset from previously processed files. The script creates a .RData file [twitter.RData](https://github.com/goodwillyoga/E107project/blob/master/pulkit/twitter.RData) with 4 datasets tweets, users, hashtags and symbols. For further analysis we will be using the tweets and symbols dataset. These datasets are read by [tweeter-score-compute.R](https://github.com/goodwillyoga/E107project/blob/master/pulkit/tweeter-score-compute.R) which mutate the time of each tweets to America/New_York timezone. It computes the sentiment score for each tweet using the [Opinion Lexicon](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), which has a list of 6800 positive/negative words. It stores the sentiment scores in a dataset scores. It also computes 2 additional datasets daily_tweet_score and hourly_tweet_score these datasets contain the normalized avg sentiment scores. It also creates a [word cloud](https://github.com/goodwillyoga/E107project/blob/master/pulkit/wordCloud.png) of the frequently used words in these tweets. It writes the 4 datasets into the file [processed-tweets.RData](https://github.com/goodwillyoga/E107project/blob/master/pulkit/processed-tweets.RData).

### Yahoo Finance Data Extraction and Processing
We wrote a rscript [yahoo-finance-data-extractor.R](https://github.com/goodwillyoga/E107project/blob/master/pulkit/yahoo-finance-data-extractor.R), that extracts the prices of the choosen 70 stocks and appends it into a csv stocks.csv. The file stocks.csv was periodically backed-up, gzipped and truncated to continue gathering further data. The compressed data files can be found under [data](https://github.com/goodwillyoga/E107project/tree/master/data) directory.The script [yahoo-finance-data-processor.R](https://github.com/goodwillyoga/E107project/blob/master/pulkit/yahoo-finance-data-processor.R) is run on these files which combines the records read from the current file with the already processed records and writes it into the file [yahoo-finance.RData](https://github.com/goodwillyoga/E107project/blob/master/pulkit/yahoo-finance.RData). This [yahoo-finance.RData](https://github.com/goodwillyoga/E107project/blob/master/pulkit/yahoo-finance.RData) is then read by [stocks-compute.R](https://github.com/goodwillyoga/E107project/blob/master/pulkit/stocks-compute.R) which creates 3 datasets stocks_est, hourly_stockData and dailyStockData. 

### Basic Analysis
We started collecting the data from 6th of April and collected till 29th April. Let us start with looking at the amount of tweets that we have collected for each symbol on log scale  

```{r, echo=FALSE, message=FALSE}
# Following is the list of all the stocks which we have downloaded the tweets and the stocks from Yahoo finance
tickers_symbols <- c("GILD","EIX","GS","AMZN", "RKUS","AAPL","GRPN","XIV","YHOO","VA","MSFT","TSLA","BSX","NVDA","ORCL","EW","CPGX","MRK","V","BXLT","FOXA","ERIC","AVP","TWX","CMCSA","XRX","WY","GNCA","WBA","MO","MA","FOLD","TLT","SNY","RTN","UTX","LOW","MAS","GPT","RICE","IBM","KHC","CDNS","ANTM","HD","INO","OCLR","LULU","SABR","DYN","AXLL","WEN","COH","GOOG","FB","TWTR","XOM","PSX","VLO","PGR","CINF","FAF","JBLU","DAL","HA","ACN","INFY","CTSH")
# Sectors associate with each stock
sectors <- c("Healthcare","Utilities","Financial","Services","Technology","Consumer Goods","Technology","Financial","Technology","Services","Technology","Consumer Goods","Healthcare","Technology","Technology","Healthcare","Basic Materials","Healthcare","Financial","Healthcare","Services","Telecommunications","Consumer Goods","Services","Services","Technology","Industrial Goods","Healthcare","Services","Consumer Goods","Financial","Healthcare",
             "Financial","Healthcare","Industrial Goods","Industrial Goods","Services","Industrial Goods","Financial","Basic Materials","Technology","Consumer Goods","Technology","Healthcare","Services","Healthcare","Technology","Consumer Goods","Technology","Utilities","Basic Materials","Services","Consumer Goods","Technology","Technology","Technology","Basic Materials","Basic Materials","Basic Materials","Financial","Financial","Financial",
             "Services-Airlines","Services-Airlines","Services-Airlines","Technology","Technology","Technology")

selectively_analyzed_symbols <- c("GS","IBM","EIX")
ticker_sector <- data.frame(symbol = tickers_symbols, sector = sectors)
```

Load the Processed tweets and stocks data from github
```{r tweets}
#load(url("https://github.com/goodwillyoga/E107project/raw/master/pulkit/processed-tweets.RData"))
load("/code/CSCIE-107/E107project/pulkit/processed-tweets.RData")
```
```{r stocks}
#load(url("https://github.com/goodwillyoga/E107project/raw/master/pulkit/processed-stocks.RData"))
load("/code/CSCIE-107/E107project/pulkit/processed-stocks.RData")
```
```{r}
# Create helper functions
normalize<-function(m){
  (m - mean(m))/sd(m)
}
```
Let us start looking at distribution of tweets per symbol
```{r,echo=FALSE, message=FALSE}
tweets_est %>% group_by(symbol) %>% 
  summarise(count = n()) %>% inner_join(ticker_sector) %>%
  ggplot(aes(symbol, log(count), color=sector)) + geom_point() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  ggtitle('Distribution of Tweets per Symbol') + xlab('Symbols')+ylab('Count(log scale) ')+
  theme(legend.position="bottom")
```

Look at distribution of tweets perr day
```{r,echo=FALSE, message=FALSE}
tweets_est %>% group_by(dt) %>% 
  summarise(count = n()) %>%
  ggplot(aes(dt, log(count))) + geom_point() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  theme(legend.position="bottom")+
  ggtitle('Distribution of Tweets per Day') + xlab('Dates')+ylab('Count(log scale) ')
```

Let us look at daily closing prices/Volumes for the 3 choosen stocks
```{r,echo=FALSE, message=FALSE}
dailyStockData %>% filter(symbol %in% selectively_analyzed_symbols) %>% 
  ggplot(aes(day, price)) + geom_line(aes(color=symbol)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  ggtitle('Varition of Daily closing Price') + xlab('Dates')+ylab('Closing Price ') + facet_wrap(~symbol,scales = "free")
dailyStockData %>% filter(symbol %in% selectively_analyzed_symbols) %>% 
  ggplot(aes(day, log(volume))) + geom_line(aes(color=symbol)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  theme(legend.position="bottom")+
  ggtitle('Varition of Volume') + xlab('Dates')+ylab('Daily Volume ') + facet_wrap(~symbol,scales = "free")
```
```{r,echo=FALSE, message=FALSE}
#We know that Stock markets are closed on weekends, let us compute the trading days so that we can focus on the trading days only
tradingDays <- stocks_est %>% mutate(day = mdy(lastTradeDate)) %>% ungroup() %>% select(day) %>% distinct(day)
```
We will start with modeling of daily stock/tweets data and try to do a linear regression to find relation between volume, closing Price and Daily Price Change with number of tweets. We will represent this once for all the shares and then selectively for 3 choosen stocks to see if we can find if this model 


```{r,echo=FALSE, message=FALSE}
# Normalized tweets count data is stored in daily_tweet_score, Daily Stock data is present in dailyStockData, 
#let us normalize the volume, closing price and Daily Price change and then join it daily_tweet_score
dailyStockTweetNormalized <- dailyStockData %>% 
  group_by(symbol) %>% 
  mutate_each(funs(normalize), volume) %>% 
  mutate_each(funs(normalize),price) %>% 
  mutate_each(funs(normalize),prcChange) %>% 
  inner_join(daily_tweet_score, by=c("symbol"="symbol", "day"="dt")) %>% select(symbol,price,volume,open,sector,date_timelb,day,prcChange,count,avgScore)

fits <- dailyStockTweetNormalized %>%
  group_by(symbol) %>%
  do(mod = lm(volume ~ count, data = .))

results_volume_count <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='count') %>% inner_join(ticker_sector)
colnames(results_volume_count)[6]<- 'pval'

# Plot for all stocks
ggplot(results_volume_count, aes(symbol, pval, color=sector)) + geom_point() + geom_hline(yintercept = .05, color = 'red') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("volume~count model for all Stocks") +
  geom_text_repel(aes(symbol, pval, label=symbol), data = filter(results_volume_count,  pval <.05 )) +
  theme(legend.position="bottom")

# Let us model closing price vs count for each stock
fits <- dailyStockTweetNormalized %>%
  group_by(symbol) %>%
  do(mod = lm(price ~ count, data = .))

results_price_count <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='count') %>% inner_join(ticker_sector)
colnames(results_price_count)[6]<- 'pval'

# Plot for all stocks
ggplot(results_price_count, aes(symbol, pval, color=sector)) + geom_point() + geom_hline(yintercept = .05, color = 'red') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("Closing Price~count model for all Stocks") +
  geom_text_repel(aes(symbol, pval, label=symbol), data = filter(results_price_count,  pval <.05 )) +
  theme(legend.position="bottom")


# Let us model Price Change vs count for all stocks
fits <- dailyStockTweetNormalized %>%
  group_by(symbol) %>%
  do(mod = lm(prcChange ~ count, data = .))

results_prcChange_count <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='count') %>% inner_join(ticker_sector)
colnames(results_prcChange_count)[6]<- 'pval'

# Plot for all stocks
ggplot(results_prcChange_count, aes(symbol, pval, color=sector)) + geom_point() + geom_hline(yintercept = .05, color = 'red') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("Daily Price Change~count model for all Stocks") +
  geom_text_repel(aes(symbol, pval, label=symbol), data = filter(results_prcChange_count,  pval <.05 )) +
  theme(legend.position="bottom")

# Let us do regression plots for the 3 stocks
dailyStockTweetNormalized %>% filter(symbol %in% selectively_analyzed_symbols) %>% ggplot( aes(count,volume))+geom_point()+geom_smooth() +facet_wrap(~symbol)
# Let us plot specific for the 3 stocks 
p1<- results_volume_count %>% filter(symbol %in% selectively_analyzed_symbols) %>% ggplot( aes(symbol, pval, color=symbol)) + geom_point() + geom_hline(yintercept = .05, color = 'red') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("volume~count model")+ theme(legend.position="bottom")

p2 <- results_price_count %>% filter(symbol %in% selectively_analyzed_symbols)  %>% ggplot( aes(symbol,pval, color=symbol))+geom_point()+geom_hline(yintercept = .05, color='red')+ 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+ ggtitle("Closing Price~count model")+ theme(legend.position="bottom")

p3<- results_prcChange_count %>% filter(symbol %in% selectively_analyzed_symbols)  %>% ggplot( aes(symbol,pval, color=symbol))+geom_point()+geom_hline(yintercept = .05, color='red')+ 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+ ggtitle("Price Change~count model")+ theme(legend.position="bottom")

dailyStockTweetNormalized %>% filter(symbol %in% selectively_analyzed_symbols) %>% ggplot( aes(count,prcChange))+geom_point()+geom_smooth() +facet_wrap(~symbol)

grid.arrange(arrangeGrob(p1+ 
                           theme(legend.position="none"), p2+ 
                           theme(legend.position="none"), p3+ 
                           theme(legend.position="none"), nrow=1), top = "P-Value for standardized")

# Let us plot the confidence intervals
p1 <- results_volume_count %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  select(symbol,estimate, pval,`conf.low`, `conf.high`) %>% 
  ggplot( aes(x = symbol, y = estimate, color = symbol)) +
  geom_point(size = 2) + ggtitle("volume~count model")+
  geom_errorbar(aes(ymax = `conf.high`, ymin = `conf.low`))

p2 <-results_price_count %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  select(symbol,estimate, pval,`conf.low`, `conf.high`) %>% 
  ggplot( aes(x = symbol, y = estimate, color = symbol)) +
  geom_point(size = 2) + ggtitle("Closing Price~count model")+
  geom_errorbar(aes(ymax = `conf.high`, ymin = `conf.low`))

p3 <-results_prcChange_count %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  select(symbol,estimate, pval,`conf.low`, `conf.high`) %>% 
  ggplot( aes(x = symbol, y = estimate, color = symbol)) +
  geom_point(size = 2) + ggtitle("Price Change~count model")+
  geom_errorbar(aes(ymax = `conf.high`, ymin = `conf.low`))

grid.arrange(arrangeGrob(p1+ 
                           theme(legend.position="none"), p2+ 
                           theme(legend.position="none"), p3+ 
                           theme(legend.position="none"), nrow=1), top = "Confidence Interval Plots")

# Cleanup all the objects created in this block
rm(p1, p2, p3, results_prcChange_count, results_price_count, results_volume_count,fits)

```

We can see that for GS we can have a successfull model of 


```{r ,echo=FALSE, message=FALSE}
# Let us try to model now based on the sentiment scores
fits <- dailyStockTweetNormalized %>%
  group_by(symbol) %>%
  do(mod = lm(volume ~ avgScore, data = .))

results_volume <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='avgScore') %>% inner_join(ticker_sector)
colnames(results_volume)[6]<- 'pval'

# Plot for all stocks
ggplot(results_volume, aes(symbol, pval, color=sector)) + geom_point() + geom_hline(yintercept = .05, color = 'red') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("volume~avgScore model") +
  geom_text_repel(aes(symbol, pval, label=symbol), data = filter(results_volume,  pval <.05 )) +
  theme(legend.position="bottom")

# Let us model closing price vs count for each stock
fits <- dailyStockTweetNormalized %>%
  group_by(symbol) %>%
  do(mod = lm(price ~ avgScore, data = .))

results_price <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='avgScore') %>% inner_join(ticker_sector)
colnames(results_price)[6]<- 'pval'

# Plot for all stocks
ggplot(results_price, aes(symbol, pval, color=sector)) + geom_point() + geom_hline(yintercept = .05, color = 'red') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("Closing Price~avgScore model") +
  geom_text_repel(aes(symbol, pval, label=symbol), data = filter(results_price,  pval <.05 )) +
  theme(legend.position="bottom")


# Let us model Price Change vs count for all stocks
fits <- dailyStockTweetNormalized %>%
  group_by(symbol) %>%
  do(mod = lm(prcChange ~ avgScore, data = .))

results_prcChange <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='avgScore') %>% inner_join(ticker_sector)
colnames(results_prcChange)[6]<- 'pval'

# Plot for all stocks
ggplot(results_prcChange, aes(symbol, pval, color=sector)) + geom_point() + geom_hline(yintercept = .05, color = 'red') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("Daily Price Change~avgScore model") +
  geom_text_repel(aes(symbol, pval, label=symbol), data = filter(results_prcChange,  pval <.05 )) +
  theme(legend.position="bottom")

# Let us plot specific for the 3 stocks 
p1<- results_volume %>% filter(symbol %in% selectively_analyzed_symbols) %>% ggplot( aes(symbol, pval, color=symbol)) + geom_point() + geom_hline(yintercept = .05, color = 'red') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("volume~avgScore model")+ theme(legend.position="bottom")

p2 <- results_price %>% filter(symbol %in% selectively_analyzed_symbols)  %>% ggplot( aes(symbol,pval, color=symbol))+geom_point()+geom_hline(yintercept = .05, color='red')+ 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+ ggtitle("Closing Price~avgScore model")+ theme(legend.position="bottom")

p3<- results_prcChange %>% filter(symbol %in% selectively_analyzed_symbols)  %>% ggplot( aes(symbol,pval, color=symbol))+geom_point()+geom_hline(yintercept = .05, color='red')+ 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+ ggtitle("Price Change~avgScore model")+ theme(legend.position="bottom")

grid.arrange(arrangeGrob(p1+ 
                           theme(legend.position="none"), p2+ 
                           theme(legend.position="none"), p3+ 
                           theme(legend.position="none"), nrow=1), top = "P-Value for standardized")

# Let us plot the confidence intervals
p1 <- results_volume %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  select(symbol,estimate, pval,`conf.low`, `conf.high`) %>% 
  ggplot( aes(x = symbol, y = estimate, color = symbol)) +
  geom_point(size = 2) + ggtitle("volume~avgScore model")+
  geom_errorbar(aes(ymax = `conf.high`, ymin = `conf.low`))

p2 <-results_price %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  select(symbol,estimate, pval,`conf.low`, `conf.high`) %>% 
  ggplot( aes(x = symbol, y = estimate, color = symbol)) +
  geom_point(size = 2) + ggtitle("Closing Price~avgScore model")+
  geom_errorbar(aes(ymax = `conf.high`, ymin = `conf.low`))

p3 <-results_prcChange %>% 
  filter(symbol %in% selectively_analyzed_symbols) %>% 
  select(symbol,estimate, pval,`conf.low`, `conf.high`) %>% 
  ggplot( aes(x = symbol, y = estimate, color = symbol)) +
  geom_point(size = 2) + ggtitle("Price Change~avgScore model")+
  geom_errorbar(aes(ymax = `conf.high`, ymin = `conf.low`))

grid.arrange(arrangeGrob(p1+ 
                           theme(legend.position="none"), p2+ 
                           theme(legend.position="none"), p3+ 
                           theme(legend.position="none"), nrow=1), top = "Confidence Interval Plots")

rm(p1, p2, p3, results_prcChange, results_price, results_volume,fits)

```

Last let us try to model hourly volume change, hourly price and hourly price change with the sentiment scores
```{r ,echo=FALSE, message=FALSE}

hourlyStockTweetNormalized <- hourly_stockData %>% 
  mutate(day = as.Date(day_hr)) %>%
  group_by(symbol,day) %>% mutate(n=n()) %>%  filter(n>1) %>%
  mutate_each(funs(normalize), hourlyVolumeChng) %>% 
  mutate_each(funs(normalize),avgPrice) %>% 
  mutate_each(funs(normalize),prcChange) %>% 
  inner_join(hourly_tweet_score, by=c("symbol"="symbol", "day_hr"="day_hr")) %>% ungroup() %>%
  select(symbol,avgPrice,hourlyVolumeChng,prcChange,sector,day_hr, date_timelb,avgScore)

# Let us model hourlyVolumeChange with avgScore and 

fits <- hourlyStockTweetNormalized %>%
  group_by(symbol) %>%
  do(mod = lm(hourlyVolumeChng ~ avgScore, data = .))

results_volume <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='avgScore') %>% inner_join(ticker_sector)
colnames(results_volume)[6]<- 'pval'

# Plot for all stocks
ggplot(results_volume, aes(symbol, pval, color=sector)) + geom_point() + geom_hline(yintercept = .05, color = 'red') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("volume~avgScore model") +
  geom_text_repel(aes(symbol, pval, label=symbol), data = filter(results_volume,  pval <.05 )) +
  theme(legend.position="bottom")

# Let us model closing avgPrice vs count for each stock
fits <- hourlyStockTweetNormalized %>%
  group_by(symbol) %>%
  do(mod = lm(avgPrice ~ avgScore, data = .))

results_price <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='avgScore') %>% inner_join(ticker_sector)
colnames(results_price)[6]<- 'pval'

# Plot for all stocks
ggplot(results_price, aes(symbol, pval, color=sector)) + geom_point() + geom_hline(yintercept = .05, color = 'red') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("Closing Price~avgScore model") +
  geom_text_repel(aes(symbol, pval, label=symbol), data = filter(results_price,  pval <.05 )) +
  theme(legend.position="bottom")


# Let us model Price Change vs count for all stocks
fits <- hourlyStockTweetNormalized %>%
  group_by(symbol) %>%
  do(mod = lm(prcChange ~ avgScore, data = .))

results_prcChange <- tidy(fits,mod, conf.int = TRUE) %>% filter(term=='avgScore') %>% inner_join(ticker_sector)
colnames(results_prcChange)[6]<- 'pval'

# Plot for all stocks
ggplot(results_prcChange, aes(symbol, pval, color=sector)) + geom_point() + geom_hline(yintercept = .05, color = 'red') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("Daily Price Change~avgScore model") +
  geom_text_repel(aes(symbol, pval, label=symbol), data = filter(results_prcChange,  pval <.05 )) +
  theme(legend.position="bottom")

# Let us plot specific for the 3 stocks 
p1<- results_volume %>% filter(symbol %in% selectively_analyzed_symbols) %>% ggplot( aes(symbol, pval, color=symbol)) + geom_point() + geom_hline(yintercept = .05, color = 'red') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("volume~avgScore model")+ theme(legend.position="bottom")

p2 <- results_price %>% filter(symbol %in% selectively_analyzed_symbols)  %>% ggplot( aes(symbol,pval, color=symbol))+geom_point()+geom_hline(yintercept = .05, color='red')+ 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+ ggtitle("Closing Price~avgScore model")+ theme(legend.position="bottom")

p3<- results_prcChange %>% filter(symbol %in% selectively_analyzed_symbols)  %>% ggplot( aes(symbol,pval, color=symbol))+geom_point()+geom_hline(yintercept = .05, color='red')+ 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+ ggtitle("Price Change~avgScore model")+ theme(legend.position="bottom")

grid.arrange(arrangeGrob(p1+ 
                           theme(legend.position="none"), p2+ 
                           theme(legend.position="none"), p3+ 
                           theme(legend.position="none"), nrow=1), top = "P-Value for standardized")

# No need to plot the confidence intervals as the pvalues itself are very high

rm(p1, p2, p3, results_prcChange, results_price, results_volume,fits)

dailyStockTweetNormalized %>% filter(symbol %in% selectively_analyzed_symbols) %>% mutate(week = isoweek(day)) %>% 
  ggplot()+geom_line(aes(day,avgScore, color="Sentiment Score"))+
  geom_line(aes(day,price, color="Price")) + facet_wrap(~symbol,scales = "free") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+ theme(legend.position="bottom")

stockEnd <- dailyStockData %>% group_by(symbol) %>% arrange(date_time)%>% do(tail(., n=1)) %>% select(symbol,price)
priceChangeOverPeriod <- dailyStockData %>% group_by(symbol) %>% arrange(date_time)%>% do(head(., n=1)) %>% select(symbol,price) %>% inner_join(stockEnd, by=c("symbol"="symbol")) %>% mutate(prcChange = `price.y` - `price.x`)
joinedDf <- tweets_est %>% group_by(symbol) %>% 
  summarise(count = n()) %>% ungroup() %>% inner_join(priceChangeOverPeriod, by=c("symbol"="symbol"))

map.market(id=joinedDf$symbol, area=joinedDf$count, group=joinedDf$symbol, color=joinedDf$prcChange, main="Stock Price changes and Tweet counts", lab=c(FALSE,TRUE))


```

![wordCloud](./wordCloud.png)